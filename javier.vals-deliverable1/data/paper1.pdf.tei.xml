<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coords="1,127.66,79.85,339.96,12.90">Small Models Struggle to Learn from Strong Reasoners</title>
				<funder ref="#_MBQ44Mw">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_afdF98E">
					<orgName type="full">Office of Naval Research</orgName>
					<orgName type="abbreviated">ONR</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100000006</idno>
				</funder>
				<funder ref="#_JgxXV85">
					<orgName type="full">)</orgName>
				</funder>
				<funder ref="#_Jqh9PYu">
					<orgName type="full">Air Force Office of Scientific Research</orgName>
					<orgName type="abbreviated">AFOSR</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100000181</idno>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-17">17 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.04,116.25,47.06,10.75"><forename type="first">Yuetai</forename><surname>Li</surname></persName>
							<email>yuetaili@uw.edu</email>
							<affiliation key="aff0" coords="1,80.84,144.84,122.76,10.37;1,212.89,142.84,6.59,6.99;1,219.98,144.84,133.49,10.37;1,362.76,142.84,6.59,6.99;1,369.85,144.84,151.68,10.37">
								<note type="raw_affiliation"><label>♣</label> University of Washington ♢ Carnegie Mellon University ♠ Western Washington University</note>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ♠ Western</orgName>
								<orgName type="institution" key="instit3">Washington University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,136.10,114.72,6.59,6.99;1,159.12,116.25,30.56,10.75"><forename type="first">♣</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0" coords="1,80.84,144.84,122.76,10.37;1,212.89,142.84,6.59,6.99;1,219.98,144.84,133.49,10.37;1,362.76,142.84,6.59,6.99;1,369.85,144.84,151.68,10.37">
								<note type="raw_affiliation"><label>♣</label> University of Washington ♢ Carnegie Mellon University ♠ Western Washington University</note>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ♠ Western</orgName>
								<orgName type="institution" key="instit3">Washington University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.17,116.25,75.40,10.75"><forename type="first">Zhangchen</forename><surname>Xu</surname></persName>
							<affiliation key="aff0" coords="1,80.84,144.84,122.76,10.37;1,212.89,142.84,6.59,6.99;1,219.98,144.84,133.49,10.37;1,362.76,142.84,6.59,6.99;1,369.85,144.84,151.68,10.37">
								<note type="raw_affiliation"><label>♣</label> University of Washington ♢ Carnegie Mellon University ♠ Western Washington University</note>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ♠ Western</orgName>
								<orgName type="institution" key="instit3">Washington University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.60,116.25,78.43,10.75"><forename type="first">Fengqing</forename><surname>Jiang</surname></persName>
							<email>fqjiang@uw.edu</email>
							<affiliation key="aff0" coords="1,80.84,144.84,122.76,10.37;1,212.89,142.84,6.59,6.99;1,219.98,144.84,133.49,10.37;1,362.76,142.84,6.59,6.99;1,369.85,144.84,151.68,10.37">
								<note type="raw_affiliation"><label>♣</label> University of Washington ♢ Carnegie Mellon University ♠ Western Washington University</note>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ♠ Western</orgName>
								<orgName type="institution" key="instit3">Washington University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,435.04,116.25,54.14,10.75"><forename type="first">Luyao</forename><surname>Niu</surname></persName>
							<email>luyaoniu@uw.edu</email>
							<affiliation key="aff0" coords="1,80.84,144.84,122.76,10.37;1,212.89,142.84,6.59,6.99;1,219.98,144.84,133.49,10.37;1,362.76,142.84,6.59,6.99;1,369.85,144.84,151.68,10.37">
								<note type="raw_affiliation"><label>♣</label> University of Washington ♢ Carnegie Mellon University ♠ Western Washington University</note>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ♠ Western</orgName>
								<orgName type="institution" key="instit3">Washington University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,489.19,114.72,6.59,6.99;1,108.28,130.31,17.94,10.75"><forename type="first">♣</forename><surname>Bill</surname></persName>
							<affiliation key="aff0" coords="1,80.84,144.84,122.76,10.37;1,212.89,142.84,6.59,6.99;1,219.98,144.84,133.49,10.37;1,362.76,142.84,6.59,6.99;1,369.85,144.84,151.68,10.37">
								<note type="raw_affiliation"><label>♣</label> University of Washington ♢ Carnegie Mellon University ♠ Western Washington University</note>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ♠ Western</orgName>
								<orgName type="institution" key="instit3">Washington University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,129.22,130.31,59.02,10.75"><forename type="first">Yuchen</forename><surname>Lin</surname></persName>
							<email>byuchenl@uw.edu</email>
							<affiliation key="aff0" coords="1,80.84,144.84,122.76,10.37;1,212.89,142.84,6.59,6.99;1,219.98,144.84,133.49,10.37;1,362.76,142.84,6.59,6.99;1,369.85,144.84,151.68,10.37">
								<note type="raw_affiliation"><label>♣</label> University of Washington ♢ Carnegie Mellon University ♠ Western Washington University</note>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ♠ Western</orgName>
								<orgName type="institution" key="instit3">Washington University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,188.24,128.78,6.59,6.99;1,211.26,130.31,43.18,10.75"><forename type="first">♣</forename><surname>Bhaskar</surname></persName>
							<affiliation key="aff0" coords="1,80.84,144.84,122.76,10.37;1,212.89,142.84,6.59,6.99;1,219.98,144.84,133.49,10.37;1,362.76,142.84,6.59,6.99;1,369.85,144.84,151.68,10.37">
								<note type="raw_affiliation"><label>♣</label> University of Washington ♢ Carnegie Mellon University ♠ Western Washington University</note>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ♠ Western</orgName>
								<orgName type="institution" key="instit3">Washington University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,381.75,130.31,98.16,10.75"><forename type="first">Radha</forename><surname>Poovendran</surname></persName>
							<affiliation key="aff0" coords="1,80.84,144.84,122.76,10.37;1,212.89,142.84,6.59,6.99;1,219.98,144.84,133.49,10.37;1,362.76,142.84,6.59,6.99;1,369.85,144.84,151.68,10.37">
								<note type="raw_affiliation"><label>♣</label> University of Washington ♢ Carnegie Mellon University ♠ Western Washington University</note>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ♠ Western</orgName>
								<orgName type="institution" key="instit3">Washington University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coords="1,127.66,79.85,339.96,12.90">Small Models Struggle to Learn from Strong Reasoners</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-17">17 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">A1B89670546BA3E33B23A2758E2030BE</idno>
					<idno type="arXiv">arXiv:2502.12143v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-02-18T10:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,87.87,244.93,185.91,8.64;1,87.87,256.89,185.90,8.64;1,87.87,268.84,184.25,8.64;1,87.87,280.80,35.28,8.64">Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise.</s><s coords="1,126.41,280.80,145.72,8.64;1,87.87,292.57,184.25,8.82;1,87.60,304.53,186.18,8.82;1,87.87,316.66,184.25,8.64;1,87.87,328.62,185.90,8.64;1,87.87,340.57,96.22,8.64">However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models (≤3B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models.</s><s coords="1,187.19,340.57,84.93,8.64;1,87.87,352.53,185.91,8.64;1,87.87,364.48,185.90,8.64;1,87.87,376.44,100.46,8.64">Instead, they perform better when fine-tuned on shorter, simpler reasoning chains that better align with their intrinsic learning capacity.</s><s coords="1,192.96,376.44,79.17,8.64;1,87.87,388.39,184.25,8.64;1,87.87,400.35,184.60,8.64;1,87.87,412.30,184.42,8.64;1,87.87,424.26,185.90,8.64;1,87.87,436.21,13.83,8.64">To address this, we propose Mix Distillation, a simple yet effective strategy that balances reasoning complexity by combining long and short CoT examples or reasoning from both larger and smaller models.</s><s coords="1,109.26,436.21,163.11,8.64;1,87.87,448.17,184.25,8.64;1,87.87,460.12,184.25,8.64;1,87.87,472.08,82.33,8.64">Our experiments demonstrate that Mix Distillation significantly improves small model reasoning performance compared to training on either data alone.</s><s coords="1,173.37,472.08,98.76,8.64;1,87.87,484.03,185.90,8.64;1,87.87,495.99,184.25,8.64;1,87.87,507.95,184.25,8.64;1,87.87,519.90,74.15,8.64">These findings highlight the limitations of direct strong model distillation and underscore the importance of adapting reasoning complexity for effective reasoning capability transfer.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,306.14,323.83,218.26,8.64;1,306.14,335.79,218.44,8.64;1,306.14,347.74,159.04,8.64">Figure <ref type="figure" coords="1,335.45,323.83,3.95,8.64">1</ref>: Small student models (≤3B parameters) do not consistently benefit from long CoT reasoning or distillation from large teacher models.</s><s coords="1,471.63,347.74,53.13,8.64;1,306.14,359.70,219.92,8.64;1,306.14,371.65,218.43,8.64;1,306.14,383.61,163.74,8.64">Instead, they perform better when fine-tuned on shorter CoT reasoning or distilled from smaller teachers, which better matches their intrinsic learning capacity.</s><s coords="1,472.97,383.61,51.43,8.64;1,306.14,395.38,193.15,8.82">We term this phenomenon the Small Model Learnability Gap.</s></p><p><s coords="1,306.14,436.84,20.71,9.46">cost.</s><s coords="1,333.87,436.84,190.54,9.46;1,306.14,450.39,219.63,9.46;1,306.14,463.94,218.27,9.46;1,306.14,477.49,218.27,9.46;1,306.14,491.04,151.05,9.46">A widely adopted strategy to achieve this is distillation <ref type="bibr" coords="1,366.99,450.39,97.54,9.46" target="#b0">(Agarwal et al., 2024;</ref><ref type="bibr" coords="1,467.42,450.39,58.35,9.46;1,306.14,463.94,25.35,9.46" target="#b9">Hinton et al., 2015;</ref><ref type="bibr" coords="1,335.14,463.94,81.21,9.46">Kim et al., 2024a)</ref>, where CoT sequences generated by a strong teacher model are used to fine-tune a weaker student model.</s><s coords="1,461.23,491.04,63.18,9.46;1,306.14,504.59,218.27,9.46;1,306.14,518.14,218.26,9.46;1,306.14,531.69,218.27,9.46;1,306.14,545.23,219.63,9.46;1,306.14,558.78,120.41,9.46">Naturally, one might expect that distilling CoT sequences from stronger models would consistently improve small models' complex reasoning capabilities <ref type="bibr" coords="1,483.05,531.69,41.36,9.46;1,306.14,545.23,52.52,9.46" target="#b0">(Agarwal et al., 2024;</ref><ref type="bibr" coords="1,361.43,545.23,115.85,9.46" target="#b4">DeepSeek-AI et al., 2024;</ref><ref type="bibr" coords="1,480.05,545.23,45.72,9.46;1,306.14,558.78,24.85,9.46" target="#b21">Min et al., 2024;</ref><ref type="bibr" coords="1,333.72,558.78,88.14,9.46" target="#b33">Tunstall et al., 2023)</ref>.</s></p><p><s coords="1,317.05,576.04,208.73,9.46;1,305.75,589.40,218.67,9.66;1,305.78,603.14,218.63,9.46;1,306.14,616.69,218.27,9.46;1,306.14,630.24,218.45,9.46;1,306.14,643.79,131.53,9.46">However, we reveal an interesting phenomenon, which we term the Small Model Learnability Gap (Fig. <ref type="figure" coords="1,330.45,603.14,4.11,9.46">1</ref>): small models do not consistently benefit from the complex reasoning sequences provided by strong teachers, such as long CoT reasoning or distillation from large models.</s><s coords="1,441.05,643.79,84.72,9.46;1,305.75,657.34,218.66,9.46;1,306.14,670.89,218.27,9.46;1,306.14,684.44,220.07,9.46;1,306.14,697.99,66.26,9.46">In our experiments, we observe that when small models are exposed to long and intricate reasoning traces, they struggle to internalize the multi-step logic due to their constrained ability.</s><s coords="1,375.79,697.99,150.43,9.46;1,306.14,711.34,218.27,9.66;1,306.14,724.89,218.26,9.66;1,306.14,738.63,79.05,9.46">Instead, small models perform better when fine-tuned on shorter, simpler reasoning chains that align more closely with their intrinsic learning capacity.</s><s coords="1,390.70,738.63,135.52,9.46;1,306.14,752.18,218.27,9.46;1,306.14,765.73,218.27,9.46;2,70.87,74.72,220.08,9.46;2,70.87,88.27,120.97,9.46">This suggests that small models struggle to process overly elaborate reasoning traces or adapt to the distribution shifts introduced by stronger teachers, ultimately limiting their ability to generalize effectively.</s></p><p><s coords="2,81.78,102.29,207.36,9.46;2,70.87,115.64,218.26,9.66;2,70.87,129.39,218.65,9.46;2,70.87,142.94,185.97,9.46">To address the challenge described above, we propose Mix Distillation, a simple yet effective approach that balances reasoning complexity by blending different types of reasoning traces.</s><s coords="2,260.07,142.94,30.88,9.46;2,70.87,156.49,219.79,9.46">Specifically, our method comprises two configurations:</s></p><p><s coords="2,70.51,169.84,218.63,9.66;2,70.87,183.59,220.08,9.46;2,70.87,197.14,220.17,9.46">(1) Mix-Long -A combination of long and short CoT examples, ensuring that small models are exposed to both detailed and concise reasoning steps.</s></p><p><s coords="2,70.51,210.49,218.63,9.66;2,70.87,224.23,218.27,9.46;2,70.87,237.78,218.27,9.46;2,70.87,251.33,72.31,9.46">(2) Mix-Large -A mixture of responses from both larger and smaller models, allowing small models to learn from reasoning chains that are better suited to their capacity.</s></p><p><s coords="2,81.78,265.15,209.17,9.66;2,70.87,278.70,220.08,9.66;2,70.87,292.45,220.17,9.46">Our experiments demonstrate that Mix Distillation consistently improves small model reasoning performance compared to standard distillation.</s><s coords="2,70.87,306.00,162.28,9.46">For instance, Qwen2.5-3B-Instruct</s><s coords="2,235.87,306.00,53.65,9.46;2,70.87,319.55,220.08,9.46;2,70.87,333.10,220.18,9.46">improves by more than 8 points on MATH and AMC using Mix-Long, compared to direct training on long CoT data.</s><s coords="2,70.87,346.89,103.64,8.76">Qwen2.5-3B-Instruct</s><s coords="2,177.78,346.65,111.35,9.46;2,70.87,360.19,220.08,9.46;2,70.87,373.74,200.97,9.46">gains more than 7 points on MATH, AIME and AMC using Mix-Large compared with training on large teacher CoT data.</s></p><p><s coords="2,81.78,387.76,209.17,9.46;2,70.87,401.31,220.08,9.46;2,70.87,414.66,220.08,9.66;2,70.87,428.21,158.97,9.66">These findings highlight a fundamental limitation of direct strong model distillation and emphasize the importance of adapting reasoning complexity for effective knowledge transfer.</s><s coords="2,234.00,428.41,55.51,9.46;2,70.87,441.96,218.66,9.46;2,70.87,455.51,218.27,9.46;2,70.87,469.06,218.27,9.46;2,70.87,482.61,112.28,9.46">By carefully designing distillation strategies, we provide new insights into overcoming the constraints of small model learning, making them more effective at reasoning-intensive tasks.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" coords="2,70.87,506.85,86.78,10.75">Preliminaries</head><p><s coords="2,70.87,529.64,65.15,9.81;2,70.87,548.13,93.60,10.63">2.1 Notation Let x = (x 1 , x 2 , . . .</s><s coords="2,166.28,548.13,124.67,10.63;2,70.87,561.68,195.30,10.63">, x n ) represent an input sequence (e.g., a prompt), and y = (y 1 , y 2 , . . .</s><s coords="2,267.99,561.68,22.42,10.63;2,70.87,575.58,175.15,9.46">, y m ) be the corresponding output sequence.</s><s coords="2,252.75,575.58,38.19,9.46;2,70.87,588.78,218.27,9.81;2,70.87,602.67,218.27,9.46;2,70.87,615.87,73.01,10.77">We consider a LLM parameterized by θ, which predicts the next token following a conditional distribution π θ y t |x, y 1:t-1 .</s><s coords="2,149.53,615.87,139.60,9.81;2,70.87,629.77,220.07,9.46;2,70.87,643.12,220.08,9.66;2,70.87,656.87,149.18,9.46">We denote by CoT(y) ⊆ y the subset of tokens in the generated output that encodes a chain-of-thought, often serving as a reasoning trace or explanatory sequence.</s></p><p><s coords="2,81.78,670.46,207.36,9.88;2,70.87,684.01,218.27,9.88;2,70.87,697.99,219.00,9.46;2,70.87,711.11,218.26,9.88;2,70.87,725.09,220.07,9.46;2,70.87,738.63,219.63,9.46;2,70.87,752.18,108.30,9.46">Throughout this work, we use the term short CoT, to describe concise reasoning paths to arrive at solutions <ref type="bibr" coords="2,127.20,697.99,81.40,9.46" target="#b21">(Min et al., 2024;</ref><ref type="bibr" coords="2,212.66,697.99,77.20,9.46" target="#b37">Yeo et al., 2025)</ref> and long CoT to describe an extended reasoning sequence that is not only longer but also demonstrates more complex reflective thoughts <ref type="bibr" coords="2,257.44,738.63,33.06,9.46;2,70.87,752.18,30.49,9.46">(Qwen, 2024b;</ref><ref type="bibr" coords="2,104.09,752.18,70.35,9.46" target="#b37">Yeo et al., 2025)</ref>.</s><s coords="2,182.58,752.18,106.56,9.46;2,70.87,765.31,218.27,9.88;2,306.14,74.72,218.27,9.46;2,306.14,87.85,218.27,9.88;2,306.14,101.82,168.22,9.46">Additionally, we use the term large teacher CoT to refer to the reasoning trace generated by a larger teacher model, and the term small teacher CoT for the reasoning steps produced by a smaller teacher model.</s><s coords="2,478.72,101.82,45.70,9.46;2,305.75,115.37,140.72,9.46">Please see Appendix D for more examples.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" coords="2,306.14,137.65,165.88,9.81">Supervised Fine-Tuning (SFT)</head><p><s coords="2,306.14,155.74,218.27,9.46;2,306.14,169.29,218.27,9.46;2,306.14,182.49,74.37,9.81;2,380.51,180.54,2.88,6.99;2,383.89,182.49,10.20,9.57;2,394.48,180.54,2.88,6.99;2,397.86,182.49,9.70,9.57;2,407.56,180.54,6.72,6.99;2,407.56,187.56,13.70,6.99;2,421.76,182.49,41.51,9.81;2,463.66,180.54,2.88,6.99;2,470.10,182.84,54.31,9.46;2,306.14,196.39,218.27,9.46;2,306.14,209.94,70.51,9.46">Supervised fine-tuning (SFT) is widely adopted to enhance reasoning capabilities of LLMs on a dataset D = {(x i , y i )} N i=1 , where y i can be short CoT, long CoT, strong model CoT or weak model CoT sequences.</s><s coords="2,380.37,209.94,145.84,9.46;2,306.14,223.14,218.27,9.81;2,306.14,237.04,218.27,9.46;2,306.14,250.24,44.47,9.81">The SFT process updates the parameters θ of a language model by minimization the negative log-likelihood loss over the instruction dataset D.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" coords="2,306.14,273.53,174.19,10.75">Small Model Learnability Gap</head><p><s coords="2,306.14,295.82,220.07,9.46;2,306.14,309.37,102.82,9.46">In this section, we fine-tune student models using different CoT data.</s><s coords="2,413.61,309.37,110.80,9.46;2,306.14,322.92,218.27,9.46;2,306.14,336.47,81.21,9.46">We then reveal the small model learnability gap given the performance of fine-tuned models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" coords="2,306.14,358.75,109.09,9.81">Experiment Setup</head><p><s coords="2,306.14,376.42,41.27,9.81">Datasets.</s><s coords="2,358.32,376.84,166.09,9.46;2,305.78,390.39,105.79,9.46">We use the 7,500 prompt set of MATH <ref type="bibr" coords="2,305.78,390.39,101.18,9.46" target="#b8">(Hendrycks et al., 2021)</ref>.</s><s coords="2,414.88,390.39,109.53,9.46;2,306.14,403.94,220.07,9.46;2,306.14,417.49,116.53,9.46">This dataset encompasses seven math topics such as advanced calculus, geometry, and linear algebra.</s></p><p><s coords="2,306.14,438.30,72.95,9.81">Student models.</s><s coords="2,390.01,438.73,134.40,9.46;2,306.14,452.28,218.27,9.46;2,305.78,465.82,220.53,9.46">Our study considers ten student models from the Qwen <ref type="bibr" coords="2,409.49,452.28,65.68,9.46">(Qwen, 2024a)</ref> and Llama <ref type="bibr" coords="2,305.78,465.82,72.09,9.46">(Meta, 2024a,b)</ref> model families of varying sizes.</s><s coords="2,305.80,479.37,218.61,9.46;2,306.14,493.16,68.24,8.76">These models include the Instruct version of Qwen2.5-0.5B,</s><s coords="2,387.29,493.16,68.24,8.76">Qwen2.5-1.5B,</s><s coords="2,468.45,493.16,57.33,8.76">Qwen2.5-3B,</s><s coords="2,306.14,506.71,57.33,8.76">Qwen2.5-7B,</s><s coords="2,370.36,506.47,155.41,9.46;2,306.14,520.02,219.63,9.46">Qwen2.5-14B, and Qwen2.5-32B, and the Instruct version of Llama3.2-1B,</s><s coords="2,306.14,533.81,62.78,8.76">Llama3.2-3B,</s><s coords="2,372.36,533.57,153.96,9.46">Llama3.1-8B, and Llama3.3-70B.</s><s coords="2,305.75,547.12,218.66,9.46;2,306.14,560.67,168.80,9.46">A comprehensive overview of the student models is presented in Table <ref type="table" coords="2,398.59,560.67,5.45,9.46" target="#tab_3">4</ref> of Appendix A.</s></p><p><s coords="2,306.14,581.48,78.91,9.81">Teacher models.</s><s coords="2,395.97,581.91,128.44,9.46;2,306.14,595.45,219.63,9.46;2,306.14,609.00,218.27,9.46;2,306.14,622.55,218.46,9.46;2,306.14,636.10,67.83,9.46">To compare long CoT with short CoT, we use QwQ-32B-Preview <ref type="bibr" coords="2,492.71,595.45,33.06,9.46;2,306.14,609.00,31.52,9.46">(Qwen, 2024b)</ref> to generate long CoT sequences and Qwen2.5-32B-Instruct as the response generator for short CoT.</s><s coords="2,379.39,636.10,145.02,9.46;2,306.14,649.65,218.27,9.46;2,306.14,663.20,218.27,9.46;2,306.14,676.75,33.11,9.46">Within each model family, we designate the larger scale model as the large teacher and the smaller scale model as the small teacher.</s><s coords="2,342.64,676.75,169.48,9.46">This includes Qwen2.5-72B-Instruct</s><s coords="2,514.86,676.75,9.55,9.46;2,306.14,690.54,106.31,8.76">vs Qwen2.5-3B-Instruct,</s><s coords="2,414.52,690.54,114.54,8.76">Llama3.1-70B-Instruct</s><s coords="2,305.87,703.85,218.54,9.46;2,305.87,717.40,80.61,9.46">vs Llama3.1-8B-Instruct, and Gemma2-27B-it vs Gemma2-9B-it.</s></p><p><s coords="2,306.14,738.21,117.73,9.81">Evaluation Benchmarks.</s><s coords="2,434.78,738.63,91.44,9.46;2,306.14,752.18,218.27,9.46;2,306.14,765.73,218.27,9.46;3,213.07,470.71,311.34,8.64;3,70.87,482.67,414.45,8.64">We evaluate the reasoning capability of fine-tuned student models on a set of commonly used benchmarks, including  For teacher models, Qwen2.5-72B-Instruct is chosen as the large teacher to generate responses, while Qwen2.5-3B-Instruct is chosen as the small teacher to generate responses.</s><s coords="3,488.43,482.67,35.98,8.64;3,70.54,494.30,357.93,9.65">Negative (positive) ∆ Large indicates that large teacher CoT is worse (better) than small teacher CoT.</s><s coords="3,430.95,494.62,93.45,8.64;3,70.87,506.26,453.55,9.65;3,70.87,518.21,233.96,9.65">Our results demonstrate that small teacher CoT is better for smaller student models (indicated by ∆ Large &lt; 0), while large model CoT is better for larger student models (indicated by ∆ Large &gt; 0).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="3,179.86,422.12,84.04,15.94">Student Model Size</head><p><s coords="3,70.87,552.39,218.27,9.46;3,70.87,565.94,220.08,9.46;3,70.87,579.49,219.63,9.46;3,70.87,593.04,28.74,9.46">MATH <ref type="bibr" coords="3,104.76,552.39,104.54,9.46" target="#b8">(Hendrycks et al., 2021)</ref>, GSM8K <ref type="bibr" coords="3,256.76,552.39,32.37,9.46;3,70.87,565.94,50.03,9.46" target="#b2">(Cobbe et al., 2021)</ref>, AMC 2023, AIME 2024, and the English math subset of OlympiadBench <ref type="bibr" coords="3,244.36,579.49,46.13,9.46;3,70.87,593.04,23.95,9.46" target="#b7">(He et al., 2024)</ref>.</s><s coords="3,105.82,593.04,183.32,9.46;3,70.87,606.59,218.26,9.46;3,70.87,620.13,139.23,9.46">These benchmarks span a wide range of challenge levels, from elementary mathematics to advanced competition problems.</s><s coords="3,213.49,620.13,77.46,9.46;3,70.87,633.68,218.27,9.46;3,70.87,647.23,76.10,9.46">We define the student model performance as the average score on five benchmarks.</s><s coords="3,152.07,647.23,137.07,9.46;3,70.87,660.78,220.08,9.46;3,70.87,674.33,122.08,9.46">Unless otherwise specified, all fine-tuned models are evaluated in a zero-shot setting using greedy decoding.</s><s coords="3,196.32,674.33,92.81,9.46;3,70.87,687.88,112.69,9.46">We set the maximum generation tokens as 16k.</s><s coords="3,186.93,687.88,102.59,9.46;3,70.87,701.43,137.99,9.46">Please see Appendix A for detailed experimental setup.</s></p><p><s coords="3,81.78,717.92,195.47,9.46">We define the following performance scores:</s></p><p><s coords="3,83.89,751.83,205.24,10.69;3,92.68,765.73,125.75,9.46">• P Long : Performance score of a student model fine-tuned on long CoT data.</s></p><p><s coords="3,319.16,552.04,205.24,10.77;3,327.96,565.94,128.17,9.46">• P Short : Performance score of a student model fine-tuned on short CoT data.</s></p><p><s coords="3,319.16,587.38,205.25,10.69;3,327.96,601.28,175.82,9.46">• P Large : Performance score of a student model fine-tuned on CoT from a larger teacher.</s></p><p><s coords="3,319.16,622.73,205.25,10.77;3,327.96,636.63,182.69,9.46">• P Small : Performance score of a student model fine-tuned on CoT from a smaller teacher.</s></p><p><s coords="3,306.14,656.91,76.40,9.81">Training Setup.</s><s coords="3,393.45,657.34,132.77,9.46;3,306.14,670.89,219.17,9.46;3,306.14,684.44,219.63,9.46;3,306.14,697.99,218.65,9.46;3,306.14,711.54,220.18,9.46">Teacher models generate responses by rejection sampling <ref type="bibr" coords="3,442.03,670.89,83.28,9.46" target="#b5">(Dong et al., 2023;</ref><ref type="bibr" coords="3,306.14,684.44,92.05,9.46" target="#b6">Gulcehre et al., 2023;</ref><ref type="bibr" coords="3,400.76,684.44,74.56,9.46" target="#b29">Tong et al., 2024;</ref><ref type="bibr" coords="3,477.89,684.44,47.88,9.46;3,306.14,697.99,25.35,9.46" target="#b39">Yuan et al., 2023;</ref><ref type="bibr" coords="3,334.35,697.99,72.89,9.46" target="#b40">Yue et al., 2023;</ref><ref type="bibr" coords="3,410.12,697.99,98.83,9.46" target="#b42">Zelikman et al., 2022)</ref> By default, teacher models employ greedy decoding.</s><s coords="3,306.14,725.09,218.27,9.46;3,306.14,738.63,220.08,9.46;3,306.14,752.18,220.08,9.46;3,306.14,765.73,88.09,9.46">By combining the math problem instructions with corresponding solutions generated by teacher models, we construct problem-solution pairs to finetune student models.</s><s coords="3,397.58,765.73,126.83,9.46;4,70.87,74.72,220.17,9.46">We train the models using the LLaMA-Factory framework <ref type="bibr" coords="4,199.40,74.72,86.85,9.46" target="#b45">(Zheng et al., 2024)</ref>.</s></p><p><s coords="4,70.87,88.27,218.27,9.46;4,70.87,101.82,220.08,9.46;4,70.87,115.37,218.27,9.46;4,70.32,128.57,10.91,9.57;4,81.23,126.62,10.82,6.99;4,95.92,128.92,193.21,9.46;4,70.51,142.47,103.65,9.46">For student models of scale less than 14B, we use full-parameter SFT and implement a cosine learning rate schedule with a maximum learning rate of 10 -5 to fine-tune student models for two epochs <ref type="bibr" coords="4,70.51,142.47,98.83,9.46">(Touvron et al., 2023b)</ref>.</s><s coords="4,177.55,142.47,111.77,9.46;4,70.87,156.02,220.08,9.46;4,70.87,169.22,58.09,9.81;4,128.95,167.27,10.82,6.99;4,142.99,169.57,66.86,9.46">For student models larger than 14B, we adopt LoRA fine-tuning with a learning rate of 10 -4 for two epochs.</s><s coords="4,213.24,169.57,77.71,9.46;4,70.87,183.12,218.27,9.46;4,70.87,196.67,165.26,9.46">Detailed hyperparameters and information about the experimental platform are provided in Appendix A.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" coords="4,70.87,217.79,94.87,9.81">Long CoT Gap</head><p><s coords="4,70.53,235.69,218.61,9.46;4,70.87,249.24,218.27,9.46;4,70.87,262.79,86.67,9.46">This section evaluates the reasoning capabilities of student models fine-tuned over long CoT data and short CoT data.</s><s coords="4,160.92,262.79,128.22,9.46;4,70.87,276.34,218.27,9.46;4,70.87,289.54,144.05,10.69">We quantify the performance difference between long and short CoT data using long CoT gap ∆ Long , defined as:</s></p><formula xml:id="formula_0" coords="4,122.10,308.95,115.80,10.77">∆ Long = P Long -P Short .</formula><p><s coords="4,81.78,328.71,207.36,9.46;4,70.87,341.91,218.27,10.69;4,70.87,355.81,33.66,9.46">Figure <ref type="figure" coords="4,113.01,328.71,5.46,9.46">2</ref> provides a comprehensive overview of the long CoT gap ∆ Long across different student models.</s><s coords="4,107.96,355.81,182.54,9.46;4,70.87,369.35,218.27,9.46;4,70.87,382.90,218.27,9.46;4,70.87,396.45,110.87,9.46">The detailed benchmark scores on MATH, GSM8K, AIME, AMC, and OlympiadBench are deferred to Table <ref type="table" coords="4,149.39,382.90,5.56,9.46" target="#tab_6">7</ref> in Appendix B. We report the following key takeaways.</s></p><p><s coords="4,90.17,418.23,124.22,9.46;4,86.46,436.56,187.09,9.46;4,86.46,450.10,187.09,9.46;4,86.46,463.65,172.77,9.46">Takeaway 1: Long CoT Gap Small student models tend to benefit more from short CoT, while large student models gain greater advantages from long CoT.</s></p><p><s coords="4,81.78,490.51,207.55,9.46;4,70.87,504.06,218.27,9.46;4,70.87,517.61,186.39,9.46">We observe that long CoT is more effective for larger models, consistently leading to improved performance across most math benchmarks.</s><s coords="4,260.48,517.61,30.47,9.46;4,70.87,531.16,218.27,9.46">For example, the student model Qwen2.5-32B-Instruct</s><s coords="4,70.87,544.71,218.27,9.46;4,70.87,558.26,49.90,9.46">improves about 15 points across all math metrics on average.</s><s coords="4,81.78,571.81,207.36,9.46;4,70.87,585.36,218.27,9.46;4,70.87,598.90,179.24,9.46">However, long CoT data is not effective for smaller models, yielding significantly less improvement compared to short CoT.</s><s coords="4,256.02,598.90,33.11,9.46;4,70.87,612.45,218.27,9.46;4,70.87,626.24,114.54,8.76">On the MATH and AMC benchmarks, student model Qwen2.5-1.5B-Instruct</s><s coords="4,187.82,626.00,101.32,9.46;4,70.87,639.55,193.84,9.46">performs over 10 points lower when fine-tuned with long CoT data.</s><s coords="4,269.35,639.55,19.78,9.46;4,70.87,653.10,220.08,9.46;4,70.87,666.65,220.18,9.46">This shows that smaller models may not be able to effectively learn and utilize the long CoT paradigm.</s><s coords="4,70.87,680.20,220.17,9.46">Please see more attribution analysis in Section 3.4.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" coords="4,70.87,701.32,138.27,9.81">Large Teacher CoT Gap</head><p><s coords="4,70.35,719.22,219.16,9.46;4,70.87,732.77,198.15,9.46">We investigate how effective small models may learn from large teacher and small teachers.</s><s coords="4,274.56,732.77,14.58,9.46;4,70.87,746.13,150.22,9.66;4,389.66,427.73,5.81,8.64">We define a large teacher CoT gap as: ).</s><s coords="4,398.58,427.73,127.48,8.64;4,306.14,439.69,215.07,8.64">We find that small student models may struggle to learn from large teacher CoT data.</s></p><formula xml:id="formula_1" coords="4,118.28,765.38,123.45,10.77">∆ Large = P Large -P Small .</formula><p><s coords="4,317.05,478.65,209.17,9.46;4,306.14,491.85,220.08,10.69;4,306.14,505.75,15.14,9.46">Figure <ref type="figure" coords="4,350.98,478.65,5.56,9.46" target="#fig_0">3</ref> provides a comprehensive comparison of the ∆ Large incurred by all student models.</s><s coords="4,329.95,505.75,195.82,9.46;4,306.14,519.29,220.08,9.46;4,306.14,532.84,220.08,9.46;4,306.14,546.39,218.27,9.46;4,306.14,560.18,65.45,8.76">The detailed benchmark scores of MATH, GSM8K, AIME, AMC and OlympiadBench are deferred to Table <ref type="table" coords="4,372.72,532.84,5.44,9.46" target="#tab_7">8</ref> in Appendix B. More experimental results of different teacher models, including Llama3.1-70B</s><s coords="4,374.67,559.94,149.74,9.46;4,305.87,573.49,193.95,9.46">vs Llama3.1-8B and Gemma2-27B vs Gemma2-9B are in Table <ref type="table" coords="4,424.07,573.49,5.45,9.46" target="#tab_8">9</ref> of Appendix B.</s></p><p><s coords="4,317.05,589.59,207.36,9.46;4,306.14,603.14,220.17,9.46">We observe that larger student models learn effectively from large teacher CoT.</s><s coords="4,306.14,616.69,69.03,9.46;4,391.28,616.69,133.13,9.46;4,306.14,630.48,109.09,8.76">For example, Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct</s><s coords="4,437.68,630.24,32.15,9.46;4,492.27,630.24,32.15,9.46;4,306.14,643.79,218.27,9.46;4,306.14,657.58,109.09,8.76">student models improve over 5 points on average, with Qwen2.5-32B-Instruct</s><s coords="4,423.29,657.34,101.12,9.46;4,306.14,670.89,220.18,9.46">achieving more than a 15 point increase on the AIMC benchmark.</s><s coords="4,306.14,684.44,220.08,9.46;4,306.14,697.99,218.27,9.46;4,306.14,711.78,111.87,8.76">However, smaller models do not learn effectively from large teacher models such as Qwen2.5-72B-Instruct.</s><s coords="4,425.16,711.54,99.43,9.46;4,306.14,725.09,190.98,9.46">Instead, small teacher models such as Qwen2.5-3B-Instruct</s><s coords="4,505.63,725.09,19.16,9.46;4,306.14,738.63,220.07,9.46;4,306.14,752.18,58.45,9.46">may serve as better teacher models for small student models.</s><s coords="4,372.91,752.18,151.51,9.46;4,306.14,765.97,114.54,8.76">For instance, the performance of Qwen2.5-0.5B-Instruct</s><s coords="4,423.48,765.73,100.92,9.46;5,70.05,384.93,151.51,9.46">degrades by more than 10 points on the AMC benchmark.</s></p><p><s coords="5,81.78,398.48,207.36,9.46;5,70.87,412.03,220.08,9.46;5,70.87,425.58,220.08,9.46;5,70.87,439.13,132.02,9.46">Note that prior studies <ref type="bibr" coords="5,183.60,398.48,79.61,9.46">(Kim et al., 2024b</ref>) also demonstrated that stronger models are not necessarily stronger teachers, emphasizing response generator and teacher-side factors.</s><s coords="5,206.27,439.13,82.86,9.46;5,70.87,452.68,218.27,9.46;5,70.87,466.23,111.81,9.46">Our work differs in that we attribute this phenomenon primarily to the size of the student model.</s></p><p><s coords="5,90.17,490.10,163.53,9.46;5,86.46,510.12,187.27,9.46;5,86.46,523.67,187.08,9.46;5,86.46,537.22,178.55,9.46">Takeaway 2: Large Teacher CoT Gap Small student models tend to learn better from small teachers, while large student models benefit more from large teachers.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4" coords="5,70.87,571.69,218.27,9.81">Analysis of Small Model Learnability Gap</head><p><s coords="5,70.87,589.17,220.17,9.81">Domain knowledge affects learnability gap.</s><s coords="5,70.35,603.14,218.78,9.46;5,70.87,616.69,218.26,9.46;5,70.87,630.24,220.08,9.46;5,70.87,643.79,219.63,9.46;5,70.47,657.34,220.47,9.46;5,70.87,670.89,199.18,9.46">We observe that math expert models, in spite of small model size, exhibit a smaller learnability gap for both long CoT and large teacher CoT data compared to general models in Figure <ref type="figure" coords="5,223.83,643.79,4.17,9.46">4</ref>. Specifically, we compare the learnability gaps between the student models Qwen2.5-Math-1.5B-Instruct</s><s coords="5,273.07,670.89,16.07,9.46;5,70.87,684.68,117.22,8.76">and Qwen2.5-1.5B-Instruct.</s><s coords="5,191.47,684.44,97.66,9.46;5,70.87,697.99,218.26,9.46;5,70.87,711.54,220.08,9.46;5,70.87,725.09,14.59,9.46">Our findings show that the long CoT gap of the small math expert model is significantly smaller than that of general small models.</s><s coords="5,88.85,725.09,200.29,9.46;5,70.87,738.87,92.73,8.76">Furthermore, the performance improvement of Qwen2.5-Math-1.5B</s><s coords="5,166.31,738.63,122.82,9.46;5,70.87,752.18,197.62,9.46">when fined-tuned with large teacher CoT exceeds that of Qwen2.5-1.5B,</s><s coords="5,271.78,752.18,19.16,9.46;5,70.87,765.73,220.08,9.46;5,306.14,384.93,151.02,9.46">suggesting that math expert models benefit more sub-stantially from large teacher CoT.</s><s coords="5,460.33,384.93,64.08,9.46;5,306.14,398.48,220.08,9.46;5,306.14,411.83,218.26,9.66;5,306.14,425.38,93.85,9.40">We conjecture that a key factor leading to the small model learnability gap is the limited in-domain knowledge of small student models.</s><s coords="5,403.37,425.58,122.85,9.46;5,306.14,439.13,134.68,9.46">We summarize this observation in the following takeaway.</s></p><p><s coords="5,325.45,464.79,186.98,9.46;5,321.73,484.81,188.90,9.46;5,321.73,498.36,187.08,9.46;5,321.73,511.91,83.61,9.46">Takeaway 3: Effect of Domain Knowledge Limited domain knowledge of small models may hinder their learning from strong reasoning teachers.</s></p><p><s coords="5,306.14,549.70,220.08,9.81;5,306.14,563.25,35.16,9.81">Base models exhibit a more significant learnability gap.</s><s coords="5,352.22,563.68,172.57,9.46;5,306.14,577.23,220.08,9.46;5,306.14,590.78,114.69,9.46">We observe that base models generally exhibit a more significant learnability gap than Instruct models in Figure <ref type="figure" coords="5,412.49,590.78,4.17,9.46" target="#fig_2">5</ref>.</s><s coords="5,424.86,590.78,99.55,9.46;5,306.14,604.33,220.07,9.46;5,306.14,617.88,218.45,9.46;5,306.14,631.43,21.32,9.46">This suggests that it is more challenging for small base models to effectively learn from long CoT data or large teacher CoT.</s></p><p><s coords="5,325.45,655.00,127.85,9.46;5,321.73,673.32,188.90,9.46;5,321.73,686.87,183.56,9.46">Takeaway 4: Base vs Instruct Small base models experience more significant learnability gap than Instruct models.</s></p><p><s coords="5,306.14,724.66,94.57,9.81">Speaking styles shift.</s><s coords="5,411.62,725.09,112.79,9.46;5,305.78,738.63,218.63,9.46;5,306.14,752.18,218.61,9.46;5,306.14,765.73,131.41,9.46">We adopt the method from <ref type="bibr" coords="5,305.78,738.63,73.77,9.46" target="#b18">(Lin et al., 2023)</ref> to evaluate the rank shift of each token before and after fine-tuning on long CoT and Large teacher CoT data.</s><s coords="5,445.02,765.73,79.39,9.46;6,70.87,433.85,218.65,9.46;6,70.87,447.40,105.01,9.46">This allows us to compare the token distribution shifts induced by the fine-tuning process.</s><s coords="6,180.70,447.40,110.24,9.46;6,70.87,460.95,218.27,9.46;6,70.87,474.50,64.98,9.46">We then annotate the tokens that exhibit the largest rank shifts as the most shifted tokens.</s><s coords="6,139.96,474.50,150.99,9.46;6,70.87,488.04,218.27,9.46;6,70.87,501.59,218.27,9.46;6,69.42,515.14,26.96,9.46">Our analysis reveals that these tokens are predominantly associated with expressive and stylistic elements, such as "wait", "But", and "Let".</s><s coords="6,99.75,515.14,174.82,9.46">Please see Appendix C for more details.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" coords="6,70.87,618.07,207.95,10.75;6,88.80,632.02,88.50,10.75">Mix Distillation: Bridge Small Model Learnability Gap</head><p><s coords="6,70.53,655.35,218.61,9.46;6,70.87,668.89,185.08,9.46">This section presents our Mix Distillation approach to bridge the small model learnability gap.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" coords="6,70.87,692.62,97.58,9.81">Mix Distillation</head><p><s coords="6,70.35,711.34,220.59,9.66;6,70.87,725.09,173.67,9.46">We propose Mix Distillation to address the learnability gap observed in small models.</s><s coords="6,252.65,725.09,38.29,9.46;6,70.87,738.63,220.08,9.46;6,70.87,752.18,218.27,9.46;6,70.87,765.73,92.12,9.46">This approach blends easier-to-learn data with more challenging data for small models, thereby leveraging the strengths of both.</s></p><p><s coords="6,317.05,433.85,207.36,9.46;6,306.14,447.40,218.27,9.46;6,306.14,460.95,218.45,9.46;6,306.14,474.50,218.27,9.46;6,306.14,488.04,114.93,9.46">Our insight is that small models tend to perform better on data that closely matches their inherent distribution (such as short CoT or small teacher CoT), while they struggle with data that exhibits greater distribution shifts.</s><s coords="6,425.35,488.04,99.07,9.46;6,306.14,501.59,218.26,9.46;6,306.14,515.14,220.08,9.46;6,306.14,528.69,218.27,9.46;6,306.14,542.24,188.64,9.46">The token distribution of the mixed long CoT and large teacher CoT data may become closer to that of small models' inherent distribution, thereby enabling them to learn more effectively from challenging datasets.</s></p><p><s coords="6,317.05,556.95,207.36,9.46;6,306.14,570.50,218.61,9.46;6,306.14,583.70,95.55,9.81">We propose Mix-Long, which combines long CoT and short CoT data with a weight of long CoT α and short CoT 1-α.</s><s coords="6,404.87,584.05,121.34,9.46;6,306.14,597.60,218.27,9.46;6,305.75,610.80,218.66,9.81;6,306.14,624.35,40.35,9.81">Similarly, we proposed Mix-Large, which combines large teacher CoT with a weight of α and small teacher CoT with a weight of 1 -α.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" coords="6,306.14,650.68,116.36,9.81">Experiment Results</head><p><s coords="6,305.63,670.89,218.78,9.46;6,306.14,684.44,168.73,9.46">We use Qwen2.5-3B-Instruct as the student model and MATH (7.5k) as the training set.</s><s coords="6,480.87,684.44,43.54,9.46;6,306.14,697.99,218.27,9.46;6,306.14,711.54,56.37,9.46">We distill different teacher models to generate responses as the baseline.</s><s coords="6,368.28,711.54,157.50,9.46;6,306.14,725.33,60.00,8.76">They include QwQ-32B (long CoT), Qwen2.5-32B</s><s coords="6,371.21,725.09,122.38,9.46">(short CoT), Qwen2.5-72B</s><s coords="6,498.66,725.09,25.75,9.46;6,306.14,738.63,121.35,9.46">(large teacher CoT), Qwen2.5-3B</s><s coords="6,431.70,738.63,94.63,9.46">(small teacher CoT).</s><s coords="6,305.63,752.18,219.51,9.46;6,306.14,765.73,218.27,9.46;7,81.78,602.85,207.36,9.46;7,70.47,616.05,218.66,9.81;7,70.87,629.95,158.72,9.46">We add Deepseek-R1-32B (DeepSeek-AI, 2025) as the teacher model to generate another set of long  Figure <ref type="figure" coords="7,119.06,602.85,5.56,9.46" target="#fig_4">6</ref> shows the average performance when taking different mix weight α of long CoT data or large teacher CoT.</s><s coords="7,236.62,629.95,52.52,9.46;7,70.87,643.49,218.27,9.46;7,70.87,656.69,218.26,9.81;7,70.87,670.59,218.45,9.46;7,70.87,684.14,137.97,9.46">We choose Qwen2.5-3B-Instruct as the student model and find that a weight α of 0.2 achieves the highest average performance across five benchmarks for both Mix-Long and Mix-Large.</s></p><p><s coords="7,81.78,697.99,207.69,9.46;7,70.87,711.54,220.08,9.46;7,70.87,725.09,218.26,9.46;7,70.87,738.63,220.08,9.46;7,70.87,752.18,218.26,9.46;7,70.87,765.73,39.88,9.46">Interestingly, we find that after mixing long CoT and short CoT data, the small student model's output incorporates characteristics of long CoT, such as a branching process, while maintaining a reduced token length and avoiding overly elaborate thinking.</s><s coords="7,116.60,765.73,135.53,9.46">This is illustrated in Figure <ref type="figure" coords="7,243.78,765.73,4.17,9.46" target="#fig_5">7</ref>.</s><s coords="7,257.99,765.73,32.95,9.46;7,306.14,403.35,218.27,9.46;7,306.14,416.90,218.27,9.46;7,306.14,430.45,220.08,9.46;7,306.14,444.00,220.07,9.46;7,306.14,457.55,27.37,9.46">We ob- served that the small student model fine-tuned on long CoT data becomes overwhelmed by repeated thoughts and fails to stop, whereas the model finetuned on short CoT data produces incorrect answers.</s><s coords="7,336.90,457.55,187.51,9.46;7,306.14,471.10,218.27,9.46;7,304.69,484.64,199.52,9.46">In contrast, our proposed Mix-Long, which incorporates branching elements (e.g., the use of "Alternatively"), delivers the correct answer.</s><s coords="7,508.92,484.64,17.30,9.46;7,306.14,498.19,218.27,9.46;7,306.14,511.74,218.27,9.46;7,306.14,525.29,50.49,9.46">Additionally, the average token lengths of responses generated by long CoT, short CoT, and Mix-Long are 3384.7,</s><s coords="7,360.38,525.29,143.66,9.46">575.7, and 1248.9, respectively.</s><s coords="7,509.84,525.29,14.58,9.46;7,306.14,538.84,218.27,9.46;7,306.14,552.39,218.61,9.46;7,306.14,565.94,220.08,9.46;7,306.14,579.49,124.55,9.46">We suggest that mixing long CoT and short CoT data is a practical approach to achieving a balanced CoT length, thereby enhancing the reasoning capabilities of small student models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" coords="7,306.14,603.04,89.09,10.75">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" coords="7,306.14,625.35,109.41,9.81">Chain-of-Thought</head><p><s coords="7,306.14,643.79,218.27,9.46;7,306.14,657.34,218.27,9.46;7,306.14,670.89,219.17,9.46;7,306.14,684.44,219.63,9.46;7,306.14,697.99,28.74,9.46">Early research on CoT primarily focused on short CoT, where models produce succinct reasoning paths to reach a solution <ref type="bibr" coords="7,424.28,670.89,101.04,9.46" target="#b16">(Lambert et al., 2025;</ref><ref type="bibr" coords="7,306.14,684.44,95.56,9.46" target="#b19">Longpre et al., 2023;</ref><ref type="bibr" coords="7,405.34,684.44,75.53,9.46" target="#b34">Wei et al., 2023;</ref><ref type="bibr" coords="7,484.50,684.44,41.27,9.46;7,306.14,697.99,23.95,9.46" target="#b38">Yu et al., 2024)</ref>.</s><s coords="7,339.48,697.99,184.94,9.46;7,306.14,711.54,220.07,9.46;7,306.14,725.09,218.27,9.46;7,305.78,738.63,219.99,9.46;7,306.14,752.18,219.63,9.46;7,306.14,765.73,114.33,9.46">Recently, researchers have turned to long CoT prompting, which encourages the generation of extended and detailed reasoning chains (DeepSeek-AI, 2025; <ref type="bibr" coords="7,400.17,738.63,71.83,9.46" target="#b10">Hou et al., 2025;</ref><ref type="bibr" coords="7,474.72,738.63,51.05,9.46;7,306.14,752.18,25.35,9.46" target="#b15">Kimi Team, 2025;</ref><ref type="bibr" coords="7,334.81,752.18,72.26,9.46" target="#b22">NovaSky, 2025;</ref><ref type="bibr" coords="7,410.38,752.18,67.29,9.46" target="#b23">OpenAI, 2024;</ref><ref type="bibr" coords="7,480.99,752.18,44.79,9.46;7,306.14,765.73,25.35,9.46" target="#b24">Pan et al., 2025;</ref><ref type="bibr" coords="7,335.43,765.73,80.26,9.46" target="#b43">Zeng et al., 2025)</ref>.</s><s coords="7,427.49,765.73,98.73,9.46;8,70.87,74.72,220.08,9.46;8,70.59,88.27,218.54,9.46;8,70.47,101.82,99.99,9.46">The model systemati-cally explores multiple paths (branching) and reverts to earlier points if a particular path proves wrong (backtracking).</s><s coords="8,177.33,101.82,111.81,9.46;8,70.87,115.37,218.27,9.46;8,70.87,128.92,220.08,9.46;8,70.87,142.47,220.07,9.46;8,70.87,156.02,164.96,9.46">Although several studies have investigated methods such as distillation and reinforcement learning to integrate long CoT capabilities into LLMs, these efforts have predominantly concentrated on large models.</s><s coords="8,240.17,156.02,50.33,9.46;8,70.87,169.57,220.08,9.46;8,70.87,183.12,148.78,9.46">In contrast, our work specifically targets the challenges associated with training smaller models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" coords="8,70.87,208.21,144.55,9.81">Synthetic Reasoning Data</head><p><s coords="8,70.47,227.91,218.66,9.46;8,70.87,241.46,218.27,9.46;8,70.51,255.01,220.44,9.46;8,70.87,268.56,220.18,9.46">Although human-crafted reasoning datasets have been used to enhance LLM reasoning capabilities <ref type="bibr" coords="8,70.51,255.01,102.00,9.46" target="#b8">(Hendrycks et al., 2021;</ref><ref type="bibr" coords="8,174.84,255.01,60.89,9.46">LI et al., 2024)</ref>, their development is both time-consuming and labor-intensive.</s><s coords="8,70.87,282.11,218.27,9.46;8,70.87,295.65,218.65,9.46;8,70.87,309.20,219.63,9.46;8,70.87,322.75,219.63,9.46;8,70.87,336.30,218.45,9.46;8,70.87,349.85,220.07,9.46;8,70.87,363.40,220.08,9.46;8,70.47,376.95,25.54,9.46">Recent advancements have streamlined this process by generating instructions or responses directly from LLMs <ref type="bibr" coords="8,129.11,309.20,80.57,9.46" target="#b12">(Hui et al., 2024;</ref><ref type="bibr" coords="8,214.07,309.20,76.42,9.46;8,70.87,322.75,24.35,9.46" target="#b30">Toshniwal et al., 2024;</ref><ref type="bibr" coords="8,97.70,322.75,65.63,9.46" target="#b35">Xu et al., 2024;</ref><ref type="bibr" coords="8,165.80,322.75,69.17,9.46" target="#b40">Yue et al., 2023;</ref><ref type="bibr" coords="8,237.44,322.75,53.06,9.46;8,70.87,336.30,25.50,9.46" target="#b44">Zhang et al., 2025)</ref> or extracting data directly from web <ref type="bibr" coords="8,259.13,336.30,30.19,9.46;8,70.87,349.85,54.01,9.46" target="#b25">(Paster et al., 2023;</ref><ref type="bibr" coords="8,128.39,349.85,73.46,9.46" target="#b41">Yue et al., 2024)</ref>, yielding more detailed and diverse chain-of-thought reasoning pathways.</s><s coords="8,102.86,376.95,186.27,9.46;8,70.87,390.50,219.63,9.46;8,70.87,404.05,220.08,9.46;8,70.87,417.60,218.45,9.46;8,70.87,431.15,220.08,9.46;8,70.87,444.70,174.23,9.46">Recent study has investigated the impact of various response generators <ref type="bibr" coords="8,205.17,390.50,80.57,9.46">(Kim et al., 2024b)</ref>, suggesting that in the domains of instruction following and reasoning, responses from stronger teacher models do not necessarily produce the most effective learning effects for student models.</s><s coords="8,248.51,444.70,41.99,9.46;8,70.87,458.24,218.27,9.46;8,70.87,471.79,220.08,9.46;8,70.87,485.34,220.08,9.46;8,70.87,498.89,203.97,9.46">However, these investigations have not recognized student model size as a critical factor influencing this phenomenon, nor have they performed the more attribution and mitigation analyses as in this paper.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" coords="8,70.87,524.64,167.28,10.75">Conclusion and Future Work</head><p><s coords="8,70.87,548.95,218.27,9.46;8,70.87,562.49,218.45,9.46;8,70.87,576.04,97.60,9.46">In this paper, we show that long CoT data and large model responses were not uniformly beneficial for small student models.</s><s coords="8,173.29,576.04,117.65,9.46;8,70.87,589.59,218.26,9.46;8,70.87,603.14,122.91,9.46">We found that small models may perform better when fine-tuned with short CoT and small model CoT.</s><s coords="8,197.13,603.14,93.81,9.46;8,70.87,616.69,196.41,9.46">We termed this challenge as the Small Model Learnability Gap.</s><s coords="8,271.84,616.69,17.30,9.46;8,70.87,630.24,220.07,9.46;8,70.87,643.79,218.27,9.46;8,70.87,657.34,220.07,9.46;8,70.87,670.89,46.98,9.46">The reason behind it may be that small student models excel on data that closely match their inherent distribution but struggle with significant distribution shifts.</s><s coords="8,121.35,670.89,168.06,9.46;8,70.87,684.44,218.27,9.46;8,70.87,697.99,220.07,9.46;8,70.87,711.54,218.45,9.46;8,70.87,725.09,21.75,9.46">To bridge the gap, we introduced Mix Distillation, including Mix-Long, which combined long CoT and short CoT data in a ratio, and Mix-Large, which integrated large and small teacher CoT.</s><s coords="8,95.44,725.09,195.51,9.46;8,70.87,738.63,218.27,9.46;8,70.87,752.18,220.07,9.46;8,70.87,765.73,220.08,9.46;8,306.14,74.72,40.89,9.46">Experimental results showed that both Mix-Long and Mix-Large outperform baselines across most evaluation metrics, which implied mix distillation outperforms training on a single data dis-tribution.</s><s coords="8,353.10,74.72,171.32,9.46;8,306.14,88.27,218.27,9.46;8,306.14,101.82,188.66,9.46">This paper provided practical insights for optimizing post-training strategies to enhance small language model reasoning capability.</s></p><p><s coords="8,317.05,118.69,207.36,9.46;8,306.14,132.24,55.12,9.46">We will explore several promising directions as future work.</s><s coords="8,365.52,132.24,158.90,9.46;8,306.14,145.79,218.27,9.46;8,306.14,159.34,218.27,9.46;8,306.14,172.89,125.93,9.46">First, we will refine mix distillation by optimally combining diverse data sources and proposing more fine-grained mixing algorithms to boost reasoning capabilities.</s><s coords="8,435.46,172.89,88.94,9.46;8,306.14,186.44,220.07,9.46;8,306.14,199.99,218.27,9.46;8,306.14,213.54,220.08,9.46;8,306.14,227.09,60.18,9.46">Second, we propose to study how strong reasoning teachers can generate data that is better suited for tuning small student models, thereby facilitating more effective knowledge transfer.</s><s coords="8,370.52,227.09,155.69,9.46;8,306.14,240.64,218.27,9.46;8,306.14,254.19,131.66,9.46">Third, we will conduct further theoretical and model interpolability studies on the small model learnability gap.</s><s coords="8,443.25,254.19,82.97,9.46;8,305.87,267.74,218.54,9.46;8,306.14,281.28,218.65,9.46;8,306.14,294.83,168.11,9.46">Lastly, we will investigate which SFT methods yield the best initial policies for subsequent RL procedure, ultimately enhancing overall model performance.</s></p><p><s coords="8,317.05,457.42,207.36,9.46;8,306.14,470.97,218.27,9.46;8,306.14,484.52,151.34,9.46">This work is supported in part by funds provided by the National Science Foundation, Department of Homeland Security, and IBM.</s><s coords="8,461.57,484.52,64.21,9.46;8,306.14,498.07,220.07,9.46;8,306.14,511.62,219.00,9.46;8,306.14,525.17,218.58,9.46;8,306.14,538.72,186.15,9.46">Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the NSF or its federal agency and industry partners.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="8,306.14,572.85,59.12,10.75">Limitations</head><p><s coords="8,305.63,603.14,218.78,9.46;8,306.14,616.69,218.27,9.46;8,306.14,630.24,218.26,9.46;8,306.14,643.79,218.26,9.46;8,306.14,657.34,218.27,9.46;8,306.14,670.89,220.07,9.46;8,306.14,684.44,39.22,9.46">While our study provides valuable insights into the understanding of small model learnability gap in math reasoning, we acknowledge that our research primarily focuses on this specific domain and does not evaluate other crucial skills such as instruction following, code generation, or multi-modal understanding.</s><s coords="8,348.76,684.44,175.65,9.46;8,306.14,697.99,220.07,9.46;8,306.14,711.54,182.17,9.46">We also did not investigate the impact of fine-grained variations in pre-training data composition on the small model learnability gap.</s><s coords="8,491.70,711.54,32.71,9.46;8,306.14,725.09,218.26,9.46;8,306.14,738.63,220.08,9.46;8,306.14,752.18,218.27,9.46;8,306.14,765.73,206.07,9.46">A more detailed analysis of how different pre-training data sources and their proportions affect learning outcomes could offer valuable insights into optimizing data selection strategies for mitigating this gap.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,70.87,73.58,172.68,10.75;12,94.79,109.11,35.35,8.06;12,172.57,109.11,27.89,8.06">A Detailed Experimental Setups Category Models</head><p><s coords="12,149.66,124.77,60.68,8.06">Teacher Models</s></p><p><s coords="12,94.79,140.43,49.32,8.06;12,172.57,140.78,79.49,7.77;12,94.79,150.39,38.36,8.06;12,172.57,150.74,80.93,7.77">Long CoT vs QwQ-32B-Preview vs ShortCoT Qwen2.5-32B-Instruct</s></p><p><s coords="12,94.79,166.06,65.82,8.06;12,94.79,176.02,54.71,8.06;12,94.79,186.17,47.89,7.73;12,172.57,186.33,80.93,7.77">Large Teacher vs Small Teacher Qwen Family Qwen2.5-72B-Instruct</s><s coords="12,255.74,186.33,7.97,7.77;12,172.57,196.29,76.45,7.77">vs Qwen2.5-3B-Instruct</s><s coords="12,94.79,206.09,49.89,7.73">Llama Family</s></p><p><s coords="12,172.57,206.26,92.64,7.77;12,172.57,216.22,77.94,7.77">Llama3.1-70B-Instruct vs Llama3.1-8B-Instruct</s><s coords="12,94.79,226.02,54.86,7.73">Gemma Family</s></p><p><s coords="12,172.57,226.18,68.99,7.77;12,172.57,236.14,54.29,7.77">Gemma2-27B-it vs Gemma2-9B-it</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,149.99,251.46,60.03,8.06">Student Models</head><p><s coords="12,94.79,267.31,47.89,7.73">Qwen Family</s></p><formula xml:id="formula_2" coords="12,94.79,267.47,163.19,97.44">Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct, Qwen2.5-32B-Instruct Llama Family Llama3.2-1B-Instruct, Llama3.2-3B-Instruct, Llama3.1-8B-Instruct, Llama3.3-70B-Instruct</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,70.87,415.98,60.91,9.81">A.1 Models</head><p><s coords="12,70.53,434.27,220.42,9.46;12,70.87,447.82,186.02,9.46">Table <ref type="table" coords="12,97.26,434.27,5.46,9.46" target="#tab_3">4</ref> presents a comprehensive overview of student and teacher models used in our paper.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,70.87,470.46,96.78,9.81">A.2 Training Setup</head><p><s coords="12,70.87,488.75,220.07,9.46;12,70.87,502.30,218.27,9.46;12,70.87,515.85,218.27,9.46;12,70.87,529.40,218.27,9.46;12,70.87,542.95,28.13,9.46">Our model training is conducted using LLaMA-Factory <ref type="bibr" coords="12,109.51,502.30,92.17,9.46" target="#b45">(Zheng et al., 2024)</ref>, on a server with four NVIDIA A100-SXM4-80GB GPUs, an AMD EPYC 7763 64-Core Processor, and 512 GB of RAM.</s><s coords="12,102.70,542.95,188.24,9.46;12,70.87,556.50,171.53,9.46">We use full parameter fine-tuning on student models less than 14B parameters.</s><s coords="12,246.12,556.50,43.02,9.46;12,70.87,570.05,218.66,9.46;12,70.87,583.60,127.57,9.46">When the student model is larger than 14B, we use LoRA fine-tuning <ref type="bibr" coords="12,122.47,583.60,71.18,9.46" target="#b11">(Hu et al., 2021)</ref>.</s><s coords="12,201.95,583.60,87.19,9.46;12,70.87,597.14,218.27,9.46;12,70.87,610.69,153.38,9.46">Table <ref type="table" coords="12,229.17,583.60,5.56,9.46" target="#tab_4">5</ref> and Table <ref type="table" coords="12,283.58,583.60,5.56,9.46" target="#tab_5">6</ref> list hyper-parameters for full parameter fine-tuning and LoRA fine-tuning respectively.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,102.10,638.06,77.15,9.23;12,217.84,638.06,24.72,9.23">Hyper-parameter Value</head><p><s coords="12,102.10,657.18,58.44,8.90;12,217.84,656.85,28.72,9.00;12,317.05,247.07,207.36,9.46;12,306.14,260.62,219.17,9.46;12,305.51,274.17,220.27,9.46;12,306.14,287.72,187.49,9.46">Learning Rate 1 × 10 Teacher models generate responses by rejection sampling <ref type="bibr" coords="12,348.39,260.62,98.61,9.46" target="#b42">(Zelikman et al., 2022;</ref><ref type="bibr" coords="12,449.73,260.62,75.59,9.46" target="#b29">Tong et al., 2024;</ref><ref type="bibr" coords="12,305.51,274.17,70.31,9.46" target="#b40">Yue et al., 2023;</ref><ref type="bibr" coords="12,378.54,274.17,78.70,9.46" target="#b28">Singh et al., 2024;</ref><ref type="bibr" coords="12,459.98,274.17,65.80,9.46;12,306.14,287.72,24.65,9.46" target="#b6">Gulcehre et al., 2023;</ref><ref type="bibr" coords="12,333.53,287.72,76.12,9.46" target="#b39">Yuan et al., 2023;</ref><ref type="bibr" coords="12,412.39,287.72,76.58,9.46" target="#b5">Dong et al., 2023)</ref>.</s><s coords="12,497.04,287.72,29.19,9.46;12,306.14,301.27,202.62,9.46">By default, teacher models employ greedy decoding.</s><s coords="12,512.18,301.27,12.62,9.46;12,306.14,314.82,220.08,9.46;12,306.14,328.37,219.63,9.46;12,305.75,341.92,218.66,9.46;12,306.14,355.46,67.43,9.46">By combining the math problem instructions with corresponding solutions generated by teacher models, we construct problem-solution pairs to fine-tune student models.</s><s coords="12,376.98,355.46,147.43,9.46;12,306.14,369.01,218.27,9.46;12,306.14,382.56,218.27,9.46;12,306.14,396.11,193.03,9.46">We perform pairwise comparisons of solutions generated by different teacher models and filter out problem-solution pairs that are correct for both models to fine-tune student models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,306.14,419.26,107.18,9.81">A.3 Evaluation Setup</head><p><s coords="12,305.63,437.85,218.78,9.46;12,306.14,451.39,220.08,9.46;12,306.14,464.94,219.63,9.46;12,306.14,478.49,218.27,9.46;12,306.14,492.04,220.07,9.46;12,306.14,505.59,104.52,9.46">We evaluate the reasoning capability of fine-tuned student models on a set of commonly used benchmarks, including MATH <ref type="bibr" coords="12,416.58,464.94,104.47,9.46" target="#b8">(Hendrycks et al., 2021)</ref>, GSM8K <ref type="bibr" coords="12,346.49,478.49,86.33,9.46" target="#b2">(Cobbe et al., 2021)</ref>, AMC 2023, AIME 2024, and the English math subset of Olympiad-Bench <ref type="bibr" coords="12,336.74,505.59,69.22,9.46" target="#b7">(He et al., 2024)</ref>.</s></p><p><s coords="12,317.05,519.48,207.36,9.46;12,306.14,533.03,218.65,9.46;12,306.14,546.58,42.76,9.46">Unless otherwise specified, all fine-tuned models are evaluated in a zero-shot setting using greedy decoding.</s><s coords="12,352.28,546.58,172.13,9.46;12,306.14,560.13,30.90,9.46">We set the maximum generation tokens as 16k.</s><s coords="12,340.43,560.13,170.91,9.46">The evaluation prompt is shown below.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,306.99,585.92,32.13,9.46">Prompt</head><p><s coords="12,320.81,602.70,188.93,9.46;12,320.81,616.25,188.92,9.46;12,320.42,630.82,114.64,9.81;12,320.81,644.72,90.22,9.46;12,320.42,658.27,36.96,9.46">Solve the following math problem and present the final answer in the format: Final Answer: {your answer} Problem: {problem} Answer:</s></p><p><s coords="12,317.05,697.99,207.36,9.46;12,306.14,711.54,220.08,9.46;12,306.14,725.09,156.59,9.46">After extracting the final answer of the evaluated model, we first employ exact matching to determine the correctness of the answer.</s><s coords="12,466.42,725.09,58.17,9.46;12,306.14,738.63,218.27,9.46;12,306.14,752.18,218.27,9.46;12,306.14,765.73,84.84,9.46">If the answer is incorrect, we use Qwen-32B-Instruct as a judge to compare the extracted final answers against that of the ground truth.</s><s coords="12,394.36,765.73,123.28,9.46">The prompt is shown below.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,70.87,464.95,155.85,10.75">B More Experiments Results</head><p><s coords="13,70.87,488.73,218.27,9.46;13,70.87,502.28,220.10,9.46">In this section we present additional experiment results of long CoT gap and large teacher CoT gap.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,70.87,526.65,190.08,9.81">B.1 Long CoT Gap: Additional Results</head><p><s coords="13,70.53,545.94,218.61,9.46;13,70.87,559.49,218.27,9.46;13,70.87,573.04,220.17,9.46">Table <ref type="table" coords="13,99.69,545.94,5.56,9.46" target="#tab_6">7</ref> shows the detailed performance scores and gap of each benchmark for different student models fine-tuned on long CoT and short CoT.</s><s coords="13,70.87,586.59,218.61,9.46;13,70.87,600.13,218.26,9.46;13,70.87,613.68,85.36,9.46">QwQ-32B-Preview is chosen to generate long CoT and awhile Qwen-2.5-32B-Instruct is chosen to generate short CoT.</s><s coords="13,158.94,613.68,130.19,9.46;13,70.87,627.23,218.27,9.46;13,70.87,640.78,218.27,9.46;13,70.87,654.33,43.44,9.46">We observe that small student models tend to benefit more from short CoT, while large student models gain greater advantages from long CoT.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,70.87,678.70,196.82,9.81;13,97.23,692.25,33.94,9.81">B.2 Large Teacher CoT Gap: Additional Results</head><p><s coords="13,70.53,711.54,218.60,9.46;13,70.87,725.09,218.27,9.46;13,70.87,738.63,200.49,9.46">Table <ref type="table" coords="13,96.87,711.54,5.38,9.46" target="#tab_7">8</ref> shows the detailed performance scores and gap of each benchmark for different student models distilled from large teacher and small teacher.</s><s coords="13,274.73,738.63,14.40,9.46;13,70.87,752.18,218.27,9.46;13,70.87,765.73,218.27,9.46;13,306.14,277.13,55.22,9.46">We summarize the performance of 10 student models from the Llama and Qwen families across various model sizes.</s><s coords="13,365.55,277.13,158.86,9.46;13,306.14,290.68,218.27,9.46;13,306.14,304.23,141.36,9.46">Qwen-2.5-72B-Instruct is chosen as the large teacher while Qwen-2.5-3B-Instruct is chosen as the small teacher.</s><s coords="13,455.76,304.23,68.66,9.46;13,306.14,317.78,77.13,9.46">The results are shown in Table <ref type="table" coords="13,375.08,317.78,4.10,9.46" target="#tab_7">8</ref>.</s><s coords="13,386.68,317.78,137.73,9.46;13,306.14,331.33,220.07,9.46;13,306.14,344.88,220.07,9.46;13,306.14,358.43,218.27,9.46;13,306.14,371.98,220.15,9.46">Our findings indicate that small student models may experience degraded performance when distilled from a large teacher compared to a small teacher, whereas larger student models benefit more from distilling a large teacher.</s></p><p><s coords="13,317.05,385.53,207.36,9.46;13,306.14,399.08,218.27,9.46;13,306.14,412.63,38.52,9.46;13,368.23,412.63,42.04,9.46;13,429.67,412.87,65.45,8.76;13,514.52,412.63,9.89,9.46;13,306.14,426.17,218.27,9.46">Table <ref type="table" coords="13,351.11,385.53,5.56,9.46" target="#tab_8">9</ref> shows more experiment results for teacher models in different model families, including Gemma-27B-it vs Gemma-9B-it and Llama3.1-72B-Instruct</s><s coords="13,305.87,439.72,124.24,9.46">vs Llama3.1-8B-Instruct.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,306.14,462.34,189.31,10.75">C Examples of Speaking Style Shift</head><p><s coords="13,305.63,484.40,218.78,9.46;13,306.14,497.95,218.27,9.46;13,306.14,511.50,185.54,9.46">We adopt the method from <ref type="bibr" coords="13,433.24,484.40,78.58,9.46" target="#b18">(Lin et al., 2023)</ref> to evaluate the most shifted tokens after fine-tuning on long CoT and Large teacher CoT data.</s><s coords="13,495.36,511.50,29.05,9.46;13,306.14,525.05,140.61,9.46">Figure <ref type="figure" coords="13,306.14,525.05,5.48,9.46">8</ref> shows the calculation process.</s><s coords="13,450.14,525.05,74.27,9.46;13,306.14,538.60,218.65,9.46;13,306.14,552.15,99.14,9.46">This allows us to compare the token distribution shifts induced by the fine-tuning process.</s><s coords="13,408.49,552.15,115.92,9.46;13,306.14,565.70,218.27,9.46;13,306.14,579.25,31.72,9.46">We annotate the tokens that exhibit the largest rank shifts as the most shifted tokens.</s><s coords="13,341.37,579.25,183.04,9.46;13,306.14,592.80,65.52,9.46">We choose Qwen2.5-3B-Instruct as the student model.</s><s coords="13,375.16,592.80,149.25,9.46;13,306.14,606.34,218.27,9.46;13,306.14,619.89,41.15,9.46">We put the results of most shifted tokens after fine-tuning on long CoT data in Figure <ref type="figure" coords="13,306.14,619.89,5.56,9.46" target="#fig_6">9</ref> and 10.</s><s coords="13,350.90,619.89,173.70,9.46;13,306.14,633.44,218.27,9.46;13,306.14,646.99,46.00,9.46">The results of most shifted tokens after fine-tuning on large teacher CoT data are shown in Figure <ref type="figure" coords="13,338.23,646.99,9.27,9.46" target="#fig_7">11</ref>.</s><s coords="13,356.47,646.99,167.94,9.46;13,306.14,660.54,218.27,9.46;13,306.14,674.09,220.18,9.46">Our analysis reveals that these tokens are predominantly associated with expressive and stylistic elements, such as "wait", "But", and "Let".</s><s coords="14,70.87,301.23,453.54,8.64;14,70.87,313.18,30.92,8.64">Qwen-2.5-72B-Instruct is chosen as the large teacher while Qwen-2.5-3B-Instruct is chosen as the small teacher.</s><s coords="14,104.88,313.18,419.53,8.64;14,70.87,325.14,453.52,8.64">We observe that small student models may experience degraded performance when distilled from a large teacher compared to a small teacher, whereas larger student models benefit more from the distilling a large teacher.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,306.14,696.71,178.88,10.75">D Examples of Various CoT Data</head><p><s coords="14,176.77,504.63,97.85,6.94;14,70.87,660.49,250.45,8.64">Gemma2-9B vs Gemma2-27B Gemma-27B-it vs Gemma-9B-it and Llama3.1-72B-Instruct</s><s coords="14,323.44,660.49,112.87,8.64">vs Llama3.1-8B-Instruct.</s><s coords="14,439.27,660.49,85.14,8.64;14,70.87,672.45,453.54,8.64;14,70.87,684.40,343.43,8.64">We observe that small student models may experience degraded performance when distilled from a large teacher compared to a small teacher, whereas larger student models benefit more from the distilling a large teacher.</s></p><p><s coords="15,70.87,381.80,221.68,8.64">Figure <ref type="figure" coords="15,98.75,381.80,3.80,8.64">8</ref>: The process of calculating most shifted tokens.</s><s coords="15,295.59,381.80,228.82,8.64;15,70.87,393.76,144.64,8.64">We decode each token generated by the fine-tuned LLM in the student model before fine-tuning.</s><s coords="15,218.59,393.76,305.83,8.64;15,70.87,405.71,97.74,8.64">Then we calculate the rank shift in the student model for each token generated by the fine-tuned model.</s><s coords="15,171.69,405.71,336.54,8.64">We annotate the tokens that exhibit the largest rank shifts as the most shifted tokens.</s><s coords="15,511.32,405.71,13.08,8.64;15,70.87,417.67,453.54,8.64;15,69.54,429.62,24.62,8.64">We found that these tokens are predominantly associated with expressive and stylistic elements, such as "But" and "Let".</s></p><p><s coords="15,71.72,629.26,36.97,9.46;15,85.54,645.13,329.43,9.81;15,414.97,643.18,4.23,6.99;15,422.54,645.13,16.67,9.57;15,439.59,643.18,4.23,6.99;15,448.38,645.13,63.27,9.57">Problem Let (x, y) be an ordered pair of real numbers that satisfies the equation x 2 + y 2 = 14x + 48y.</s><s coords="15,85.03,658.68,147.87,9.81">What is the maximum value of y?</s></p><p><s coords="16,71.72,92.12,292.90,9.46">Example of most shifted tokens after fine-tuning on long CoT data.</s><s coords="16,368.00,92.12,21.21,9.46;16,85.54,108.89,158.82,9.46">(1/2) So I've got this math problem here.</s><s coords="16,248.70,108.89,261.05,9.46;16,85.54,122.09,97.92,9.81;16,183.46,120.14,4.23,6.99;16,190.32,122.09,15.96,9.57;16,206.67,120.14,4.23,6.99;16,214.44,122.09,60.72,9.57">It says: "Let (x, y) be an ordered pair of real numbers that satisfies the equation x 2 + y 2 = 14x + 48y.</s><s coords="16,278.52,122.09,231.62,9.81;16,85.54,135.99,72.41,9.46">What is the maximum value of y? Okay, let's see how to approach this.</s><s coords="16,85.54,149.54,253.06,9.46">First, I need to understand what the equation is telling me.</s><s coords="16,341.98,149.19,167.75,9.81;16,85.54,163.09,90.29,9.46">It's an equation involving x and y, and it's set equal to zero:</s></p><p><s coords="16,238.51,176.29,6.23,9.57;16,244.74,173.79,4.23,6.99;16,251.90,176.29,16.26,9.57;16,268.55,173.79,4.23,6.99;16,275.70,176.29,81.07,9.57">x 2 + y 2 -14x -48y = 0.</s></p><p><s coords="16,85.54,196.32,389.82,9.81">I need to find the maximum value of y for real numbers x and y that satisfy this equation.</s><s coords="16,85.54,210.21,321.84,9.46">Hmm, maybe I can rearrange this equation to make it easier to work with.</s><s coords="16,410.76,210.21,41.51,9.46">Let's see:</s></p><p><s coords="16,238.51,234.37,6.23,9.57;16,244.74,231.88,4.23,6.99;16,251.90,234.37,46.73,9.57;16,299.02,231.88,4.23,6.99;16,306.18,234.37,50.59,9.57">x 2 -14x + y 2 -48y = 0.</s></p><p><s coords="16,85.54,258.88,214.65,9.81">I can complete the square for both x and y terms.</s><s coords="16,85.54,272.43,36.82,9.81;16,122.35,270.48,4.23,6.99;16,129.50,272.43,382.15,9.81">For x: x 2 -14x, to complete the square, I take half of -14, which is -7, and square it to get 49.</s><s coords="16,85.54,285.98,23.21,9.81;16,108.75,284.03,4.23,6.99;16,115.90,285.98,105.19,9.81;16,221.09,284.03,4.23,6.99;16,228.25,285.98,24.55,9.57">So, x 2 -14x becomes (x -7) 2 -49.</s></p><p><s coords="16,85.54,299.53,35.22,9.81;16,121.15,297.58,4.23,6.99;16,128.30,299.53,187.24,9.81">For y: y 2 -48y, half of -48 is -24, squared is 576.</s><s coords="16,318.93,299.53,22.32,9.81;16,341.64,297.58,4.23,6.99;16,348.80,299.53,109.65,9.81;16,458.45,297.58,4.23,6.99;16,465.60,299.53,30.00,9.57">So, y 2 -48y becomes (y -24) 2 -576.</s><s coords="16,85.54,313.43,192.74,9.46">Now, plugging these back into the equation:</s></p><formula xml:id="formula_3" coords="16,85.54,335.09,297.97,70.39">(x -7) 2 -49 + (y -24) 2 -576 = 0. Simplify: (x -7) 2 + (y -24) 2 -625 = 0, So, (x -7) 2 + (y -24) 2 = 625.</formula><p><s coords="16,85.20,415.69,326.20,9.81">This looks like the equation of a circle with center at (7, 24) and radius 25.</s></p><p><s coords="16,85.03,429.24,390.85,9.81">Wait, but the problem is asking for the maximum value of y, and I have a circle equation.</s><s coords="16,479.26,429.59,30.48,9.46;16,85.54,442.79,424.20,9.81;16,85.54,456.69,45.45,9.46">So, the maximum y would be the y-coordinate of the highest point on the circle, which is the center plus the radius.</s></p><p><s coords="16,85.03,469.89,424.72,9.81;16,84.26,483.44,30.61,9.57">Wait, but the center is at (7,24), and the radius is 25, so the highest point would be at (7,24 + 25) = (7,49).</s><s coords="16,85.54,497.34,197.14,9.46">But, let me double-check if this makes sense.</s><s coords="16,286.06,497.34,67.27,9.46">The equation is</s></p><p><s coords="16,247.90,521.50,6.23,9.57;16,254.14,519.00,4.23,6.99;16,261.29,521.50,16.26,9.57;16,277.94,519.00,4.23,6.99;16,285.70,521.50,61.67,9.57;16,85.54,546.35,119.27,9.46;16,233.28,559.55,33.51,9.57;16,266.79,557.06,4.23,6.99;16,273.95,559.55,49.38,9.57;16,323.32,557.06,4.23,6.99;16,331.09,559.55,30.91,9.57;16,85.15,579.58,251.16,9.81">x 2 + y 2 = 14x + 48y, and I've transformed it into (x -7) 2 + (y -24) 2 = 625, which is indeed a circle with center (7, 24) and radius 25.</s><s coords="16,85.54,593.13,194.72,9.81">So, the maximum y would be 24 + 25 = 49.</s><s coords="16,85.54,606.68,374.24,9.81">But, wait, the problem says "the maximum value of y," and I've found that to be 49.</s><s coords="16,463.35,607.03,46.39,9.46;16,85.54,620.58,305.13,9.46">But let me consider if there are any other constraints or if I've missed something.</s></p><p><s coords="16,85.54,634.12,178.78,9.46">Let me think about the original equation:</s></p><p><s coords="16,247.90,658.28,6.23,9.57;16,254.14,655.79,4.23,6.99;16,261.29,658.28,16.26,9.57;16,277.94,655.79,4.23,6.99;16,285.70,658.28,61.67,9.57">x 2 + y 2 = 14x + 48y.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="16,85.54,683.14,88.45,9.46">I can rearrange it to:</head><p><s coords="16,238.51,696.34,6.23,9.57;16,244.74,693.84,4.23,6.99;16,251.90,696.34,46.73,9.57;16,299.02,693.84,4.23,6.99;16,306.18,696.34,50.59,9.57">x 2 -14x + y 2 -48y = 0.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="18,71.72,98.11,307.28,9.46">Example of Most Shifted Tokens after fine-tuning on large model CoT</head><p><s coords="18,85.20,114.88,239.72,9.46">To solve the problem, we start with the given equation:</s></p><p><s coords="18,252.45,141.63,6.23,9.57;18,258.68,139.14,4.23,6.99;18,265.84,141.63,16.26,9.57;18,282.49,139.14,4.23,6.99;18,287.22,141.63,55.22,9.57">x 2 + y 2 =14x + 48y</s></p><p><s coords="18,85.54,162.01,266.32,9.46">First, we rearrange the equation to bring all terms to oneside:</s></p><p><s coords="18,240.02,188.76,6.23,9.57;18,246.26,186.26,4.23,6.99;18,253.41,188.76,46.73,9.57;18,300.54,186.26,4.23,6.99;18,307.70,188.76,47.56,9.57">x 2 -14x + y 2 -48y = 0</s></p><p><s coords="18,85.54,208.78,205.40,9.81">Next, we complete the square for both x and y.</s><s coords="18,85.54,222.33,26.98,9.81">For x:</s></p><p><s coords="18,276.92,235.88,6.23,9.57;18,283.15,233.38,4.23,6.99;18,290.31,235.88,28.05,9.57">x 2 -14x</s></p><p><s coords="18,85.20,256.25,193.63,9.46;18,287.75,253.62,8.47,6.99;18,289.86,261.67,4.23,6.99;18,302.41,251.63,4.23,6.99;18,310.17,255.90,25.46,9.57">To complete the square, we add and subtract 14 2 2 = 49:</s></p><formula xml:id="formula_4" coords="18,252.67,277.92,89.93,36.57">x 2 -14x + 49 -49 (x -7) 2 -49</formula><p><s coords="18,85.54,324.95,26.49,9.81;18,277.41,338.49,5.35,9.57;18,283.15,336.00,4.23,6.99;18,290.31,338.49,27.17,9.57">For y: y 2 -48y</s></p><p><s coords="18,85.20,358.87,193.63,9.46;18,287.75,356.23,8.47,6.99;18,289.86,364.29,4.23,6.99;18,302.41,354.25,4.23,6.99;18,310.17,358.52,30.91,9.57">To complete the square, we add and subtract 48 2 2 = 576:</s></p><formula xml:id="formula_5" coords="18,247.71,380.53,99.85,36.57">y 2 -48y + 576 -576 (y -24) 2 -576</formula><p><s coords="18,85.54,427.91,279.39,9.46">Now, substitute these completed squares back into the equation:</s></p><formula xml:id="formula_6" coords="18,213.28,449.57,168.72,56.60">(x -7) 2 -49 + (y -24) 2 -576 = 0 (x -7) 2 + (y -24) 2 -625 = 0 (x -7) 2 + (y -24) 2 = 625</formula><p><s coords="18,85.20,516.63,280.25,9.81">This is the equation of a circle with center (7, 24) and radius 25.</s></p><p><s coords="18,85.20,530.18,345.32,9.81">To find the maximum value of y, we need to find the highest point on the circle.</s><s coords="18,433.93,530.53,75.81,9.46;18,85.54,543.73,312.50,9.81">The highest point on the circle is directly above the center, which is at y = 24 + 25 = 49.</s><s coords="18,85.20,557.27,146.98,9.81;18,282.04,570.82,31.20,9.57">Thus, the maximum value of y is: y = 49</s></p><p><s coords="18,85.54,590.85,79.79,9.81;20,85.54,284.42,125.31,9.46">Final Answer: 49 Next, we sum these divisors:</s></p><formula xml:id="formula_7" coords="20,251.27,311.16,92.73,9.57">1 + 3 + 5 + 15 = 24</formula><p><s coords="20,85.20,331.19,241.99,9.81">Therefore, the sum of the positive odd divisors of 60 is:</s></p><p><s coords="20,292.18,358.29,10.91,9.57">24</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="20,71.72,601.72,36.97,9.46">Problem</head><p><s coords="20,85.54,615.89,123.03,9.81;20,225.68,639.32,97.36,10.71;20,323.04,638.51,4.23,6.99;20,330.20,640.46,16.36,9.57;20,254.81,647.66,9.09,9.57;20,263.90,655.72,53.53,10.71;20,350.79,647.99,20.00,9.57">Find the greatest a such that 7 (2a) 2 + (1) 2 -4a 2 -1 √ 1 + 4a 2 + 3 = 2.</s></p><formula xml:id="formula_8" coords="21,189.79,166.58,73.82,28.17">√ (2a) 2 +1 2 -4a 2 -1 √ 1+4a 2 +3</formula><p><s coords="21,268.55,180.44,241.20,9.81;21,85.54,195.44,125.74,9.46">= 2, we start by simplifying the expression inside the square root in the numerator.</s><s coords="21,214.66,195.44,101.19,9.46">The equation becomes:</s></p><formula xml:id="formula_9" coords="21,238.96,210.50,118.56,35.02">7 √ 4a 2 + 1 -4a 2 -1 √ 1 + 4a 2 + 3 = 2</formula><p><s coords="21,85.54,254.77,74.11,9.81;21,162.67,245.57,9.09,9.57;21,171.76,253.63,37.47,10.95">Let's denote x = √ 1 + 4a 2 .</s><s coords="21,212.61,255.12,150.89,9.46">Then the equation transforms into:</s></p><formula xml:id="formula_10" coords="21,257.76,276.46,80.96,26.38">7x -4a 2 -1 x + 3 = 2</formula><p><s coords="21,85.54,309.05,274.23,9.81;21,241.00,336.15,36.24,9.57;21,277.25,333.65,4.23,6.99;21,284.40,336.15,69.87,9.57">Next, we clear the fraction by multiplying both sides by x + 3: 7x -4a 2 -1 = 2(x + 3)</s></p><p><s coords="21,85.54,356.52,215.75,9.46">Expanding and simplifying the right side, we get:</s></p><formula xml:id="formula_11" coords="21,245.25,380.78,104.78,12.06">7x -4a 2 -1 = 2x + 6</formula><p><s coords="21,85.54,403.30,159.25,9.81;21,244.79,401.35,4.23,6.99;21,249.52,403.65,44.15,9.46">Rearranging terms to isolate x and a 2 , we have:</s></p><p><s coords="21,91.98,445.28,61.27,9.57;21,153.25,442.78,4.23,6.99;21,160.40,445.28,121.09,9.57;21,281.49,442.78,4.23,6.99;21,288.65,445.28,103.51,9.57;21,392.17,442.78,4.23,6.99;21,399.32,445.28,63.81,9.57;21,467.36,437.90,11.22,9.57;21,478.58,435.95,4.23,6.99;21,485.74,437.90,16.36,9.57;21,482.00,452.76,5.45,9.57">7x -2x -4a 2 -1 -6 = 0 =⇒ 5x -4a 2 -7 = 0 =⇒ 5x = 4a 2 + 7 =⇒ x = 4a 2 + 7 5</s></p><p><s coords="21,85.54,474.79,67.44,9.81;21,156.01,465.59,9.09,9.57;21,165.10,473.64,37.47,10.95">Recall that x = √ 1 + 4a 2 .</s><s coords="21,205.95,475.14,87.15,9.46">Therefore, we have:</s></p><p><s coords="21,259.88,504.66,46.26,10.71;21,310.37,498.43,11.22,9.57;21,321.59,496.48,4.23,6.99;21,328.74,498.43,16.36,9.57;21,325.01,513.29,5.45,9.57;21,85.20,528.51,221.21,9.46">1 + 4a 2 = 4a 2 + 7 5 To eliminate the square root, we square both sides:</s></p><p><s coords="21,244.03,561.16,30.01,9.57;21,274.04,558.66,4.23,6.99;21,281.80,561.16,8.49,9.57;21,302.54,553.78,11.22,9.57;21,313.77,551.83,4.23,6.99;21,320.92,553.78,16.36,9.57;21,317.19,568.65,5.45,9.57">1 + 4a 2 = 4a 2 + 7 5</s></p><p><s coords="21,346.51,549.85,4.23,6.99">2</s></p><p><s coords="21,85.54,586.74,144.23,9.46;22,119.06,520.12,4.23,6.99;22,128.02,514.70,2.73,9.46">Expanding the right side, we get: 2 .</s><s coords="22,134.13,514.35,110.01,9.81;22,246.87,505.32,9.09,9.57;22,255.96,514.35,8.18,9.57;22,85.20,527.90,211.51,9.81">The greatest value of a is √ 2. Thus, the greatest a that satisfies the equation is:</s></p><formula xml:id="formula_12" coords="21,199.16,610.03,65.95,16.95">1 + 4a 2 = (<label>4a</label></formula><formula xml:id="formula_13" coords="22,290.37,546.70,14.55,19.12">√ 2 √ (2a) 2 +(1) 2 -4a 2 -1 √ 1+4a 2 +3</formula><p><s coords="23,272.75,150.01,236.98,9.81;23,85.54,165.01,150.88,9.46">= 2, we start by simplifying the expression inside the square root and the equation itself.</s><s coords="23,85.54,178.56,300.11,9.46">First, simplify the expression under the square root in the numerator:</s></p><p><s coords="23,249.02,202.72,19.71,9.57;23,268.72,200.22,4.23,6.99;23,275.88,202.72,16.36,9.57;23,292.24,200.22,4.23,6.99;23,300.00,202.72,22.74,9.57;23,322.74,200.22,4.23,6.99;23,329.90,202.72,16.36,9.57;23,85.54,227.57,111.81,9.46">(2a) 2 + 1 2 = 4a 2 + 1 So the equation becomes:</s></p><formula xml:id="formula_14" coords="23,238.96,231.45,118.56,35.02">7 √ 4a 2 + 1 -4a 2 -1 √ 4a 2 + 1 + 3 = 2</formula><p><s coords="23,85.54,275.72,57.89,9.81;23,146.45,266.52,9.09,9.57;23,155.54,274.58,37.47,10.71">Next, let x = √ 4a 2 + 1.</s><s coords="23,196.39,276.07,150.89,9.46">Then the equation transforms into:</s></p><formula xml:id="formula_15" coords="23,257.76,297.18,80.96,26.38">7x -4a 2 -1 x + 3 = 2</formula><p><s coords="23,85.54,336.93,44.72,9.81;23,133.29,327.73,9.09,9.57;23,142.38,335.79,120.01,10.95;23,262.39,334.98,4.23,6.99;23,269.85,336.93,59.26,9.81">Since x = √ 4a 2 + 1, we can express 4a 2 in terms of x:</s></p><formula xml:id="formula_16" coords="23,85.54,358.94,264.32,74.85">4a 2 = x 2 -1 Substitute 4a 2 = x 2 -1 into the equation: 7x -(x 2 -1) -1 x + 3 = 2</formula><p><s coords="23,85.54,444.83,104.85,9.46;23,235.59,458.03,31.26,9.57;23,266.85,455.54,4.23,6.99;23,274.00,458.03,80.95,9.57;23,354.96,455.54,4.23,6.99">Simplify the numerator: 7x -x 2 + 1 -1 = 7x -x 2</s></p><p><s coords="23,85.54,478.41,111.81,9.46;23,269.64,490.51,31.26,9.57;23,300.90,488.56,4.23,6.99">So the equation becomes: 7x -x 2</s></p><p><s coords="23,275.13,505.37,25.02,9.57;23,309.86,497.89,16.97,9.57">x + 3 = 2</s></p><p><s coords="23,85.54,521.15,238.35,9.81">Multiply both sides by x + 3 to clear the denominator:</s></p><p><s coords="23,252.89,545.66,31.26,9.57;23,284.15,543.16,4.23,6.99;23,291.91,545.66,50.47,9.57">7x -x 2 = 2(x + 3)</s></p><p><s coords="23,85.54,570.52,93.94,9.46;23,257.13,583.72,31.26,9.57;23,288.39,581.22,4.23,6.99;23,296.15,583.72,41.99,9.57">Expand and simplify: 7x -x 2 = 2x + 6</s></p><p><s coords="23,85.54,604.09,138.46,9.46">Rearrange all terms to one side:</s></p><p><s coords="23,243.50,628.25,14.72,9.57;23,258.22,625.75,4.23,6.99;23,265.37,628.25,86.41,9.57;23,256.01,652.76,14.72,9.57;23,270.73,650.26,4.23,6.99;23,277.88,652.76,61.39,9.57">-x 2 + 7x -2x -6 = 0 -x 2 + 5x -6 = 0</s></p><p><s coords="23,85.54,673.13,277.05,9.46">Multiply through by -1 to make the leading coefficient positive:</s></p><p><s coords="23,260.25,697.29,6.23,9.57;23,266.48,694.79,4.23,6.99;23,273.64,697.29,61.39,9.57">x 2 -5x + 6 = 0</s></p><p><s coords="24,71.72,130.06,150.12,9.46">Small Teacher CoT Example (2/2)</s></p><p><s coords="24,85.54,146.83,130.42,9.46;24,254.13,160.03,87.02,9.57">Factor the quadratic equation: (x -2)(x -3) = 0</s></p><p><s coords="24,85.54,180.41,91.52,9.46;24,255.95,193.61,83.38,9.81">So, the solutions are: x = 2 or x = 3</s></p><p><s coords="24,85.54,213.63,67.44,9.81;24,156.01,204.43,9.09,9.57;24,165.10,212.49,37.47,10.71">Recall that x = √ 4a 2 + 1.</s><s coords="24,205.95,213.98,144.26,9.46;24,84.72,227.53,8.18,9.46">We need to check both solutions: 1.</s><s coords="24,96.28,227.18,46.98,9.81;24,275.72,239.59,54.74,10.71">For x = 2: 4a 2 + 1 = 2</s></p><p><s coords="24,85.54,261.10,80.00,9.46">Square both sides:</s></p><formula xml:id="formula_17" coords="24,85.54,271.81,244.93,122.10">4a 2 + 1 = 4 4a 2 = 3 a 2 = 3 4 a = ± √ 3 2 2. For x = 3: 4a 2 + 1 = 3</formula><p><s coords="24,85.54,404.72,80.00,9.46">Square both sides:</s></p><formula xml:id="formula_18" coords="24,270.27,415.42,54.74,72.14">4a 2 + 1 = 9 4a 2 = 8 a 2 = 2 a = ± √ 2</formula><p><s coords="24,85.20,498.02,200.62,9.81;24,288.55,488.99,9.09,9.57;24,297.64,498.02,8.18,9.57;24,85.20,511.92,109.39,9.46">The greatest value of a among the solutions is √ 2. Thus, the final answer is:</s></p><p><s coords="24,290.37,518.40,9.09,9.57;24,299.46,527.96,5.45,9.57;24,71.72,677.86,36.97,9.46;24,85.03,692.38,255.15,9.46">√ 2 Problem What is the greatest common divisor of 128, 144 and 480?</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,70.87,458.44,453.71,9.65;3,70.54,470.71,453.88,8.64;3,70.87,482.67,453.54,8.64;3,70.54,494.30,453.87,9.65;3,70.87,506.26,453.55,9.65;3,70.87,518.21,233.96,9.65"><head>Figure 3 :</head><label>3</label><figDesc><div><p><s coords="3,70.87,458.44,453.71,9.65;3,70.54,470.71,139.44,8.64">Figure3: Large model CoT Gap (∆ Large = P Large -P Small ) of student models with different models sizes for (a) Qwen family (b) Llama family.</s><s coords="3,213.07,470.71,311.34,8.64;3,70.87,482.67,414.45,8.64">For teacher models, Qwen2.5-72B-Instruct is chosen as the large teacher to generate responses, while Qwen2.5-3B-Instruct is chosen as the small teacher to generate responses.</s><s coords="3,488.43,482.67,35.98,8.64;3,70.54,494.30,357.93,9.65">Negative (positive) ∆ Large indicates that large teacher CoT is worse (better) than small teacher CoT.</s><s coords="3,430.95,494.62,93.45,8.64;3,70.87,506.26,453.55,9.65;3,70.87,518.21,233.96,9.65">Our results demonstrate that small teacher CoT is better for smaller student models (indicated by ∆ Large &lt; 0), while large model CoT is better for larger student models (indicated by ∆ Large &gt; 0).</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,70.87,194.75,453.55,8.64;6,70.87,206.70,453.54,8.64;6,70.87,218.66,288.22,8.64"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc><div><p><s coords="6,70.87,194.75,409.62,8.64">Figure 4: Math expert models usually have a less significant Learnability Gap than the general models.</s><s coords="6,483.58,194.75,40.84,8.64;6,70.87,206.70,315.35,8.64">A positive Gap means long CoT or large teacher CoT is better while negative means worse.</s><s coords="6,389.31,206.70,135.10,8.64;6,70.87,218.66,288.22,8.64">This indicates that the math expert model could more easily learn from long CoT data or large teacher CoT.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,90.17,540.03,152.12,9.46;6,86.46,560.06,188.90,9.46;6,86.46,573.61,187.08,9.46;6,86.46,587.15,169.88,9.46"><head>Takeaway 5 :</head><label>5</label><figDesc><div><p><s coords="6,148.34,540.03,93.95,9.46;6,86.46,560.06,188.90,9.46;6,86.46,573.61,187.08,9.46;6,86.46,587.15,169.88,9.46">Speaking Styles ShiftLong CoT and large teacher CoT primarily shift the student model's distribution of tokens associated with speaking styles.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,70.87,190.92,218.27,8.64;7,70.87,202.87,220.01,8.64;7,70.87,214.83,220.01,8.64;7,70.51,226.78,218.63,8.64;7,70.87,238.74,85.95,8.64;7,306.14,70.86,226.77,226.47"><head>Figure 6 :</head><label>6</label><figDesc><div><p><s coords="7,70.87,190.92,218.27,8.64;7,70.87,202.87,220.01,8.64">Figure 6: The average performance varies with the mix weight of long CoT or large teacher CoT data.</s><s coords="7,70.87,214.83,220.01,8.64">Qwen2.5-3B-Instruct is chosen as the student model.</s><s coords="7,70.51,226.78,218.63,8.64;7,70.87,238.74,85.95,8.64">At a weight of 0.2, mix distillation achieves the highest average performance.</s></p></div></figDesc><graphic coords="7,306.14,70.86,226.77,226.47" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,306.14,309.18,218.27,8.64;7,306.14,321.13,218.27,8.64;7,306.14,333.09,219.52,8.64;7,306.14,345.04,219.92,8.64;7,306.14,357.00,218.27,8.64;7,306.14,368.95,131.83,8.64"><head>Figure 7 :</head><label>7</label><figDesc><div><p><s coords="7,306.14,309.18,141.97,8.64">Figure 7: Case Study of Mix-Long.</s><s coords="7,451.21,309.18,73.20,8.64;7,306.14,321.13,218.27,8.64;7,306.14,333.09,170.52,8.64">Models fine-tuned on long CoT tended to overthink, while those trained on short CoT produced incorrect answers.</s><s coords="7,479.93,333.09,45.73,8.64;7,306.14,345.04,219.92,8.64;7,306.14,357.00,218.27,8.64;7,306.14,368.95,131.83,8.64">In contrast, Mix-Long, incorporating branching elements (e.g., "Alternatively"), achieved a balanced reasoning process and arrived at the correct answer.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="16,70.87,733.54,453.55,8.64;16,70.87,745.49,26.00,8.64"><head>Figure 9 :</head><label>9</label><figDesc><div><p><s coords="16,70.87,733.54,300.48,8.64">Figure 9: Example of most shifted tokens after fine-tuning on long CoT data.</s><s coords="16,374.41,733.54,150.00,8.64;16,70.87,745.49,26.00,8.64">All the most shifted tokens are labeled as red.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="18,70.87,629.31,453.54,8.64;18,70.87,641.26,57.26,8.64"><head>Figure 11 :</head><label>11</label><figDesc><div><p><s coords="18,70.87,629.31,328.09,8.64">Figure 11: Example of most shifted tokens after fine-tuning on large teacher CoT.</s><s coords="18,401.44,629.31,122.97,8.64;18,70.87,641.26,57.26,8.64">All the most shifted tokens are labeled as red.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="20,236.53,396.75,122.21,8.64"><head>Figure 13 :</head><label>13</label><figDesc><div><p><s coords="20,236.53,396.75,122.21,8.64">Figure 13: Short CoT example</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="21,265.10,608.08,4.23,6.99;21,272.26,610.03,20.60,9.57;21,292.86,608.08,4.23,6.99;21,268.16,624.90,10.91,9.57;21,301.82,617.41,8.49,9.57;21,314.53,610.03,16.68,9.57;21,331.21,608.08,4.23,6.99;21,338.37,610.03,27.58,9.57;21,365.95,608.08,4.23,6.99;21,373.11,610.03,21.82,9.57;21,349.27,624.90,10.91,9.57;21,85.54,640.11,285.92,9.46;21,224.62,666.86,46.37,9.57;21,270.99,664.37,4.23,6.99;21,278.75,666.86,28.19,9.57;21,306.94,664.37,4.23,6.99;21,314.10,666.86,27.58,9.57;21,341.68,664.37,4.23,6.99;21,348.84,666.86,21.82,9.57;22,71.72,263.89,149.90,9.46;22,85.54,280.67,321.77,9.46;22,150.18,307.42,16.68,9.57;22,166.86,304.92,4.23,6.99;22,174.01,307.42,27.58,9.57;22,201.60,304.92,4.23,6.99;22,208.75,307.42,57.28,9.57;22,266.03,304.92,4.23,6.99;22,273.19,307.42,88.19,9.57;22,361.38,304.92,4.23,6.99;22,368.54,307.42,27.58,9.57;22,396.12,304.92,4.23,6.99;22,403.28,307.42,41.82,9.57;22,85.54,327.44,43.32,9.81;22,128.86,325.49,4.23,6.99;22,133.59,327.44,199.39,9.81;22,249.84,354.54,16.26,9.57;22,266.48,352.04,4.23,6.99;22,273.64,354.54,71.80,9.57;22,85.03,375.84,296.11,9.81;22,386.43,373.53,16.80,6.99;22,403.23,366.86,7.06,6.99;22,410.29,372.62,26.76,7.90;22,407.37,381.61,8.73,6.99;22,438.24,375.84,72.86,9.81;22,85.54,389.39,95.76,9.81;22,111.26,421.72,17.26,9.57;22,132.74,413.05,124.13,10.71;22,182.68,429.20,24.24,9.57;22,261.09,421.72,8.49,9.57;22,273.80,414.34,21.82,9.57;22,298.04,405.77,9.09,9.57;22,307.13,414.34,56.97,9.57;22,313.50,429.20,10.91,9.57;22,368.33,421.72,8.49,9.57;22,381.04,414.34,21.82,9.57;22,405.28,405.32,9.09,9.57;22,414.37,414.34,16.36,9.57;22,400.43,429.20,10.91,9.57;22,434.96,421.72,8.49,9.57;22,447.67,414.34,35.15,9.57;22,459.79,429.20,10.91,9.57;22,85.20,444.07,145.20,9.81;22,224.07,473.03,17.26,9.57;22,245.55,465.65,10.91,9.57;22,245.55,480.51,10.91,9.57;22,260.69,473.03,71.80,9.81;22,336.71,465.65,10.91,9.57;22,336.71,480.51,10.91,9.57;22,351.84,473.03,8.49,9.57;22,364.55,465.65,5.45,9.57;22,364.55,480.51,5.45,9.57;22,85.54,497.88,56.29,9.81;22,141.83,495.93,4.23,6.99;22,146.56,497.88,52.26,9.81;22,198.82,495.93,4.23,6.99;22,207.67,497.88,39.74,9.81;22,247.42,495.93,4.23,6.99;22,256.28,497.88,8.49,9.57;22,270.08,495.59,4.23,6.99;22,270.08,503.65,4.23,6.99;22,275.51,497.88,200.29,9.81;22,475.81,488.85,9.09,9.57;22,484.90,497.88,24.84,9.81;22,85.54,514.35,28.80,9.57;22,115.53,505.36,7.06,6.99;22,122.59,511.96,4.23,6.99"><head/><label/><figDesc><div><p><s coords="21,187.07,640.11,184.39,9.46;21,224.62,666.86,46.37,9.57;21,270.99,664.37,4.23,6.99;21,278.75,666.86,28.19,9.57;21,306.94,664.37,4.23,6.99;21,314.10,666.86,27.58,9.57;21,341.68,664.37,4.23,6.99;21,348.84,666.86,21.82,9.57;22,71.72,263.89,149.90,9.46;22,85.54,280.67,321.77,9.46;22,150.18,307.42,16.68,9.57;22,166.86,304.92,4.23,6.99;22,174.01,307.42,27.58,9.57;22,201.60,304.92,4.23,6.99;22,208.75,307.42,57.28,9.57;22,266.03,304.92,4.23,6.99;22,273.19,307.42,88.19,9.57;22,361.38,304.92,4.23,6.99;22,368.54,307.42,27.58,9.57;22,396.12,304.92,4.23,6.99;22,403.28,307.42,41.82,9.57;22,85.54,327.44,43.32,9.81;22,128.86,325.49,4.23,6.99;22,133.59,327.79,2.73,9.46">by 25 to clear the denominator, we obtain:25 + 100a 2 = 16a 4 + 56a 2 + 49 Large Teacher CoT Example (2/2)Rearranging all terms to one side, we get a standard polynomial equation:16a 4 + 56a 2 + 49 -100a 2 -25 = 0 =⇒ 16a 4 -44a 2 + 24 = 0Let y = a 2 .</s><s coords="22,139.70,327.44,193.28,9.81;22,249.84,354.54,16.26,9.57;22,266.48,352.04,4.23,6.99;22,273.64,354.54,71.80,9.57;22,85.03,375.84,296.11,9.81;22,386.43,373.53,16.80,6.99;22,403.23,366.86,7.06,6.99;22,410.29,372.62,26.76,7.90;22,407.37,381.61,8.73,6.99;22,438.24,375.84,72.86,9.81;22,85.54,389.39,95.76,9.81;22,85.54,497.88,56.29,9.81;22,141.83,495.93,4.23,6.99;22,146.56,497.88,52.26,9.81;22,198.82,495.93,4.23,6.99;22,207.67,497.88,39.74,9.81;22,247.42,495.93,4.23,6.99;22,256.28,497.88,8.49,9.57;22,270.08,495.59,4.23,6.99;22,270.08,503.65,4.23,6.99;22,275.51,498.23,2.78,9.46">Then the equation becomes a quadratic in y:16y 2 -44y + 24 = 0We solve this quadratic equation using the quadratic formula y= -b± √ b 2 -4ac2a, where a = 16, b = -44, and c = 24:Since y = a 2 , we have a 2 = 2 or a 2 = 3 4 .</s><s coords="22,283.46,497.88,179.74,9.81">The corresponding values for a are a =</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="15,70.87,159.67,453.54,210.29"><head/><label/><figDesc><div><p/></div></figDesc><graphic coords="15,70.87,159.67,453.54,210.29" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,65.25,66.80,459.16,362.71"><head/><label/><figDesc><div><p><s coords="3,185.20,229.59,339.22,9.65;3,70.87,241.86,98.69,8.64">Long = P Long -P Short ) of student models with different models sizes for (a) Qwen family (b) Llama family.</s><s coords="3,172.66,241.86,351.75,8.64;3,70.87,253.82,270.28,8.64">For teacher models, QwQ-preview-32B is chosen to generate long CoT responses, while Qwen2.5-32B-Instruct is chosen to generate short CoT responses.</s><s coords="3,344.23,253.50,180.18,9.65;3,70.87,265.77,150.25,8.64">Negative (Positive) ∆ Long indicates that long CoT is worse (better) than short CoT.</s><s coords="3,223.61,265.77,300.80,8.64;3,70.54,277.41,410.90,9.65">Our results demonstrate that short CoT is better for smaller student models (indicated by ∆ Long &lt; 0), while long CoT is better for larger student models (indicated by ∆ Long &gt; 0).</s></p></div></figDesc><table coords="3,65.25,66.80,445.35,362.71"><row><cell>Long = P Long CoT Gap Long P Short</cell><cell>0 10 20 30</cell><cell>Average MATH GSM8k</cell><cell/><cell cols="3">Long CoT vs Short CoT Olympiad AMC AIME</cell><cell/><cell/><cell>0 5 10</cell><cell>Long CoT vs Short CoT</cell></row><row><cell/><cell>10</cell><cell>0.5B</cell><cell>1.5B</cell><cell cols="3">3B Student Model Size 7B (a) Qwen Family</cell><cell>14B</cell><cell>32B</cell><cell>5</cell><cell>1B</cell><cell>3B Student Model Size 8B (b) Llama Family</cell><cell>70B</cell></row><row><cell cols="5">Figure 2: Long CoT Gap (∆ 0.5B 1.5B</cell><cell>3B</cell><cell>7B</cell><cell>14B</cell><cell>32B</cell><cell/></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,305.81,75.60,220.25,361.47"><head>Table 2 :</head><label>2</label><figDesc/><table coords="4,305.81,75.60,220.25,312.70"><row><cell cols="5">Student Model PLong P Short ∆Long Better?</cell></row><row><cell>Qwen2.5-0.5B</cell><cell>14.8</cell><cell>19.5</cell><cell>-4.7</cell><cell>Short</cell></row><row><cell>Qwen2.5-1.5B</cell><cell>27.0</cell><cell>34.2</cell><cell>-7.1</cell><cell>Short</cell></row><row><cell>Qwen2.5-3B</cell><cell>40.3</cell><cell>43.4</cell><cell>-3.1</cell><cell>Short</cell></row><row><cell>Qwen2.5-7B</cell><cell>48.9</cell><cell>47.2</cell><cell>1.7</cell><cell>Long</cell></row><row><cell>Qwen2.5-14B</cell><cell>59.2</cell><cell>54.3</cell><cell>4.9</cell><cell>Long</cell></row><row><cell>Qwen2.5-32B</cell><cell>73.0</cell><cell>59.3</cell><cell>13.7</cell><cell>Long</cell></row><row><cell>Llama-3.2-1B</cell><cell>15.8</cell><cell>19.5</cell><cell>-3.7</cell><cell>Short</cell></row><row><cell>Llama-3.2-3B</cell><cell>32.5</cell><cell>33.1</cell><cell>-0.6</cell><cell>Short</cell></row><row><cell>Llama-3.1-8B</cell><cell>35.2</cell><cell>31.5</cell><cell>3.7</cell><cell>Long</cell></row><row><cell>Llama-3.3-70B</cell><cell>58.2</cell><cell>54.3</cell><cell>3.8</cell><cell>Long</cell></row><row><cell cols="5">Table 1: Comparison of the average performance be-</cell></row><row><cell cols="5">tween fine-tuning with long CoT (P Long ) and short CoT</cell></row><row><cell cols="5">(P Short ). We find that small student models may strug-</cell></row><row><cell cols="3">gle to learn from long CoT data.</cell><cell/><cell/></row><row><cell cols="5">Student Model PLarge P Small ∆Large Better?</cell></row><row><cell>Qwen2.5-0.5B</cell><cell>16.9</cell><cell>20.4</cell><cell>-3.5</cell><cell>Weak</cell></row><row><cell>Qwen2.5-1.5B</cell><cell>32.2</cell><cell>33.0</cell><cell>-0.8</cell><cell>Weak</cell></row><row><cell>Qwen2.5-3B</cell><cell>39.7</cell><cell>39.4</cell><cell>0.3</cell><cell>Strong</cell></row><row><cell>Qwen2.5-7B</cell><cell>48.9</cell><cell>42.3</cell><cell>6.6</cell><cell>Strong</cell></row><row><cell>Qwen2.5-14B</cell><cell>52.9</cell><cell>49.9</cell><cell>3.0</cell><cell>Strong</cell></row><row><cell>Qwen2.5-32B</cell><cell>59.5</cell><cell>53.0</cell><cell>6.5</cell><cell>Strong</cell></row><row><cell>Llama-3.2-1B</cell><cell>16.5</cell><cell>18.5</cell><cell>-1.9</cell><cell>Weak</cell></row><row><cell>Llama-3.2-3B</cell><cell>32.8</cell><cell>31.2</cell><cell>1.6</cell><cell>Strong</cell></row><row><cell>Llama-3.2-8B</cell><cell>25.6</cell><cell>25.1</cell><cell>0.5</cell><cell>Strong</cell></row><row><cell>Llama-3.2-70B</cell><cell>57.6</cell><cell>53.3</cell><cell>4.3</cell><cell>Strong</cell></row></table><note coords="4,341.39,403.82,183.02,8.64;4,306.14,415.78,218.27,9.33;4,306.14,427.73,82.97,9.33"><p><s coords="4,341.39,403.82,183.02,8.64;4,306.14,415.78,218.27,9.33;4,306.14,427.73,82.97,9.33">Comparison of average performance between fine-tuning with large teacher CoT (P Long ) and small teacher CoT (P Small</s></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,70.51,76.07,453.90,283.65"><head>Table 3 :</head><label>3</label><figDesc><div><p><s coords="5,105.97,290.91,17.28,8.96">Mix</s></p></div></figDesc><table coords="5,92.20,76.07,410.86,199.50"><row><cell cols="2">Student Model Distillation Method</cell><cell cols="3">MATH AMC GSM8k</cell><cell>Olympiad Bench</cell><cell cols="2">AIME Average</cell></row><row><cell/><cell>Long CoT</cell><cell>56.2</cell><cell>37.5</cell><cell>80.0</cell><cell>24.4</cell><cell>3.3</cell><cell>40.3</cell></row><row><cell/><cell>Short CoT</cell><cell>61.0</cell><cell>37.5</cell><cell>82.0</cell><cell>26.4</cell><cell>10.0</cell><cell>43.4</cell></row><row><cell/><cell>Strong Model CoT</cell><cell>57.5</cell><cell>35.0</cell><cell>80.0</cell><cell>25.9</cell><cell>0.0</cell><cell>39.7</cell></row><row><cell>Qwen2.5-3B</cell><cell>Weak Model CoT</cell><cell>60.3</cell><cell>27.5</cell><cell>79.5</cell><cell>26.4</cell><cell>3.3</cell><cell>39.4</cell></row><row><cell/><cell>Deepseek-R1-32B (Long CoT)</cell><cell>50.7</cell><cell>20.0</cell><cell>81.2</cell><cell>15.7</cell><cell>0.0</cell><cell>33.5</cell></row><row><cell/><cell>Ours</cell><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell/><cell>Mix-Long</cell><cell>64.7</cell><cell>45.0</cell><cell>81.4</cell><cell>28.6</cell><cell>10.0</cell><cell>45.9</cell></row><row><cell/><cell>Mix-Large</cell><cell>65.8</cell><cell>42.5</cell><cell>81.7</cell><cell>29.0</cell><cell>10.0</cell><cell>45.8</cell></row><row><cell/><cell>Long CoT</cell><cell>48.7</cell><cell>17.5</cell><cell>75.1</cell><cell>17.6</cell><cell>3.3</cell><cell>32.5</cell></row><row><cell/><cell>Short CoT</cell><cell>50.9</cell><cell>15.0</cell><cell>77.5</cell><cell>18.7</cell><cell>3.3</cell><cell>33.1</cell></row><row><cell/><cell>Strong Model CoT</cell><cell>47.4</cell><cell>25.0</cell><cell>71.2</cell><cell>16.9</cell><cell>3.3</cell><cell>32.8</cell></row><row><cell>Llama3.2-3B</cell><cell>Weak Model CoT</cell><cell>47.9</cell><cell>17.5</cell><cell>74.1</cell><cell>16.4</cell><cell>3.3</cell><cell>31.2</cell></row><row><cell/><cell>Deepseek-R1-32B (Long CoT)</cell><cell>48.5</cell><cell>17.5</cell><cell>77.7</cell><cell>16.1</cell><cell>6.7</cell><cell>33.3</cell></row><row><cell/><cell>Ours</cell><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell/><cell>Mix-Long</cell><cell>53.0</cell><cell>22.5</cell><cell>79.4</cell><cell>17.2</cell><cell>3.3</cell><cell>35.1</cell></row><row><cell/><cell>Mix-Large</cell><cell>51.8</cell><cell>25.0</cell><cell>76.3</cell><cell>17.2</cell><cell>3.3</cell><cell>34.7</cell></row></table><note coords="5,125.72,290.91,398.69,9.03;5,70.87,303.26,453.54,8.64;5,70.87,315.21,453.54,8.64;5,70.87,327.17,453.54,8.64;5,70.51,339.12,453.90,8.64;5,70.87,351.08,384.99,8.64"><p><s coords="5,125.72,290.91,264.51,9.03">Distillation outperforms the baseline models across most metrics.</s><s coords="5,393.33,291.30,131.08,8.64">We use Llama3.2-3B-Instruct</s><s coords="5,70.87,303.26,412.33,8.64">and Qwen2.5-3B-Instruct as the student model and 7.5k samples in MATH dataset as the training set.</s><s coords="5,486.29,303.26,38.12,8.64;5,70.87,315.21,250.11,8.64">We distill different teacher models to generate responses as the baseline.</s><s coords="5,324.07,315.21,200.34,8.64;5,70.87,327.17,453.54,8.64;5,70.51,339.12,100.29,8.64">Our proposed Mix-Long combines long CoT data and normal CoT data in a 1:4 ratio, while Mix-Large combines strong model response and weak model response with the same proportion.</s><s coords="5,173.86,339.12,350.54,8.64;5,70.87,351.08,97.18,8.64">Experimental results demonstrate that both Mix-Long and Mix-Large surpass baselines in most evaluation metrics.</s><s coords="5,171.14,351.08,284.72,8.64">The highest score is bolded, and the second highest score is underlined.</s></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,79.37,380.64,200.95,8.64"><head>Table 4 :</head><label>4</label><figDesc><div><p><s coords="12,114.59,380.64,165.74,8.64">Overview of Teacher and Student Models</s></p></div></figDesc><table/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,70.56,76.42,422.05,694.98"><head>Table 5 :</head><label>5</label><figDesc><div><p><s coords="12,105.71,750.81,183.42,8.64;12,70.87,762.76,88.54,8.64">This table shows the hyper-parameters for full parameter fine-tuning.</s></p></div></figDesc><table coords="12,102.10,654.82,155.23,79.69"><row><cell/><cell>-5</cell></row><row><cell>Number of Epochs</cell><cell>2</cell></row><row><cell>Number of Devices</cell><cell>4</cell></row><row><cell>Per-device Batch Size</cell><cell>2</cell></row><row><cell>Optimizer</cell><cell>Adamw</cell></row><row><cell cols="2">Learning Rate Scheduler cosine</cell></row><row><cell>Max Sequence Length</cell><cell>16384</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,305.83,200.57,218.74,20.59"><head>Table 6 :</head><label>6</label><figDesc><div><p><s coords="12,346.30,200.57,178.28,8.64;12,306.14,212.53,73.61,8.64">This table shows the hyper-parameters for LoRA fine-tuning.</s></p></div></figDesc><table/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,70.56,74.55,454.16,350.58"><head>Table 7 :</head><label>7</label><figDesc><div><p><s coords="13,149.47,86.07,321.63,6.52;13,105.80,195.46,418.92,8.64;13,70.87,207.41,76.36,8.64">Short ∆ Long P Long P Short ∆ Long P Long P Short ∆ Long P Long P Short ∆ Long P Long P Short ∆ Long This table summarizes the performance of models in Llama and Qwen families fine-tuned with long CoT and short CoT data.</s><s coords="13,150.22,207.41,296.72,8.64">They are evaluated on MATH, GSM8K, AIME, AMC, and OlympiadBench.</s><s coords="13,449.94,207.63,74.72,8.00;13,70.87,219.37,404.98,8.64">QwQ-32B-Preview is chosen to generate long CoT and awhile Qwen-2.5-32B-Instruct is chosen to generate short CoT.</s><s coords="13,478.33,219.37,46.08,8.64;13,70.87,231.32,453.54,8.64;13,70.87,243.28,61.53,8.64">We observe that small student models tend to benefit more from short CoT, while large student models gain greater advantages from long CoT.</s></p></div></figDesc><table coords="13,71.72,74.55,447.07,350.58"><row><cell/><cell/><cell>MATH</cell><cell/><cell/><cell>GSM8K</cell><cell/><cell/><cell>AIME</cell><cell/><cell/><cell>AMC</cell><cell/><cell/><cell>Olympiad</cell><cell/><cell>Average ∆ Long</cell></row><row><cell cols="3">Model P Long P Llama-3.2-1B 28.6 33.4</cell><cell>-4.78</cell><cell>42.3</cell><cell>49.2</cell><cell>-6.90</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>2.50</cell><cell>7.50</cell><cell>-5.00</cell><cell>5.48</cell><cell>7.40</cell><cell>-1.92</cell><cell>-3.72</cell></row><row><cell>Llama-3.2-3B</cell><cell>48.7</cell><cell>50.9</cell><cell>-2.14</cell><cell>75.1</cell><cell>77.5</cell><cell>-2.42</cell><cell>3.33</cell><cell>3.33</cell><cell>0.00</cell><cell>17.5</cell><cell>15.0</cell><cell>2.50</cell><cell>17.6</cell><cell>18.7</cell><cell>-1.04</cell><cell>-0.619</cell></row><row><cell>Llama-3.1-8B</cell><cell>50.0</cell><cell>44.6</cell><cell>5.36</cell><cell>81.4</cell><cell>75.5</cell><cell>5.84</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>27.5</cell><cell>22.5</cell><cell>5.00</cell><cell>17.3</cell><cell>14.8</cell><cell>2.52</cell><cell>3.74</cell></row><row><cell cols="2">Llama-3.3-70B 75.3</cell><cell>74.9</cell><cell>0.340</cell><cell>92.7</cell><cell>91.2</cell><cell>1.44</cell><cell>26.7</cell><cell>13.3</cell><cell>13.3</cell><cell>55.0</cell><cell>52.5</cell><cell>2.50</cell><cell>41.3</cell><cell>39.7</cell><cell>1.63</cell><cell>3.85</cell></row><row><cell>Qwen2.5-0.5B</cell><cell>23.0</cell><cell>31.5</cell><cell>-8.44</cell><cell>39.5</cell><cell>45.3</cell><cell>-5.84</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>7.50</cell><cell>15.0</cell><cell>-7.50</cell><cell>4.00</cell><cell>5.93</cell><cell>-1.93</cell><cell>-4.74</cell></row><row><cell>Qwen2.5-1.5B</cell><cell>41.6</cell><cell>52.3</cell><cell>-10.7</cell><cell>63.8</cell><cell>71.7</cell><cell>-7.89</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>17.5</cell><cell>27.5</cell><cell>-10.0</cell><cell>12.3</cell><cell>19.4</cell><cell>-7.11</cell><cell>-7.13</cell></row><row><cell>Qwen2.5-3B</cell><cell>56.2</cell><cell>61.0</cell><cell>-4.84</cell><cell>80.0</cell><cell>82.0</cell><cell>-1.98</cell><cell>3.33</cell><cell>10.0</cell><cell>-6.67</cell><cell>37.5</cell><cell>37.5</cell><cell>0.00</cell><cell>24.4</cell><cell>26.4</cell><cell>-1.93</cell><cell>-3.08</cell></row><row><cell>Qwen2.5-7B</cell><cell>68.2</cell><cell>67.8</cell><cell>0.460</cell><cell>86.2</cell><cell>85.7</cell><cell>0.560</cell><cell>13.3</cell><cell>6.67</cell><cell>6.67</cell><cell>40.0</cell><cell>40.0</cell><cell>0.00</cell><cell>36.6</cell><cell>35.7</cell><cell>0.889</cell><cell>1.72</cell></row><row><cell>Qwen2.5-14B</cell><cell>78.3</cell><cell>76.2</cell><cell>2.04</cell><cell>93.3</cell><cell>92.5</cell><cell>0.760</cell><cell>20.0</cell><cell>6.67</cell><cell>13.3</cell><cell>60.0</cell><cell>55.0</cell><cell>5.00</cell><cell>44.4</cell><cell>40.9</cell><cell>3.56</cell><cell>4.94</cell></row><row><cell>Qwen2.5-32B</cell><cell>84.8</cell><cell>82.3</cell><cell>2.44</cell><cell>94.9</cell><cell>94.3</cell><cell>0.610</cell><cell>40.0</cell><cell>10.0</cell><cell>30.0</cell><cell>85.0</cell><cell>62.5</cell><cell>22.5</cell><cell>60.4</cell><cell>47.3</cell><cell>13.2</cell><cell>13.7</cell></row><row><cell>Prompt</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="8">Given a math problem, its correct final an-</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="8">swer, and the model's generated final an-</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="8">swer, determine if the model's answer is cor-</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="8">rect. Respond with 'True' if the it is correct</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="5">and 'False' if it is incorrect.</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="4">Problem: {problem}</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="7">Correct Final Answer: {ground truth}</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="8">Model's Generated Final Answer: {resp</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>answer}</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="3">Your Judgement:</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="13,305.80,718.77,220.42,50.11"><head>Table 8 :</head><label>8</label><figDesc><div><p><s coords="13,305.80,718.77,218.61,9.46;13,306.14,732.32,218.27,9.46;13,306.14,745.87,152.82,9.46">This section demonstrates examples of different CoT data including long CoT, short CoT, large teacher CoT and small teacher CoT.</s><s coords="13,461.48,745.87,64.74,9.46;13,306.14,759.42,60.44,9.46">See the following examples.</s><s coords="14,149.91,171.72,341.22,6.27;14,108.21,277.32,416.20,8.64;14,70.87,289.27,455.28,8.64">Weak ∆Strong PStrong P Weak ∆Strong PStrong P Weak ∆Strong PStrong P Weak ∆Strong PStrong P Weak ∆Strong This table summarizes the performance of models in Llama and Qwen families fine-tuned with large teacher CoT and small teacher CoT when evaluated on MATH, GSM8K, AIME, AMC, and OlympiadBench.</s></p></div></figDesc><table coords="14,74.34,152.98,445.02,109.85"><row><cell/><cell/><cell>MATH</cell><cell/><cell/><cell>GSM8k</cell><cell/><cell/><cell>AIME</cell><cell/><cell/><cell>AMC</cell><cell/><cell/><cell>Olympiad</cell><cell/><cell>Average ∆Strong</cell></row><row><cell cols="3">Model PStrong P Llama-3.2-1B 29.8 29.6</cell><cell>0.160</cell><cell>44.4</cell><cell>47.5</cell><cell>-3.18</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>2.50</cell><cell>7.50</cell><cell>-5.00</cell><cell>6.07</cell><cell>7.70</cell><cell>-1.63</cell><cell>-1.93</cell></row><row><cell>Llama-3.2-3B</cell><cell>47.4</cell><cell>47.9</cell><cell>-0.500</cell><cell>71.2</cell><cell>74.1</cell><cell>-2.88</cell><cell>3.33</cell><cell>0.00</cell><cell>3.33</cell><cell>25.0</cell><cell>17.5</cell><cell>7.50</cell><cell>16.9</cell><cell>16.4</cell><cell>0.445</cell><cell>1.58</cell></row><row><cell>Llama-3.2-8B</cell><cell>37.6</cell><cell>37.6</cell><cell>-0.040</cell><cell>67.0</cell><cell>69.2</cell><cell>-2.20</cell><cell>6.67</cell><cell>0.00</cell><cell>6.67</cell><cell>7.50</cell><cell>7.50</cell><cell>0.00</cell><cell>9.19</cell><cell>11.0</cell><cell>-1.78</cell><cell>0.530</cell></row><row><cell>Llama-3.2-70B</cell><cell>74.5</cell><cell>72.2</cell><cell>2.28</cell><cell>92.0</cell><cell>92.2</cell><cell>-0.152</cell><cell>16.7</cell><cell>16.7</cell><cell>0.00</cell><cell>67.5</cell><cell>50.0</cell><cell>17.5</cell><cell>37.3</cell><cell>35.7</cell><cell>1.63</cell><cell>4.25</cell></row><row><cell>Qwen2.5-0.5B</cell><cell>30.0</cell><cell>31.0</cell><cell>-0.920</cell><cell>43.1</cell><cell>45.4</cell><cell>-2.35</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>5.00</cell><cell>17.5</cell><cell>-12.5</cell><cell>6.52</cell><cell>8.30</cell><cell>-1.78</cell><cell>-3.51</cell></row><row><cell>Qwen2.5-1.5B</cell><cell>50.3</cell><cell>50.7</cell><cell>-0.440</cell><cell>70.6</cell><cell>71.0</cell><cell>-0.455</cell><cell>0.00</cell><cell>3.33</cell><cell>-3.33</cell><cell>22.5</cell><cell>20.0</cell><cell>2.50</cell><cell>17.8</cell><cell>20.0</cell><cell>-2.22</cell><cell>-0.790</cell></row><row><cell>Qwen2.5-3B</cell><cell>57.5</cell><cell>60.3</cell><cell>-2.82</cell><cell>79.9</cell><cell>79.5</cell><cell>0.379</cell><cell>0.00</cell><cell>3.33</cell><cell>-3.33</cell><cell>35.0</cell><cell>27.5</cell><cell>7.50</cell><cell>25.9</cell><cell>26.4</cell><cell>-0.444</cell><cell>0.256</cell></row><row><cell>Qwen2.5-7B</cell><cell>71.3</cell><cell>63.6</cell><cell>7.66</cell><cell>87.8</cell><cell>84.1</cell><cell>3.72</cell><cell>6.67</cell><cell>0.00</cell><cell>6.67</cell><cell>40.0</cell><cell>35.0</cell><cell>5.00</cell><cell>38.8</cell><cell>29.0</cell><cell>9.78</cell><cell>6.56</cell></row><row><cell>Qwen2.5-14B</cell><cell>76.4</cell><cell>72.8</cell><cell>3.66</cell><cell>93.1</cell><cell>89.6</cell><cell>3.49</cell><cell>6.67</cell><cell>3.33</cell><cell>3.33</cell><cell>47.5</cell><cell>45.0</cell><cell>2.50</cell><cell>41.0</cell><cell>39.0</cell><cell>2.07</cell><cell>3.01</cell></row><row><cell>Qwen2.5-32B</cell><cell>80.5</cell><cell>76.8</cell><cell>3.72</cell><cell>92.2</cell><cell>92.7</cell><cell>-0.531</cell><cell>20.0</cell><cell>3.33</cell><cell>16.7</cell><cell>57.5</cell><cell>50.0</cell><cell>7.50</cell><cell>47.4</cell><cell>42.4</cell><cell>5.04</cell><cell>6.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="14,70.56,504.63,453.85,152.54"><head>Table 9 :</head><label>9</label><figDesc><div><p><s coords="14,106.14,648.54,418.27,8.64">This table presents the performance of student models distilled from different teacher models, including</s></p></div></figDesc><table coords="14,373.47,504.63,100.08,6.94"><row><cell>Llama3.1-8B vs Llama3.1-70B</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head coords="8,306.14,328.96,88.88,10.75">Acknowledgment</head><p><s coords="8,305.80,359.26,218.61,9.46;8,306.14,372.81,218.26,9.46;8,306.14,386.36,218.27,9.46;8,305.78,399.90,220.43,9.46;8,306.14,413.45,218.45,9.46;8,305.75,427.00,220.47,9.46;8,306.14,440.55,180.14,9.46">This work is partially supported by the <rs type="funder">Air Force Office of Scientific Research (AFOSR)</rs> under grant <rs type="grantNumber">FA9550-23-1-0208</rs>, the <rs type="funder">Office of Naval Research (ONR)</rs> under grant <rs type="grantNumber">N0014-23-1-2386</rs>, and the <rs type="funder">National Science Foundation (NSF) AI Institute for Agent</rs><rs type="projectName">-based Cyber Threat Intelligence and Operation (ACTION</rs><rs type="funder">)</rs> under grant <rs type="grantNumber">IIS 2229876</rs>.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Jqh9PYu">
					<idno type="grant-number">FA9550-23-1-0208</idno>
				</org>
				<org type="funding" xml:id="_afdF98E">
					<idno type="grant-number">N0014-23-1-2386</idno>
				</org>
				<org type="funded-project" xml:id="_MBQ44Mw">
					<orgName type="project" subtype="full">-based Cyber Threat Intelligence and Operation (ACTION</orgName>
				</org>
				<org type="funding" xml:id="_JgxXV85">
					<idno type="grant-number">IIS 2229876</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="9,70.87,73.58,91.31,10.75">Ethical Statement</head><p><s coords="9,70.53,97.19,220.41,9.46;9,70.87,110.74,218.27,9.46;9,70.87,124.29,166.04,9.46">This paper focuses on the evaluation and enhancement of reasoning capabilities in small language models through distillation techniques.</s><s coords="9,240.24,124.29,48.89,9.46;9,70.87,137.83,220.08,9.46;9,70.87,151.38,62.57,9.46">The dataset and benchmarks used in our experiments are publicly available.</s><s coords="9,136.84,151.38,152.67,9.46;9,70.87,164.93,220.18,9.46">We do not introduce or endorse any applications that could cause harm or be misused.</s><s coords="9,70.53,178.48,214.33,9.46">This paper does not present any ethical concerns.</s></p><p><s coords="17,71.72,283.91,314.11,9.46;17,85.20,300.69,127.25,9.46">Example of most shifted tokens after fine-tuning on long CoT data (2/2) Then, completing the square:</s></p><p><s coords="17,84.91,369.38,227.65,9.81">Yes, that's a circle with center (7, 24) and radius 25.</s><s coords="17,85.54,382.93,193.62,9.81">So, the maximum y is indeed 24 + 25 = 49.</s><s coords="17,85.54,396.48,424.20,9.81;17,85.16,410.03,13.69,9.57">But, just to be thorough, let's consider if there are any points on the circle where y is greater than 49.</s><s coords="17,102.29,410.03,407.45,9.81;17,85.54,423.58,117.16,9.81">The center is at (7, 24), and the radius is 25, so the highest point is at (7, 49), and the lowest point would be at (7, -1).</s><s coords="17,85.54,437.13,368.36,9.81">But, the problem is asking for the maximum value of y, and I've found that to be 49.</s><s coords="17,85.20,450.68,143.90,9.81">Therefore, the final answer is 49.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="19,71.72,79.23,86.66,9.46">Long CoT Example</head><p><s coords="19,85.20,96.00,424.54,9.46;19,85.54,109.55,194.80,9.46">To find the sum of the positive odd divisors of 60, I first need to understand what the divisors of 60 are and then identify which of those are odd.</s><s coords="19,85.54,123.10,196.83,9.46">First, let's find all the positive divisors of 60.</s><s coords="19,285.75,123.10,223.99,9.46;19,85.54,136.65,32.30,9.46;19,281.76,163.40,5.45,9.57;19,287.21,160.90,4.23,6.99;19,294.36,163.40,16.36,9.57;19,310.73,160.90,4.23,6.99;19,317.88,163.40,16.36,9.57;19,334.25,160.90,4.23,6.99;19,85.54,183.78,424.20,9.46;19,85.54,197.32,121.35,9.46">To do this, I can start by factoring 60 into its prime factors. <ref type="table" coords="19,281.76,163.40,5.45,9.57;19,287.21,160.90,4.23,6.99;19,294.36,163.40,16.36,9.57;19,310.73,160.90,4.23,6.99;19,317.88,163.40,16.36,9.57;19,334.25,160.90,4.23,6.99">2 2 × 3 1 × 5 1</ref> Now, to find all the positive divisors, I can take all possible products of these prime factors raised to their possible exponents.</s><s coords="19,210.27,196.98,301.37,9.81;19,85.54,210.87,75.16,9.46">For 2, the exponents can be 0, 1, or 2; for 3, 0 or 1; and for 5, 0 or 1. Listing them out:</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="60" coords="19,256.30,163.40,22.43,9.57">=</head><p><s coords="19,85.54,448.59,148.68,9.46">So, the positive divisors of 60 are:</s></p><p><s coords="19,220.37,472.75,154.55,9.57;19,85.54,497.60,215.26,9.46">1, 2, <ref type="bibr" coords="19,240.97,472.75,8.49,9.57">3,</ref><ref type="bibr" coords="19,251.27,472.75,8.49,9.57">4,</ref><ref type="bibr" coords="19,261.58,472.75,8.49,9.57">5,</ref><ref type="bibr" coords="19,271.88,472.75,8.49,9.57">6,</ref><ref type="bibr" coords="19,282.18,472.75,13.94,9.57">10,</ref><ref type="bibr" coords="19,297.94,472.75,13.94,9.57">12,</ref><ref type="bibr" coords="19,313.70,472.75,13.94,9.57">15,</ref><ref type="bibr" coords="19,329.46,472.75,13.94,9.57">20,</ref><ref type="bibr" coords="19,345.21,472.75,13.94,9.57">30,</ref><ref type="bibr" coords="19,360.97,472.75,9.29,9.57">60</ref>. Now, I need to identify which of these are odd.</s><s coords="19,306.42,497.60,203.32,9.46;19,85.54,511.15,156.68,9.46">Odd numbers are not divisible by 2, so I can eliminate any divisors that are even.</s><s coords="19,85.20,524.70,92.14,9.46">The odd divisors are:</s></p><p><s coords="19,275.21,537.90,44.85,9.57">1, 3, 5, 15.</s></p><p><s coords="19,85.54,557.93,317.98,9.81">Now, I need to find the sum of these odd divisors: 1 + 3 + 5 + 15 = 24.</s><s coords="19,406.90,558.28,102.84,9.46;19,85.54,571.48,424.20,9.81;19,85.54,585.37,193.19,9.46">Alternatively, using the prime factorization approach: Since the odd divisors are those that do not include the factor 2, I can ignore the 2's in the prime factorization.</s><s coords="19,282.11,585.37,115.75,9.46">The remaining odd part is:</s></p><p><s coords="19,85.20,632.50,184.88,9.46">The sum of the divisors of this odd part is:</s></p><p><s coords="19,85.20,679.62,143.65,9.46">This matches the previous result.</s><s coords="19,85.20,692.82,94.82,9.81;19,180.01,690.87,4.23,6.99;19,187.17,692.82,16.36,9.57;19,203.53,690.87,4.23,6.99;19,210.69,692.82,16.36,9.57;19,227.05,690.87,4.23,6.99;19,231.79,693.17,136.70,9.46">Therefore, for 60 = 2 2 × 3 1 × 5 1 , the sum of the odd divisors is:</s></p><p><s coords="19,290.67,719.92,17.33,9.57">24 .</s><s coords="25,85.20,282.00,424.54,9.46;25,85.54,295.55,35.45,9.46">To find the greatest common divisor (GCD) of 128, 144, and 480, we can use the prime factorization method.</s></p><p><s coords="25,94.19,318.64,110.13,9.88">1. Prime Factorization:</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." coords="25,94.19,407.52,179.19,9.88">Identify the common prime factors:</head><p><s coords="25,107.02,421.14,248.65,9.81">The common prime factor among 128, 144, and 480 is 2.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." coords="25,94.19,443.58,279.53,9.88">Determine the lowest power of the common prime factor:</head><p><s coords="25,107.02,457.21,205.07,9.81;25,312.08,455.26,4.23,6.99;25,316.82,457.56,2.73,9.46">The lowest power of 2 in the factorizations is 2 4 .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." coords="25,94.19,479.65,105.27,9.88">Calculate the GCD:</head><p><s coords="25,107.02,493.62,55.76,9.46">The GCD is:</s></p><p><s coords="25,85.20,531.68,278.17,9.46">Therefore, the greatest common divisor of 128, 144, and 480 is:</s></p><p><s coords="26,85.20,109.86,424.54,9.46;26,85.54,123.41,127.73,9.46">To find the greatest common divisor (GCD) of 128, 144, and 480, we start by finding the prime factorization of each number.</s><s coords="26,85.20,136.96,146.84,9.46">The prime factorization of 128 is:</s></p><p><s coords="26,85.20,301.54,146.84,9.46">The prime factorization of 144 is:</s></p><p><s coords="26,85.20,449.58,146.84,9.46;26,85.20,662.82,268.03,9.81">The prime factorization of 480 is: The only common prime factor among all three numbers is 2.</s></p><p><s coords="26,85.54,676.72,292.59,9.46">Next, we determine the smallest power of the common prime factor.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="27,71.72,460.89,126.18,9.46">Small Teacher CoT Example</head><p><s coords="27,85.20,479.70,194.47,9.46;27,283.59,477.06,4.23,6.99;27,283.59,485.12,4.23,6.99;27,291.75,479.70,217.99,9.46;27,85.54,493.25,24.85,9.46">To solve the problem of finding the value of 7 9 rounded to 2 decimal places, we can follow these steps:</s></p><p><s coords="27,94.19,516.34,155.93,9.88;27,254.04,514.13,4.23,6.99;27,254.04,522.18,4.23,6.99;27,259.47,516.76,2.73,9.46">1. Perform the division: Calculate 7 9 .</s></p><p><s coords="27,94.19,538.85,328.48,9.88">2. Convert to decimal: Perform the division to get the decimal equivalent.</s></p><p><s coords="27,94.19,561.37,313.19,9.88">3. Round to 2 decimal places: Round the decimal to 2 decimal places.</s></p><p><s coords="27,85.54,585.31,122.16,9.46;27,265.21,606.53,5.45,9.57;27,265.21,621.40,5.45,9.57;27,274.89,613.91,56.36,9.57">Let's start with the division: 7 9 ≈ 0.7777 . . .</s></p><p><s coords="27,85.54,636.61,247.57,9.46">Next, we need to round this decimal to 2 decimal places.</s><s coords="27,336.51,636.61,174.59,9.46;27,85.15,650.16,424.19,9.46;27,85.54,663.36,61.82,9.81">The digit in the third decimal place is 7, which is greater than or equal to 5. Therefore, we round the second decimal place up from 7 to 8. So, 0.7777 . . .</s><s coords="27,150.09,663.36,157.25,9.81">rounded to 2 decimal places is 0.78.</s><s coords="27,85.20,677.26,85.75,9.46">The final answer is:</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,70.87,236.75,219.92,8.64;9,81.78,247.71,207.36,8.64;9,81.78,258.67,209.01,8.64;9,81.78,269.63,209.01,8.64;9,81.78,280.41,139.84,8.82;9,70.87,303.01,148.85,8.64" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Rishabh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nino</forename><surname>Vieillard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Stanczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sabela</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.13649</idno>
		<idno>Anthropic. 2023</idno>
		<title level="m" coords="9,176.22,258.67,114.57,8.64;9,81.78,269.63,209.01,8.64;9,81.78,280.58,19.05,8.64">On-policy distillation of language models: Learning from self-generated mistakes</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>Introducing claude</note>
	<note type="raw_reference">Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Pi- otr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. 2024. On-policy distillation of lan- guage models: Learning from self-generated mis- takes. Preprint, arXiv:2306.13649. Anthropic. 2023. Introducing claude.</note>
</biblStruct>

<biblStruct coords="9,70.87,325.44,218.26,8.64;9,81.78,336.40,207.36,8.64;9,81.78,347.36,207.35,8.64;9,81.42,358.32,208.96,8.64;9,81.78,369.28,208.60,8.64;9,81.42,380.24,208.96,8.64;9,81.78,391.19,208.60,8.64;9,81.78,402.15,207.36,8.64;9,81.78,413.11,209.01,8.64;9,81.78,424.07,207.36,8.64;9,81.42,435.03,209.36,8.64;9,81.78,445.81,131.09,8.82" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Mc-Candlish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m" coords="9,143.99,435.03,146.79,8.64;9,81.78,445.99,10.58,8.64">Language models are few-shot learners</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc- Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learn- ers. Preprint, arXiv:2005.14165.</note>
</biblStruct>

<biblStruct coords="9,70.87,468.42,219.52,8.64;9,81.78,479.38,207.36,8.64;9,81.78,490.34,207.36,8.64;9,81.78,501.29,209.10,8.64;9,81.78,512.25,209.01,8.64;9,81.78,523.03,138.29,8.82" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<title level="m" coords="9,110.83,512.25,179.96,8.64;9,81.78,523.21,17.05,8.64">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word prob- lems. Preprint, arXiv:2110.14168.</note>
</biblStruct>

<biblStruct coords="9,70.87,545.64,219.92,8.64;9,81.78,556.60,209.10,8.64;9,81.47,567.38,113.39,8.82" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Deepseek-Ai</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2501.12948</idno>
		<title level="m" coords="9,160.11,545.64,130.68,8.64;9,81.78,556.60,205.20,8.64">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">DeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea- soning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948.</note>
</biblStruct>

<biblStruct coords="9,70.87,589.99,219.92,8.64;9,81.78,600.94,207.36,8.64;9,81.78,611.72,143.26,8.82" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Deepseek-Ai</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bingxuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.19437</idno>
		<title level="m" coords="9,196.52,600.94,92.62,8.64;9,81.78,611.90,22.53,8.64">Deepseek-v3 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingx- uan Wang, and et al. 2024. Deepseek-v3 technical report. Preprint, arXiv:2412.19437.</note>
</biblStruct>

<biblStruct coords="9,70.87,634.33,218.27,8.64;9,81.78,645.29,207.36,8.64;9,81.78,656.25,208.75,8.64;9,81.78,667.21,207.36,8.64;9,81.78,677.99,186.72,8.82" xml:id="b5">
	<monogr>
		<title level="m" type="main" coords="9,270.48,656.25,20.04,8.64;9,81.78,667.21,207.36,8.64;9,81.78,678.17,65.50,8.64">Raft: Reward ranked finetuning for generative foundation model alignment</title>
		<author>
			<persName coords=""><forename type="first">Hanze</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deepanshu</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yihan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Winnie</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shizhe</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kashun</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.06767</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. Raft: Reward ranked finetuning for generative foundation model alignment. Preprint, arXiv:2304.06767.</note>
</biblStruct>

<biblStruct coords="9,70.87,700.60,219.92,8.64;9,81.53,711.55,207.86,8.64;9,81.78,722.51,207.36,8.64;9,81.31,733.47,207.83,8.64;9,81.78,744.43,209.10,8.64;9,81.78,755.39,209.10,8.64;9,81.47,766.17,113.39,8.82" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Srivatsan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ksenia</forename><surname>Konyushkova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lotte</forename><surname>Weerts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Ahern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miaosen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenjie</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.08998</idno>
		<title level="m" coords="9,81.78,755.39,204.73,8.64">Reinforced self-training (rest) for language modeling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Caglar Gulcehre, Tom Le Paine, Srivatsan Srini- vasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and Nando de Freitas. 2023. Reinforced self-training (rest) for language modeling. Preprint, arXiv:2308.08998.</note>
</biblStruct>

<biblStruct coords="9,306.14,75.34,219.51,8.64;9,317.05,86.30,209.01,8.64;9,317.05,97.26,207.36,8.64;9,317.05,108.22,208.74,8.64;9,316.69,119.18,207.72,8.64;9,317.05,130.13,209.01,8.64;9,317.05,140.91,138.29,8.82" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Chaoqun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuzhuo</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhen</forename><surname>Leng Thai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junhao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinyi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yujie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.14008</idno>
		<title level="m" coords="9,458.06,108.22,67.74,8.64;9,316.69,119.18,207.72,8.64;9,317.05,130.13,209.01,8.64;9,317.05,141.09,17.05,8.64">Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yu- jie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. 2024. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific prob- lems. Preprint, arXiv:2402.14008.</note>
</biblStruct>

<biblStruct coords="9,306.14,160.88,218.26,8.64;9,316.69,171.84,207.72,8.64;9,316.86,182.80,207.55,8.64;9,317.05,193.58,208.60,8.82;9,317.05,204.71,75.02,8.64" xml:id="b8">
	<monogr>
		<title level="m" type="main" coords="9,422.42,182.80,101.98,8.64;9,317.05,193.76,160.43,8.64">Measuring mathematical problem solving with the math dataset</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. Preprint, arXiv:2103.03874.</note>
</biblStruct>

<biblStruct coords="9,306.14,224.50,220.01,8.64;9,317.05,235.46,209.10,8.64;9,316.74,246.24,113.39,8.82" xml:id="b9">
	<monogr>
		<title level="m" type="main" coords="9,317.05,235.46,204.63,8.64">Distilling the knowledge in a neural network</title>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. Preprint, arXiv:1503.02531.</note>
</biblStruct>

<biblStruct coords="9,306.14,266.20,218.27,8.64;9,317.05,277.16,207.36,8.64;9,317.05,288.12,207.36,8.64;9,317.05,299.08,209.10,8.64;9,316.74,309.86,113.39,8.82" xml:id="b10">
	<monogr>
		<title level="m" type="main" coords="9,371.11,288.12,153.30,8.64;9,317.05,299.08,205.34,8.64">Advancing language model reasoning through reinforcement learning and inference scaling</title>
		<author>
			<persName coords=""><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rui</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiajie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yujiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zijun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.11651</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. 2025. Advancing language model reasoning through reinforcement learning and inference scaling. Preprint, arXiv:2501.11651.</note>
</biblStruct>

<biblStruct coords="9,306.14,329.83,218.27,8.64;9,316.69,340.78,207.71,8.64;9,316.58,351.74,207.83,8.64;9,317.05,362.52,208.93,8.82" xml:id="b11">
	<monogr>
		<title level="m" type="main" coords="9,403.32,351.74,121.09,8.64;9,317.05,362.70,87.49,8.64">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName coords=""><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Phillip</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shean</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Preprint, arXiv:2106.09685.</note>
</biblStruct>

<biblStruct coords="9,306.14,382.49,219.51,8.64;9,317.05,393.45,207.36,8.64;9,317.05,404.23,208.60,8.82;9,317.05,415.36,75.02,8.64" xml:id="b12">
	<monogr>
		<title level="m" type="main" coords="9,452.18,393.45,72.23,8.64;9,317.05,404.41,158.27,8.64">Smaller language models are better instruction evolvers</title>
		<author>
			<persName coords=""><forename type="first">Tingfeng</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lulu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guanting</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sen</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.11231</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Tingfeng Hui, Lulu Zhao, Guanting Dong, Yaqi Zhang, Hua Zhou, and Sen Su. 2024. Smaller language models are better instruction evolvers. Preprint, arXiv:2412.11231.</note>
</biblStruct>

<biblStruct coords="9,306.14,435.15,220.01,8.64;9,317.05,446.11,209.01,8.64;9,317.05,457.07,209.01,8.64;9,317.05,467.85,132.20,8.82" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Gyeongman</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Doohyuk</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.12842</idno>
		<title level="m" coords="9,347.37,446.11,178.69,8.64;9,317.05,457.07,209.01,8.64;9,317.05,468.03,11.42,8.64">Promptkd: Distilling student-friendly knowledge for generative language models via prompt tuning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Gyeongman Kim, Doohyuk Jang, and Eunho Yang. 2024a. Promptkd: Distilling student-friendly knowl- edge for generative language models via prompt tun- ing. Preprint, arXiv:2402.12842.</note>
</biblStruct>

<biblStruct coords="9,306.14,487.81,218.61,8.64;9,316.69,498.77,207.72,8.64;9,317.05,509.73,207.36,8.64;9,317.05,520.69,105.44,8.64;9,437.79,520.69,86.62,8.64;9,317.05,531.47,208.60,8.82;9,317.05,542.61,75.02,8.64" xml:id="b14">
	<monogr>
		<title level="m" type="main" coords="9,437.79,520.69,86.62,8.64;9,317.05,531.65,154.49,8.64">Evaluating language models as synthetic data generators</title>
		<author>
			<persName coords=""><forename type="first">Seungone</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juyoung</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seongyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kiril</forename><surname>Gashteovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carolin</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.03679</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Seungone Kim, Juyoung Suk, Xiang Yue, Vijay Viswanathan, Seongyun Lee, Yizhong Wang, Kiril Gashteovski, Carolin Lawrence, Sean Welleck, and Graham Neubig. 2024b. Evaluating language models as synthetic data generators. Preprint, arXiv:2412.03679.</note>
</biblStruct>

<biblStruct coords="9,306.14,562.39,218.27,8.64;9,317.05,573.17,191.98,8.82" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Kimi</forename><surname>Team</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.12599</idno>
		<title level="m" coords="9,385.66,562.39,138.75,8.64;9,317.05,573.35,71.07,8.64">Kimi k1.5: Scaling reinforcement learning with llms</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Kimi Team. 2025. Kimi k1.5: Scaling reinforcement learning with llms. Preprint, arXiv:2501.12599.</note>
</biblStruct>

<biblStruct coords="9,306.14,593.14,219.52,8.64;9,317.05,604.10,208.60,8.64;9,317.05,615.06,208.60,8.64;9,317.05,626.02,207.36,8.64;9,317.05,636.97,207.36,8.64;9,317.05,647.93,208.60,8.64;9,317.05,658.89,207.36,8.64;9,317.05,669.85,209.01,8.64;9,317.05,680.63,208.60,8.82;9,317.05,691.77,75.02,8.64" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentina</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shengyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Faeze</forename><surname>Brahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lester</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shane</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuling</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saumya</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victoria</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiangjiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.15124</idno>
		<title level="m" coords="9,435.68,669.85,90.38,8.64;9,317.05,680.81,165.69,8.64">Tulu 3: Pushing frontiers in open language model post-training</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. 2025. Tulu 3: Pushing fron- tiers in open language model post-training. Preprint, arXiv:2411.15124.</note>
</biblStruct>

<biblStruct coords="9,306.14,711.55,219.52,8.64;9,317.05,722.51,207.36,8.64;9,317.05,733.47,207.36,8.64;9,317.05,744.43,207.36,8.64;9,317.05,755.39,156.02,8.64;9,489.93,755.39,36.13,8.64;9,317.05,766.35,208.35,8.64;10,81.78,75.56,200.25,8.00;10,81.78,86.52,200.75,8.00;10,81.78,97.48,156.91,8.00" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">I</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Beeching</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Lipkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roman</forename><surname>Soletskyi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Costa</forename><surname>Shengyi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kashif</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Longhui</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Albert</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziju</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zihan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Fleureau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stanislas</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Polu</surname></persName>
		</author>
		<ptr target="https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf"/>
		<title level="m" coords="9,489.93,755.39,36.13,8.64;9,317.05,766.35,18.29,8.64">Numinamath</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. 2024. Numina- math. [https://github.com/project-numina/ aimo-progress-prize](https://github.com/ project-numina/aimo-progress-prize/blob/ main/report/numina_dataset.pdf).</note>
</biblStruct>

<biblStruct coords="10,70.87,116.87,219.52,8.64;10,81.78,127.83,209.01,8.64;10,81.78,138.79,209.01,8.64;10,81.78,149.75,207.36,8.64;10,81.78,160.53,194.86,8.82" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Abhilasha</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ximing</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nouha</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Melanie</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Khyathi</forename><surname>Sclar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chandra</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.01552</idno>
		<title level="m" coords="10,242.32,138.79,48.47,8.64;10,81.78,149.75,207.36,8.64;10,81.78,160.71,73.97,8.64">The unlocking spell on base llms: Rethinking alignment via in-context learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chan- dra Bhagavatula, and Yejin Choi. 2023. The unlock- ing spell on base llms: Rethinking alignment via in-context learning. Preprint, arXiv:2312.01552.</note>
</biblStruct>

<biblStruct coords="10,70.87,180.32,219.52,8.64;10,81.78,191.28,209.10,8.64;10,81.78,202.24,209.10,8.64;10,81.78,213.19,207.36,8.64;10,81.78,223.97,208.60,8.82;10,81.78,235.11,75.02,8.64" xml:id="b19">
	<monogr>
		<title level="m" type="main" coords="10,115.37,213.19,173.77,8.64;10,81.78,224.15,161.99,8.64">The flan collection: Designing data and methods for effective instruction tuning</title>
		<author>
			<persName coords=""><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.13688</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning. Preprint, arXiv:2301.13688.</note>
</biblStruct>

<biblStruct coords="10,70.87,254.72,221.75,8.64;10,81.78,265.90,133.50,8.00;10,70.87,285.30,221.75,8.64;10,81.78,296.47,17.43,8.00" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Meta</forename></persName>
		</author>
		<ptr target="https://llama.meta.com"/>
		<title level="m" coords="10,129.97,254.72,39.97,8.64">Llama-3.2</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Meta. 2024a. Llama-3.2-3b. https://huggingface. co/meta-llama/Llama-3.2-3B. Meta. 2024b. Meet llama 3.1. https://llama.meta. com.</note>
</biblStruct>

<biblStruct coords="10,70.87,315.87,219.52,8.64;10,81.59,326.83,208.80,8.64;10,81.42,337.78,208.96,8.64;10,81.78,348.74,209.10,8.64;10,81.78,359.70,209.01,8.64;10,81.78,370.66,209.10,8.64;10,81.47,381.44,113.39,8.82" xml:id="b21">
	<monogr>
		<title level="m" type="main" coords="10,109.56,359.70,181.23,8.64;10,81.78,370.66,204.76,8.64">Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems</title>
		<author>
			<persName coords=""><forename type="first">Yingqian</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiapeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoxue</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huatong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.09413</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. 2024. Imitate, explore, and self-improve: A repro- duction report on slow-thinking reasoning systems. Preprint, arXiv:2412.09413.</note>
</biblStruct>

<biblStruct coords="10,70.87,401.23,218.62,8.64;10,81.78,412.19,173.81,8.64" xml:id="b22">
	<monogr>
		<title level="m" type="main" coords="10,140.78,401.23,148.71,8.64;10,81.78,412.19,52.86,8.64">Sky-T1: Train your own o1 preview model within</title>
		<author>
			<persName coords=""><surname>Novasky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025-01-09">2025. 2025-01-09</date>
		</imprint>
	</monogr>
	<note type="raw_reference">NovaSky. 2025. Sky-T1: Train your own o1 preview model within $450. Accessed: 2025-01-09.</note>
</biblStruct>

<biblStruct coords="10,70.87,431.80,153.54,8.64;10,70.87,451.42,180.12,8.64" xml:id="b23">
	<monogr>
		<title level="m" type="main" coords="10,133.94,431.80,86.72,8.64">Gpt-4 technical report</title>
		<author>
			<persName coords=""><surname>Openai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023. 2024</date>
			<publisher>OpenAI</publisher>
		</imprint>
	</monogr>
	<note>Learning to reason with llms</note>
	<note type="raw_reference">OpenAI. 2023. Gpt-4 technical report. OpenAI. 2024. Learning to reason with llms.</note>
</biblStruct>

<biblStruct coords="10,70.87,471.03,219.52,8.64;10,81.78,481.99,154.84,8.64;10,252.15,481.99,38.73,8.64;10,81.78,492.95,208.74,8.64;10,81.78,503.90,48.98,8.64" xml:id="b24">
	<monogr>
		<title level="m" type="main" coords="10,252.15,481.99,34.42,8.64">Tinyzero</title>
		<author>
			<persName coords=""><forename type="first">Jiayi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xingyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lifan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<ptr target="https://github.com/Jiayi-Pan/TinyZero"/>
		<imprint>
			<date type="published" when="2025-01-24">2025. 2025-01-24</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. 2025. Tinyzero. https://github.com/Jiayi-Pan/TinyZero. Accessed: 2025-01-24.</note>
</biblStruct>

<biblStruct coords="10,70.87,523.52,219.92,8.64;10,81.78,534.48,207.36,8.64;10,81.78,545.43,209.10,8.64;10,81.47,556.21,113.39,8.82" xml:id="b25">
	<monogr>
		<title level="m" type="main" coords="10,209.15,534.48,79.98,8.64;10,81.78,545.43,205.60,8.64">Openwebmath: An open dataset of high-quality mathematical web text</title>
		<author>
			<persName coords=""><forename type="first">Keiran</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><forename type="middle">Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhangir</forename><surname>Azerbayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06786</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Keiran Paster, Marco Dos Santos, Zhangir Azer- bayev, and Jimmy Ba. 2023. Openwebmath: An open dataset of high-quality mathematical web text. Preprint, arXiv:2310.06786.</note>
</biblStruct>

<biblStruct coords="10,70.87,576.01,220.01,8.64;10,70.87,595.62,218.26,8.64;10,81.78,606.58,95.36,8.64" xml:id="b26">
	<monogr>
		<title level="m" type="main" coords="10,129.63,576.01,156.82,8.64">Qwen2.5: A party of foundation models</title>
		<author>
			<persName coords=""><surname>Qwen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note>Qwen. 2024b. Qwq: Reflect deeply on the boundaries of the unknown</note>
	<note type="raw_reference">Qwen. 2024a. Qwen2.5: A party of foundation models. Qwen. 2024b. Qwq: Reflect deeply on the boundaries of the unknown. 2024b.</note>
</biblStruct>

<biblStruct coords="10,70.87,626.19,219.52,8.64;10,81.59,637.15,207.54,8.64;10,81.78,648.11,209.10,8.64;10,81.78,659.07,209.01,8.64;10,81.78,669.85,208.60,8.82;10,81.78,680.98,75.02,8.64" xml:id="b27">
	<monogr>
		<title level="m" type="main" coords="10,81.78,659.07,209.01,8.64;10,81.78,670.02,160.59,8.64">Deepseekmath: Pushing the limits of mathematical reasoning in open language models</title>
		<author>
			<persName coords=""><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junxiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiao</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.03300</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathemati- cal reasoning in open language models. Preprint, arXiv:2402.03300.</note>
</biblStruct>

<biblStruct coords="10,70.87,700.60,219.52,8.64;10,81.42,711.55,209.37,8.64;10,81.78,722.51,207.36,8.64;10,81.42,733.47,208.96,8.64;10,81.42,744.43,207.72,8.64;10,81.78,755.39,207.52,8.64;10,81.78,766.35,207.52,8.64;10,317.05,75.34,209.01,8.64;10,317.05,86.30,207.36,8.64;10,317.05,97.26,209.01,8.64;10,317.05,108.22,208.60,8.64;10,316.47,119.18,207.93,8.64;10,317.05,130.13,209.10,8.64;10,317.05,141.09,207.53,8.64;10,317.05,151.87,208.60,8.82;10,317.05,163.01,75.02,8.64" xml:id="b28">
	<monogr>
		<title level="m" type="main" coords="10,343.16,141.09,181.41,8.64;10,317.05,152.05,160.19,8.64">Beyond human data: Scaling self-training for problem-solving with language models</title>
		<author>
			<persName coords=""><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rishabh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ankesh</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piyush</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Rizkowsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Azade</forename><surname>Nova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gamaleldin</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanie</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isabelle</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiri</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathleen</forename><surname>Kenealy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kshiteej</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Culp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxwell</forename><forename type="middle">L</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tris</forename><surname>Warkentin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yundi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yamini</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.06585</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Pe- ter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Ke- nealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Con- stant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. 2024. Beyond human data: Scaling self-training for problem-solving with language models. Preprint, arXiv:2312.06585.</note>
</biblStruct>

<biblStruct coords="10,306.14,182.56,219.51,8.64;10,317.05,193.52,207.36,8.64;10,317.05,204.48,209.10,8.64;10,316.74,215.26,113.39,8.82" xml:id="b29">
	<monogr>
		<title level="m" type="main" coords="10,410.96,193.52,113.45,8.64;10,317.05,204.48,204.74,8.64">Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving</title>
		<author>
			<persName coords=""><forename type="first">Yuxuan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruidong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.13690</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. 2024. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. Preprint, arXiv:2407.13690.</note>
</biblStruct>

<biblStruct coords="10,306.14,234.99,218.51,8.64;10,317.05,245.94,209.10,8.64;10,317.05,256.90,207.35,8.64;10,316.69,267.68,208.96,8.82;10,317.05,278.82,75.02,8.64" xml:id="b30">
	<monogr>
		<title level="m" type="main" coords="10,342.92,256.90,181.48,8.64;10,316.69,267.86,165.71,8.64">Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data</title>
		<author>
			<persName coords=""><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Moshkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Branislav</forename><surname>Kisacanin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexan</forename><surname>Ayrapetyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.01560</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. 2024. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. Preprint, arXiv:2410.01560.</note>
</biblStruct>

<biblStruct coords="10,306.14,298.37,218.44,8.64;10,317.05,309.33,208.60,8.64;10,317.05,320.29,207.35,8.64;10,316.69,331.25,207.72,8.64;10,317.05,342.21,207.36,8.64;10,317.05,352.99,208.60,8.82;10,317.05,364.12,75.02,8.64" xml:id="b31">
	<monogr>
		<title level="m" type="main" coords="10,471.97,342.21,52.44,8.64;10,317.05,353.16,164.24,8.64">Llama: Open and efficient foundation language models</title>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Preprint, arXiv:2302.13971.</note>
</biblStruct>

<biblStruct coords="10,306.14,383.67,219.92,8.64;10,317.05,394.63,207.71,8.64;10,317.05,405.59,207.36,8.64;10,317.05,416.55,209.01,8.64;10,317.05,427.33,207.36,8.82;10,317.05,438.29,75.27,8.59" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m" coords="10,422.10,416.55,103.96,8.64;10,317.05,427.51,133.38,8.64">Llama 2: Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</note>
</biblStruct>

<biblStruct coords="10,306.14,458.02,219.52,8.64;10,317.05,468.97,208.60,8.64;10,317.05,479.93,207.36,8.64;10,317.05,490.89,209.01,8.64;10,317.05,501.85,209.10,8.64;10,317.05,512.63,208.60,8.82;10,317.05,523.77,75.02,8.64" xml:id="b33">
	<monogr>
		<title level="m" type="main" coords="10,317.05,512.81,165.70,8.64">Zephyr: Direct distillation of lm alignment</title>
		<author>
			<persName coords=""><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Beeching</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nazneen</forename><surname>Rajani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Younes</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shengyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clémentine</forename><surname>Fourrier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nathan</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nathan</forename><surname>Sarrazin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omar</forename><surname>Sanseviero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.16944</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar San- seviero, Alexander M. Rush, and Thomas Wolf. 2023. Zephyr: Direct distillation of lm alignment. Preprint, arXiv:2310.16944.</note>
</biblStruct>

<biblStruct coords="10,306.14,543.32,218.27,8.64;10,317.05,554.28,207.36,8.64;10,317.05,565.24,209.01,8.64;10,317.05,576.02,208.60,8.82;10,317.05,587.15,75.02,8.64" xml:id="b34">
	<monogr>
		<title level="m" type="main" coords="10,395.91,565.24,130.16,8.64;10,317.05,576.20,159.86,8.64">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elic- its reasoning in large language models. Preprint, arXiv:2201.11903.</note>
</biblStruct>

<biblStruct coords="10,306.14,606.70,219.92,8.64;10,317.05,617.66,207.36,8.64;10,317.05,628.62,207.36,8.64;10,317.05,639.58,207.36,8.64;10,316.69,650.36,170.12,8.82" xml:id="b35">
	<monogr>
		<title level="m" type="main" coords="10,420.92,628.62,103.50,8.64;10,317.05,639.58,207.36,8.64;10,316.69,650.54,49.02,8.64">Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing</title>
		<author>
			<persName coords=""><forename type="first">Zhangchen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luyao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Radha</forename><surname>Poovendran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Yuchen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lin</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2406.08464</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yun- tian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2024. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. Preprint, arXiv:2406.08464.</note>
</biblStruct>

<biblStruct coords="10,306.14,670.09,219.52,8.64;10,317.05,681.05,209.01,8.64;10,317.05,692.01,208.60,8.64;10,317.05,702.96,207.36,8.64;10,317.05,713.92,209.01,8.64;10,317.05,724.88,207.35,8.64;10,317.05,735.66,190.01,8.82" xml:id="b36">
	<monogr>
		<author>
			<persName coords=""><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Beichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bofei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingfeng</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Runji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xingzhang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenru</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.12122</idno>
		<title level="m" coords="10,443.54,713.92,82.52,8.64;10,317.05,724.88,207.35,8.64;10,317.05,735.84,68.73,8.64">Qwen2.5-math technical report: Toward mathematical expert model via self-improvement</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jian- hong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. 2024. Qwen2.5-math tech- nical report: Toward mathematical expert model via self-improvement. Preprint, arXiv:2409.12122.</note>
</biblStruct>

<biblStruct coords="10,306.14,755.39,218.27,8.64;10,317.05,766.35,207.36,8.64;11,81.78,75.16,208.60,8.82;11,81.78,86.30,75.02,8.64" xml:id="b37">
	<monogr>
		<title level="m" type="main" coords="10,468.52,766.35,55.89,8.64;11,81.78,75.34,162.35,8.64">Demystifying long chain-of-thought reasoning in llms</title>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxuan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Morry</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2502.03373</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. 2025. Demystifying long chain-of-thought reasoning in llms. Preprint, arXiv:2502.03373.</note>
</biblStruct>

<biblStruct coords="11,70.87,106.22,219.52,8.64;11,81.78,117.18,209.01,8.64;11,81.78,128.14,209.10,8.64;11,81.78,139.10,207.36,8.64;11,81.78,149.88,208.60,8.82;11,81.78,161.02,75.02,8.64" xml:id="b38">
	<monogr>
		<title level="m" type="main" coords="11,81.78,139.10,207.36,8.64;11,81.78,150.06,155.19,8.64">Metamath: Bootstrap your own mathematical questions for large language models</title>
		<author>
			<persName coords=""><forename type="first">Longhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weisen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jincheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhengying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.12284</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhen- guo Li, Adrian Weller, and Weiyang Liu. 2024. Metamath: Bootstrap your own mathematical questions for large language models. Preprint, arXiv:2309.12284.</note>
</biblStruct>

<biblStruct coords="11,70.87,180.94,218.27,8.64;11,81.78,191.90,207.36,8.64;11,81.59,202.86,207.55,8.64;11,81.78,213.82,209.10,8.64;11,81.47,224.60,113.39,8.82" xml:id="b39">
	<monogr>
		<title level="m" type="main" coords="11,164.15,202.86,124.98,8.64;11,81.78,213.82,204.65,8.64">Scaling relationship on learning mathematical reasoning with large language models</title>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guanting</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.01825</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. Preprint, arXiv:2308.01825.</note>
</biblStruct>

<biblStruct coords="11,70.87,244.70,219.92,8.64;11,81.78,255.66,209.10,8.64;11,81.78,266.62,209.01,8.64;11,81.78,277.40,208.60,8.82;11,81.78,288.54,75.02,8.64" xml:id="b40">
	<monogr>
		<title level="m" type="main" coords="11,112.12,266.62,178.67,8.64;11,81.78,277.58,156.75,8.64">Mammoth: Building math generalist models through hybrid instruction tuning</title>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xingwei</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.05653</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wen- hao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mammoth: Building math generalist mod- els through hybrid instruction tuning. Preprint, arXiv:2309.05653.</note>
</biblStruct>

<biblStruct coords="11,70.87,308.47,220.01,8.64;11,81.78,319.42,207.36,8.64;11,81.42,330.20,136.07,8.82" xml:id="b41">
	<monogr>
		<title level="m" type="main" coords="11,112.14,319.42,177.00,8.64;11,81.42,330.38,14.32,8.64">Mammoth2: Scaling instructions from the web</title>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tuney</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.03548</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. 2024. Mammoth2: Scaling instructions from the web. Preprint, arXiv:2405.03548.</note>
</biblStruct>

<biblStruct coords="11,70.87,350.31,220.01,8.64;11,81.78,361.27,207.36,8.64;11,81.78,372.05,158.21,8.82" xml:id="b42">
	<monogr>
		<title level="m" type="main" coords="11,151.17,361.27,137.97,8.64;11,81.78,372.23,37.10,8.64">Star: Bootstrapping reasoning with reasoning</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesse</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14465</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Preprint, arXiv:2203.14465.</note>
</biblStruct>

<biblStruct coords="11,70.87,392.15,218.27,8.64;11,81.78,403.11,209.10,8.64;11,81.53,414.07,209.26,8.64;11,81.78,425.03,207.36,8.64;11,81.78,435.99,208.35,8.64;11,81.78,446.95,133.33,8.64" xml:id="b43">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Weihao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zejun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://hkust-nlp.notion.site/simplerl-reason.NotionBlog"/>
		<title level="m" coords="11,81.53,414.07,209.26,8.64;11,81.78,425.03,207.36,8.64;11,81.78,435.99,49.69,8.64">7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 2025. 7b model and 8k examples: Emerging reason- ing with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/ simplerl-reason. Notion Blog.</note>
</biblStruct>

<biblStruct coords="11,70.87,466.87,218.27,8.64;11,81.78,477.65,208.60,8.82;11,81.78,488.79,75.02,8.64" xml:id="b44">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Dylan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qirun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2502.04194</idno>
		<title level="m" coords="11,255.87,466.87,33.26,8.64;11,81.78,477.83,163.28,8.64">The best instruction-tuning data are those that fit</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note type="raw_reference">Dylan Zhang, Qirun Dai, and Hao Peng. 2025. The best instruction-tuning data are those that fit. Preprint, arXiv:2502.04194.</note>
</biblStruct>

<biblStruct coords="11,70.87,508.71,218.26,8.64;11,81.20,519.67,209.68,8.64;11,81.78,530.63,207.36,8.64;11,81.78,541.41,207.36,8.82;11,81.78,552.37,209.01,8.59;11,81.78,563.33,209.01,8.59;11,81.78,574.29,209.02,8.82;11,81.78,585.43,71.96,8.64" xml:id="b45">
	<analytic>
		<title level="a" type="main" coords="11,111.45,530.63,177.69,8.64;11,81.78,541.59,104.74,8.64">Llamafactory: Unified efficient fine-tuning of 100+ language models</title>
		<author>
			<persName coords=""><forename type="first">Yaowei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanhan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheyan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhangchi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongqiang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,210.94,541.41,78.19,8.59;11,81.78,552.37,209.01,8.59;11,81.78,563.33,209.01,8.59;11,81.78,574.29,21.22,8.59">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Compu- tational Linguistics (Volume 3: System Demonstra- tions), Bangkok, Thailand. Association for Computa- tional Linguistics.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>