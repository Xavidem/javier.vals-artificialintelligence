<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coords="1,86.07,93.99,439.88,12.68;1,180.37,109.93,251.24,12.68">Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the Lens of Class Hierarchy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-17">17 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,105.89,175.50,85.23,8.81"><forename type="first">Roman</forename><surname>Malashin</surname></persName>
							<affiliation key="aff0" coords="1,164.93,186.86,278.83,9.96;1,224.12,198.82,157.13,9.96">
								<note type="raw_affiliation">Saint-Petersburg State University of Aerospace Instrumentation Pavlov Institute of Physiology , RAS</note>
								<orgName type="institution" key="instit1">Saint-Petersburg State University of Aerospace Instrumentation Pavlov Institute of Physiology</orgName>
								<orgName type="institution" key="instit2">RAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.84,175.50,86.31,8.81"><forename type="first">Valeria</forename><surname>Yachnaya</surname></persName>
							<affiliation key="aff0" coords="1,164.93,186.86,278.83,9.96;1,224.12,198.82,157.13,9.96">
								<note type="raw_affiliation">Saint-Petersburg State University of Aerospace Instrumentation Pavlov Institute of Physiology , RAS</note>
								<orgName type="institution" key="instit1">Saint-Petersburg State University of Aerospace Instrumentation Pavlov Institute of Physiology</orgName>
								<orgName type="institution" key="instit2">RAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,417.66,175.50,88.36,8.81"><forename type="first">Alexander</forename><surname>Mullin</surname></persName>
							<affiliation key="aff0" coords="1,164.93,186.86,278.83,9.96;1,224.12,198.82,157.13,9.96">
								<note type="raw_affiliation">Saint-Petersburg State University of Aerospace Instrumentation Pavlov Institute of Physiology , RAS</note>
								<orgName type="institution" key="instit1">Saint-Petersburg State University of Aerospace Instrumentation Pavlov Institute of Physiology</orgName>
								<orgName type="institution" key="instit2">RAS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coords="1,86.07,93.99,439.88,12.68;1,180.37,109.93,251.24,12.68">Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the Lens of Class Hierarchy</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-17">17 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">32C106BB80CE83726B1730A68D988A37</idno>
					<idno type="arXiv">arXiv:2502.12125v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-02-18T10:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,82.92,263.31,194.16,9.96;1,82.92,275.27,194.16,9.96;1,82.92,287.22,194.16,9.96;1,82.92,299.18,16.05,9.96">We investigate the training dynamics of deep classifiers by examining how hierarchical relationships between classes evolve during training.</s><s coords="1,105.23,299.18,171.84,9.96;1,82.92,311.13,194.16,9.96;1,82.92,323.09,194.16,9.96;1,82.92,335.04,82.69,9.96">Through extensive experiments, we argue that the learning process in classification problems can be understood through the lens of label clustering.</s><s coords="1,173.61,335.04,103.46,9.96;1,82.92,347.00,194.16,9.96;1,82.92,358.95,194.16,9.96;1,82.92,370.91,194.16,9.96;1,82.92,382.86,68.26,9.96">Specifically, we observe that networks tend to distinguish higher-level (hypernym) categories in the early stages of training, and learn more specific (hyponym) categories later.</s><s coords="1,155.46,382.86,121.63,9.96;1,82.92,394.82,194.16,9.96;1,82.92,406.77,194.16,9.96;1,82.92,418.73,194.16,9.96;1,82.92,430.68,138.58,9.96">We introduce a novel framework to track the evolution of the feature manifold during training, revealing how the hierarchy of class relations emerges and refines across the network layers.</s><s coords="1,228.78,430.68,48.29,9.96;1,82.92,442.64,194.16,9.96;1,82.92,454.59,194.16,9.96;1,82.92,466.55,194.16,9.96;1,82.92,478.50,167.60,9.96">Our analysis demonstrates that the learned representations closely align with the semantic structure of the dataset, providing a quantitative description of the clustering process.</s><s coords="1,261.31,478.50,15.77,9.96;1,82.92,490.46,194.16,9.96;1,82.92,502.42,194.16,9.96;1,82.92,514.37,194.16,9.96;1,82.92,526.33,194.16,9.96;1,82.92,538.28,173.19,9.96">Notably, we show that in the hypernym label space, certain properties of neural collapse appear earlier than in the hyponym label space, helping to bridge the gap between the initial and terminal phases of learning.</s><s coords="1,263.25,538.28,13.83,9.96;1,82.92,550.24,194.16,9.96;1,82.92,562.19,194.15,9.96;1,82.92,574.15,194.15,9.96;1,82.92,586.10,194.15,9.96;1,82.92,598.06,60.09,9.96">We believe our findings offer new insights into the mechanisms driving hierarchical learning in deep networks, paving the way for future advancements in understanding deep learning dynamics.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" coords="1,63.00,635.31,130.11,10.57">INTRODUCTION</head><p><s coords="1,63.00,660.61,234.00,9.96;1,63.00,672.56,234.00,9.96;1,315.00,233.97,234.00,9.96;1,315.00,245.92,19.92,9.96">Deep neural networks have demonstrated remarkable capabilities in learning complex functions, yet the un- derlying reasons for their success remain an open question.</s><s coords="1,339.27,245.92,209.72,9.96;1,315.00,257.88,234.00,9.96;1,315.00,270.43,234.00,8.81;1,315.00,282.39,97.88,8.81">In this work, we aim to shed light on one crucial aspect of this puzzle: What patterns do neural networks tend to learn first, and which ones do they learn last?</s><s coords="1,420.79,281.79,128.21,9.96;1,315.00,293.74,234.00,9.96;1,315.00,305.70,72.87,9.96">The question has been studied for a long time, but valuable interpretations continue to emerge.</s><s coords="1,395.73,305.70,153.27,9.96;1,315.00,317.65,234.00,9.96;1,315.00,329.61,234.00,9.96;1,315.00,341.56,234.00,9.96;1,315.00,353.52,96.70,9.96">One important contribution is the concept of simplicity bias proposed by <ref type="bibr" coords="1,496.27,317.65,52.73,9.96;1,315.00,329.61,26.09,9.96" target="#b2">Arpit et al. (2017)</ref>, which suggests that neural networks tend to learn simple (frequent) patterns followed by more complex or noisy patterns.</s><s coords="1,415.97,353.52,133.03,9.96;1,315.00,365.47,234.00,9.96;1,315.00,377.43,153.26,9.96">Neural collapse is another phenomenon that characterizes the terminal phase of classifier training <ref type="bibr" coords="1,375.54,377.43,88.29,9.96" target="#b26">(Papyan et al., 2020)</ref>.</s><s coords="1,472.52,377.43,76.48,9.96;1,315.00,389.38,234.00,9.96;1,315.00,401.34,234.00,9.96;1,315.00,413.30,234.00,9.96;1,315.00,425.25,100.98,9.96">During this phase variance of penultimate features within the same class converges to zero, while the class means are arranged in the feature space according to Equiangular Tight Frame(ETF) structure.</s></p><p><s coords="1,315.00,443.18,234.00,9.96;1,315.00,455.14,234.00,9.96;1,315.00,467.09,76.86,9.96">In this work we study dynamics of the classifier learning process through the lens of evolving hierarchy of object categories.</s><s coords="1,400.02,467.09,148.98,9.96;1,315.00,479.05,234.00,9.96;1,315.00,491.00,234.00,9.96;1,315.00,502.96,234.00,9.96;1,315.00,514.91,234.00,9.96;1,315.00,526.87,234.00,9.96;1,315.00,538.82,105.05,9.96">According to our experiments we argue that in the classification task the network tend to learn relations between high-level categories (hypernyms) on early stages, while more specific (hyponyms) are learned in the later training epochs; terminal phase is characterized by removing hypernymy relations from the features of the layer.</s><s coords="1,424.34,538.82,124.65,9.96;1,315.00,550.78,233.99,9.96;1,315.00,562.73,234.00,9.96;1,315.00,574.69,234.00,9.96;1,315.00,586.65,234.00,9.96;1,315.00,598.60,136.99,9.96">By analogue we call this tendency a hypernym bias, as it can be nicely interpreted as manifestation of simplicity bias: classes of same hypernym share frequent (simple) features, therefore neurons responsible for their detection are activated more often and trained faster.</s><s coords="1,461.12,598.60,87.88,9.96;1,315.00,610.56,234.00,9.96;1,315.00,622.51,234.00,9.96;1,315.00,634.47,234.00,9.96;1,315.00,646.42,148.16,9.96">The neural collapse state can be interpreted as final state of the label clustering process in the specific layer: every cluster contains only a single label, and information about labels' relations is removed from features.</s><s coords="1,467.40,646.42,81.60,9.96;1,315.00,658.38,234.00,9.96;1,315.00,670.33,99.61,9.96;1,414.60,669.38,3.97,6.16">The basic intuition about the phenomenon we study in this paper can be grasped from Figure <ref type="figure" coords="1,406.86,670.33,3.87,9.96" target="#fig_1">1</ref>. <ref type="foot" coords="1,414.60,669.38,3.97,6.16" target="#foot_0">1</ref></s><s coords="1,315.00,688.26,234.00,9.96;1,315.00,700.22,234.00,9.96;2,63.00,416.65,55.70,9.96">rom Figure <ref type="figure" coords="1,372.54,688.26,10.51,9.96" target="#fig_1">1b</ref> training can be interpreted as top-tobottom hierarchical label clustering that starts by in- In this work:</s></p><p><s coords="2,72.96,442.71,224.04,9.96;2,82.92,454.66,214.08,9.96;2,82.92,466.62,214.08,9.96;2,82.92,478.57,214.08,9.96;2,82.92,490.53,214.08,9.96;2,82.92,502.48,168.41,9.96">• We provide empirical evidence that on early epochs of training classifiers tend to prioritize learning decision boundaries that distinguish superclasses of labels that unite large hypernyms, and thus learning can be seen from the perspective of hierarchical clustering of labels.</s></p><p><s coords="2,72.96,522.72,224.04,9.96;2,82.92,534.68,90.87,9.96">• We assess the evolution of class hierarchical structure of the manifold.</s><s coords="2,178.87,534.68,118.12,9.96;2,82.92,546.63,214.08,9.96;2,82.92,558.59,214.08,9.96;2,82.92,570.54,214.08,9.96;2,82.92,582.50,48.44,9.96">By comparing within manifold and hyponymy distances we show that clusters of labels that network learn are aligned with semantic dataset (WordNet) until the early stages of training.</s></p><p><s coords="2,72.96,602.74,224.04,9.96;2,82.92,614.69,214.08,9.96;2,82.92,626.65,145.17,9.96">• We show that in hypernym label spaces some aspects of neural collapse manifest earlier than in original (hyponyms) label spaces.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" coords="2,63.00,654.54,140.75,10.57">RELATED WORKS</head><p><s coords="2,63.00,680.38,130.52,8.81">Hierarchical classification.</s><s coords="2,199.43,679.79,97.58,9.96;2,63.00,691.74,233.99,9.96;2,315.00,350.90,234.00,9.96;2,315.00,362.85,110.94,9.96">Errors in trained classifiers tend to occur more frequently between closely related classes, such as different species of animals or various types of vehicles.</s><s coords="2,433.48,362.85,115.52,9.96;2,315.00,374.81,234.00,9.96;2,315.00,386.76,208.69,9.96">This pattern is evident in the block-diagonal structure of the confusion matrix, provided that the classes are properly sorted.</s><s coords="2,533.78,386.76,15.22,9.96;2,315.00,398.72,234.00,9.96;2,315.00,410.67,234.00,9.96;2,315.00,422.63,234.00,9.96;2,315.00,434.58,234.00,9.96;2,315.00,446.54,97.89,9.96">Initially, the ImageNet dataset was organized according to the lexical database of semantic relations WordNet <ref type="bibr" coords="2,315.00,422.63,74.79,9.96" target="#b12">(Fellbaum, 2010)</ref> under assumption that it will help to build new efficient classification algorithms <ref type="bibr" coords="2,525.01,434.58,19.19,9.96;2,315.00,446.54,93.46,9.96" target="#b34">(Russakovsky et al., 2015)</ref>.</s><s coords="2,418.80,446.54,130.20,9.96;2,315.00,458.49,234.00,9.96;2,315.00,470.45,233.99,9.96;2,315.00,482.40,234.00,9.96;2,315.00,494.36,234.00,9.96;2,315.00,506.31,87.34,9.96">Since then lots of works tried to incorporate hierarchy bias into the training process of deep neural networks: either in architecture <ref type="bibr" coords="2,526.03,470.45,18.37,9.96;2,315.00,482.40,73.56,9.96" target="#b16">(Hinton et al., 2015;</ref><ref type="bibr" coords="2,393.52,482.40,70.04,9.96" target="#b23">Malashin, 2016;</ref><ref type="bibr" coords="2,468.53,482.40,80.48,9.96" target="#b43">Xiao et al., 2014)</ref> or in loss function <ref type="bibr" coords="2,396.67,494.36,124.74,9.96" target="#b33">(Redmon and Farhadi, 2017;</ref><ref type="bibr" coords="2,524.70,494.36,24.29,9.96;2,315.00,506.31,82.91,9.96" target="#b6">Brust and Denzler, 2019)</ref>.</s><s coords="2,410.13,506.31,138.87,9.96;2,315.00,518.27,48.36,9.96">Some authors reported promising results.</s><s coords="2,369.69,518.27,179.31,9.96;2,315.00,530.22,234.00,9.96;2,315.00,542.18,234.00,9.96;2,315.00,554.13,72.71,9.96">However these findings are often derived in nonoptimal training settings <ref type="bibr" coords="2,459.35,530.22,89.65,9.96;2,315.00,542.18,23.79,9.96" target="#b6">(Brust and Denzler, 2019)</ref> or from closed datasets, which complicates fair comparison (e.g.</s><s coords="2,395.85,554.13,153.16,9.96"><ref type="bibr" coords="2,395.85,554.13,92.79,9.96" target="#b16">(Hinton et al., 2015)</ref> as example).</s></p><p><s coords="2,315.00,566.09,234.00,9.96;2,315.00,578.04,234.00,9.96;2,315.00,590.00,234.00,9.96;2,315.00,601.95,168.82,9.96">It is also well-known that, in few-shot leaning settings removing hierarchically related classes from the pretraining process significantly impacts the performance on these classes <ref type="bibr" coords="2,387.87,601.95,91.52,9.96" target="#b39">(Vinyals et al., 2016)</ref>.</s><s coords="2,490.07,601.95,58.93,9.96;2,315.00,613.91,234.00,9.96;2,315.00,625.86,95.77,9.96">This suggests that features of the network inherently capture hierarchical relationships.</s><s coords="2,418.98,626.47,130.02,8.80;2,315.00,638.43,234.00,8.80;2,315.00,650.38,109.29,8.80">We believe our result provide insight into why hierarchical structure is not used in state-of-the-art solutions.</s></p><p><s coords="2,315.00,668.30,106.74,8.81">Frequency Principle.</s><s coords="2,435.58,667.71,113.42,9.96;2,315.00,679.66,234.00,9.96;2,315.00,691.62,234.00,9.96;2,315.00,703.57,234.00,9.96;2,315.00,715.53,233.99,9.96;2,315.00,727.48,157.85,9.96">The Frequency Principle (FP) or Spectral bias is a phenomenon in neural network training, independently discovered in <ref type="bibr" coords="2,503.49,691.62,45.51,9.96;2,315.00,703.57,49.93,9.96" target="#b31">(Rahaman et al., 2019)</ref> and <ref type="bibr" coords="2,385.54,703.57,67.43,9.96" target="#b46">(Xu et al., 2019)</ref>, which states that the low-frequency components of the target function are learned first by the neural network.</s><s coords="2,479.42,727.48,69.58,9.96;3,63.00,74.49,234.00,9.96;3,63.00,86.45,233.99,9.96;3,63.00,98.40,161.65,9.96">The principle is formulated for the multidimensional spectrum of data (where the number of dimensions equals the number of elements in the input feature vector).</s><s coords="3,228.99,98.40,68.01,9.96;3,63.00,110.36,234.00,9.96;3,63.00,122.31,233.99,9.96;3,63.00,134.27,74.98,9.96">However, its interpretation is often associated with one-dimensional (time sequences) or two-dimensional (images) spaces <ref type="bibr" coords="3,63.00,134.27,70.55,9.96" target="#b45">(Xu et al., 2024)</ref>.</s></p><p><s coords="3,63.00,152.20,234.00,9.96;3,63.00,164.15,234.00,9.96;3,63.00,176.11,234.00,9.96;3,63.00,188.07,31.54,9.96">Theoretical justification of FP remains challenging <ref type="bibr" coords="3,63.00,164.15,77.52,9.96" target="#b45">(Xu et al., 2024)</ref> and its applicability may be limited in some cases <ref type="bibr" coords="3,148.84,176.11,88.47,9.96" target="#b48">(Zhang et al., 2020;</ref><ref type="bibr" coords="3,241.71,176.11,55.29,9.96;3,63.00,188.07,27.04,9.96">Wang et al., 2023a)</ref>.</s><s coords="3,100.07,188.07,196.93,9.96;3,63.00,200.02,234.00,9.96;3,63.00,211.98,145.25,9.96">Nevertheless, FP serves as a valuable tool to think about training dynamics, providing intuitive interpretations for existing results.</s><s coords="3,215.39,211.98,81.61,9.96;3,63.00,223.93,172.38,9.96;3,126.53,690.18,170.47,9.96;3,63.00,702.14,234.00,9.96;3,63.00,714.09,234.00,9.96;3,63.00,726.05,234.00,9.96;3,315.00,74.49,234.00,9.96;3,315.00,86.45,165.54,9.96">For example, deep image prior (DIP) <ref type="bibr" coords="3,142.86,223.93,92.53,9.96" target="#b38">(Ulyanov et al., 2018)</ref>   <ref type="bibr" coords="3,126.53,690.18,82.69,9.96" target="#b49">Zhang et al. (2021)</ref> emphasize that this property of neural networks helps prevent overfitting, even in conditions of significant overparameterization. Therefore, early stopping can be considered a method to prevent the learning of high-frequency noise components in individual signal examples.</s><s coords="3,484.91,86.45,64.09,9.96;3,315.00,98.40,234.00,9.96;3,315.00,110.36,234.00,9.96;3,315.00,122.31,234.00,9.96;3,315.00,134.27,19.92,9.96">Simplicity bias sometimes is considered to be a pitfall <ref type="bibr" coords="3,493.06,98.40,55.94,9.96;3,315.00,110.36,22.69,9.96" target="#b35">(Shah et al., 2020;</ref><ref type="bibr" coords="3,341.25,110.36,65.72,9.96" target="#b27">Pezeshki, 2022)</ref>, because it allows learning spurious correlations (shortcuts) and harms generalization.</s><s coords="3,343.40,134.27,205.60,9.96;3,315.00,146.22,234.00,9.96;3,315.00,158.18,111.99,9.96">These features are often not useful for generalization and are referred as shortcuts <ref type="bibr" coords="3,483.61,146.22,65.39,9.96;3,315.00,158.18,22.69,9.96" target="#b13">(Geirhos et al., 2020;</ref><ref type="bibr" coords="3,341.75,158.18,80.81,9.96" target="#b40">Wang et al., 2022)</ref>.</s><s coords="3,433.66,158.18,115.34,9.96;3,315.00,170.13,234.00,9.96;3,315.00,182.09,234.00,9.96;3,315.00,194.04,147.39,9.96">A separate research direction focuses on finding ways to reduce false correlations and studying learning dynamics in their presence <ref type="bibr" coords="3,315.00,194.04,69.92,9.96" target="#b27">(Pezeshki, 2022;</ref><ref type="bibr" coords="3,388.24,194.04,69.72,9.96" target="#b28">Qiu et al., 2024)</ref>.</s></p><p><s coords="3,315.00,211.98,234.00,9.96;3,315.00,223.93,234.00,9.96;3,315.00,235.89,192.32,9.96">The Frequency Principle and Simplicity Bias are closely related; the latter can be thought of as more general but less quantifiable of the former.</s><s coords="3,515.41,235.89,33.58,9.96;3,315.00,247.84,234.00,9.96;3,315.00,259.80,234.00,9.96;3,315.00,271.75,234.00,9.96"><ref type="bibr" coords="3,515.41,235.89,33.58,9.96;3,315.00,247.84,55.31,9.96" target="#b44">Xu and Zhou (2021)</ref> demonstrate that, in regression tasks, low frequencies carry the majority of the information needed for signal reconstruction and are easier to learn.</s></p><p><s coords="3,315.00,290.29,234.00,8.80;3,315.00,302.25,234.00,8.80;3,315.00,314.20,234.00,8.80;3,315.00,326.16,90.64,8.80">In simplicity bias terminology frequent features are called simple, and simplicity bias should manifest in accelerated learning of hypernym features common among many classes.</s><s coords="3,410.09,326.16,138.90,8.80;3,315.00,338.11,205.38,8.80">This is what we robustly observe in neural networks trained in practical settings.</s></p><p><s coords="3,315.00,356.03,79.51,8.81">Neural collapse.</s><s coords="3,398.95,355.44,150.05,9.96;3,315.00,367.39,234.00,9.96;3,315.00,379.35,234.00,9.96;3,315.00,391.30,209.05,9.96">Neural collapse (NC) phenomenon was first described by <ref type="bibr" coords="3,415.13,367.39,90.56,9.96" target="#b26">Papyan et al. (2020)</ref> and then was further extended to broader settings in <ref type="bibr" coords="3,522.02,379.35,26.98,9.96;3,315.00,391.30,49.01,9.96" target="#b10">(Dang et al., 2024)</ref>, <ref type="bibr" coords="3,371.22,391.30,78.37,9.96" target="#b11">(Fang et al., 2021)</ref> and other works.</s><s coords="3,528.25,391.30,20.75,9.96;3,315.00,403.26,234.00,9.96;3,315.00,415.21,234.00,9.96;3,315.00,427.17,49.98,9.96">Neural collapse is a state of the last layer of the neural classifier, which can be observed in the terminal phase of training.</s><s coords="3,373.99,427.17,175.01,9.96;3,315.00,439.12,234.00,9.96;3,315.00,451.08,234.00,9.96;3,315.00,463.03,234.00,9.96;3,315.00,474.99,26.56,9.96">It is characterized by zero within-class variability of features, when class means form vertices of the Equiangular Tight Frame (ETF), and classification decision is similar to prototypes <ref type="bibr" coords="3,493.68,463.03,55.33,9.96;3,315.00,474.99,22.13,9.96" target="#b37">(Snell et al., 2017)</ref>.</s><s coords="3,347.96,474.99,201.04,9.96;3,315.00,486.94,234.00,9.96;3,315.00,498.90,234.00,9.96;3,315.00,510.86,234.00,9.96;3,315.00,522.81,234.00,9.96;3,315.00,534.77,26.56,9.96">While the ETF configuration is optimal from the perspective of robustness to adversarial attacks <ref type="bibr" coords="3,315.00,498.90,92.74,9.96" target="#b26">(Papyan et al., 2020)</ref>, its contribution to generalization is arguable, as NC is observed only when accuracy on a balanced train set reaches 100% <ref type="bibr" coords="3,501.24,522.81,47.76,9.96;3,315.00,534.77,22.13,9.96" target="#b19">(Hui et al., 2022)</ref>.</s><s coords="3,345.97,534.77,203.02,9.96;3,315.00,546.72,234.00,9.96;3,315.00,558.68,233.99,9.96;3,315.00,570.63,112.33,9.96">Nevertheless knowledge about dynamics of the last layer is helpful, for example, for designing fewshot learning algorithms, where often only last layer is tuned <ref type="bibr" coords="3,343.22,570.63,79.69,9.96" target="#b47">(Yang et al., 2023)</ref>.</s></p><p><s coords="3,315.00,589.17,234.00,8.80;3,315.00,601.13,234.00,8.80;3,315.00,613.08,234.00,8.80;3,315.00,625.04,77.57,8.80">Neural collapse of penultimate layer can be seen as deeper representation of perfectly diagonal confusion matrix, which doesn't posses any information about hyponymy relations.</s><s coords="3,399.61,625.04,149.39,8.80;3,315.00,636.99,234.00,8.80;3,315.00,648.95,100.76,8.80">From the perspective of hypernym bias NC is the final stage of top-to-bottom hierarchical label clustering process.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" coords="3,315.00,679.45,131.02,10.57">METHODOLOGY</head><p><s coords="3,315.00,706.62,234.00,9.96;3,315.00,718.57,234.00,9.96;3,315.00,730.53,234.00,9.96;4,63.00,74.49,77.18,9.96">We base our conclusions from experiments with Im-ageNet dataset, which classes are organized according to lexical database of semantic relations WordNet <ref type="bibr" coords="4,63.00,74.49,72.75,9.96" target="#b12">(Fellbaum, 2010)</ref>.</s><s coords="4,146.64,74.49,150.36,9.96;4,63.00,86.45,234.00,9.96;4,63.00,98.40,16.91,9.96">Validation of these conclusions on additional datasets will be presented later in the paper.</s><s coords="4,88.31,98.40,208.69,9.96;4,63.00,110.36,234.00,9.96;4,63.00,122.31,175.44,9.96">WordNet defines a graph where the nodes are synsets (sets of synonyms), and the edges represent the hypernymy-hyponymy relationships.</s><s coords="4,243.48,122.31,53.52,9.96;4,63.00,134.27,234.00,9.96">Examples of synsets are: {car, automobile} and {small, compact}.</s><s coords="4,63.00,146.22,234.00,9.96;4,63.00,158.18,210.16,9.96">Synset {dog} is a hypernym relative to the synsets {bulldog} and {spitz}, which are its hyponyms.</s><s coords="4,279.03,158.18,17.97,9.96;4,63.00,170.13,234.00,9.96;4,63.00,182.09,234.00,9.96;4,63.00,194.04,234.00,9.96;4,63.00,206.00,234.00,9.96;4,63.00,217.95,167.32,9.96">It is important to note that the hyponymy relationship is not the only one; for instance, meronymy (part-whole, e.g., {wheel}-{car}), troponymy (one expressing an aspect of the other, e.g., {whisper}-{speak}), and others relationships are often considered.</s><s coords="4,237.35,217.95,59.65,9.96;4,63.00,229.91,234.00,9.96;4,63.00,241.86,234.00,9.96;4,63.00,253.82,234.00,9.96;4,63.00,265.77,233.99,9.96;4,63.00,277.73,75.04,9.96">While it's arguably impossible to determine the relationships fully and consistently, if we view hypernym bias through the lens of simplicity bias <ref type="bibr" coords="4,177.02,253.82,80.85,9.96" target="#b2">(Arpit et al., 2017)</ref>, the imperfect nature of these relationships should not negate the phenomenon.</s><s coords="4,143.46,277.73,153.53,9.96;4,63.00,289.68,178.81,9.96">This is because hypernyms tend to share features across multiple hyponyms.</s></p><p><s coords="4,63.00,307.62,234.00,9.96;4,63.00,319.57,209.49,9.96">For first series of experiments we used simplified to a tree WordNet graph <ref type="bibr" coords="4,154.77,319.57,113.29,9.96" target="#b34">(Russakovsky et al., 2015)</ref>.</s><s coords="4,277.58,319.57,19.42,9.96;4,63.00,331.53,234.00,9.96;4,63.00,343.48,149.70,9.96">This allows as to interpret hyponym classifier as unambiguous greedy classifier of hypernyms.</s><s coords="4,217.01,343.48,79.99,9.96;4,63.00,355.44,234.00,9.96;4,63.00,367.39,154.53,9.96">For manifold analysis, we utilize complete WordNet graph, which permits multiple parent relationships.</s><s coords="4,226.31,367.39,70.69,9.96;4,63.00,379.35,234.00,9.96;4,63.00,391.30,234.00,9.96">To avoid confusion we refer to the simplified version as WordNet tree, while the full version is referred to as WordNet graph.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" coords="4,63.00,417.22,183.54,8.81">Greedy hypernym classification</head><p><s coords="4,63.00,438.58,65.99,8.81">Label spaces.</s><s coords="4,134.79,437.98,38.55,10.93;4,178.23,438.65,26.98,8.74;4,205.21,437.08,12.26,6.73;4,222.25,437.98,74.26,10.32;4,63.00,449.94,160.32,10.32;4,223.32,449.03,10.64,6.73;4,239.63,449.94,57.36,9.96;4,63.00,462.56,12.90,9.65;4,81.25,461.90,215.75,9.96;4,63.00,474.52,36.97,9.65;4,99.97,472.89,102.17,10.93;4,63.41,510.44,14.03,6.12;4,79.60,496.49,36.80,10.32;4,127.35,495.82,169.25,10.32">Let H N H = {h} N H be the set of N H hyponyms (classes), and S Ns = {s} N S be the set of N S hypernyms (superclasses), where each superclass s i = {h} Ns i is defined such that s∈S s = H N h and s i s j = ∅ ∀s i , s j ∈ S, i ̸ = j.</s></p><p><s coords="4,284.27,517.72,12.73,9.96;4,63.00,529.67,211.56,10.32">(1) In the case of ImageNet, we have N H = 1000.</s><s coords="4,282.45,529.67,14.56,9.96;4,63.00,541.63,234.00,9.96;4,63.00,553.58,119.62,9.96">For simplicity we omit subscript notations indicating the sizes of sets where possible.</s><s coords="4,187.28,553.58,109.73,9.96;4,63.00,565.54,234.00,9.96;4,63.00,577.49,234.00,9.96;4,63.00,589.45,172.22,9.96">A standard classifier network f (x; θ) equipped with softmax, maps image x to a probability distribution over hyponyms (classes) h ∈ H based on the current weights θ.</s><s coords="4,241.56,589.45,55.44,9.96;4,63.00,601.40,211.43,9.96">The training objective utilizes the cross-entropy loss function:</s></p><formula xml:id="formula_0" coords="4,112.67,622.82,184.33,20.81">L CE = - h∈H log f h (x; θ)y h (x),<label>(2)</label></formula><p><s coords="4,63.00,652.82,234.00,10.32;4,63.00,664.78,234.00,10.32;4,63.00,676.73,149.26,9.96">where y h (x) represents true probability that x belongs to h (0 or 1 in one-hot encoding), and f h (x; θ) is the estimated probability of the same.</s></p><p><s coords="4,63.00,694.66,234.00,9.96;4,63.00,706.62,233.99,9.96;4,63.00,718.57,74.69,9.96">We consider f (x; θ) as a function that also maps an image to different label spaces greedily (not considering confidence level).</s><s coords="4,143.73,718.57,153.27,9.96;4,63.00,730.53,86.52,9.96">We denote labels and label predictions with ˆsymbol.</s><s coords="4,156.52,730.53,140.48,9.96;4,315.00,74.49,39.29,9.96">The predicted hyponym is then given by:</s></p><formula xml:id="formula_1" coords="4,367.30,95.16,181.70,13.22">fH (x; θ) = argmax h∈H f h (x; θ),<label>(3)</label></formula><p><s coords="4,315.00,119.75,152.98,9.96;4,467.98,124.89,16.86,6.12;4,485.33,120.42,23.49,9.65">and the true label ŷH (x) = argmax h∈H y h (x)</s></p><p><s coords="4,315.00,137.68,234.00,9.96;4,315.00,149.64,234.00,9.96;4,315.00,161.59,234.00,10.32;4,315.00,173.55,234.00,9.96;4,315.00,185.50,38.48,9.96">We explore manifestation of hypernym bias through the convergence dynamics by introducing a trivial mapping T H→S : H → S, which maps each hyponym to its parent hypernym at a specified level in the Word-Net tree.</s><s coords="4,357.91,185.50,133.13,9.96">The predicted hypernym then:</s></p><formula xml:id="formula_2" coords="4,374.89,206.17,174.11,12.28">fS (x; θ) = T H→S ( fH (x; θ)),<label>(4)</label></formula><formula xml:id="formula_3" coords="4,382.80,231.83,166.20,10.32">ŷS (x) = T H→S (ŷ H (x)).<label>(5)</label></formula><p><s coords="4,315.00,250.83,115.17,10.32;4,435.90,250.83,113.11,9.96;4,315.00,262.78,234.00,9.96;4,317.15,272.78,227.58,11.92">In our experiments T H→S is implemented as simple reverse traversal of the WordNet tree starting from fH ; it returns the first match with any element of S.</s></p><p><s coords="4,315.00,292.67,234.00,9.96;4,315.00,304.63,234.00,9.96;4,315.00,316.58,234.00,9.96">To evaluate the specific impact of hypernym relationships in the WordNet tree, we introduce R, a randomly generated label space that is isomorphic to S.</s></p><p><s coords="4,315.00,328.54,234.00,9.96;4,315.00,340.49,234.00,9.96;4,315.00,352.45,234.00,9.96">Isomorphism here means that there is a one-to-one correspondence between labels in R and S, and the sizes of the corresponding superclasses are identical.</s><s coords="4,315.00,364.40,234.00,9.96;4,315.00,377.02,132.62,8.74">Formally there exists bijection g : R → S, such that ∀r ∈ R ∃s ∈ S : |r| = |g(r)|.</s><s coords="4,453.12,376.36,95.88,9.96;4,315.00,388.31,234.00,9.96;4,315.00,400.27,234.00,9.96;4,315.00,412.22,234.00,9.96;4,315.00,424.18,234.00,9.96;4,315.00,436.13,232.32,9.96">This ensures that any differences in network performance on R and S can be attributed to the semantic structure encoded in S. For example, while S might group classes into "Animals", "Artifacts", and "Others", R would have three groups of the same sizes but with randomly assigned classes.</s></p><p><s coords="4,315.00,454.07,78.81,10.32;4,398.30,454.07,45.13,10.32;4,448.43,454.07,100.57,9.96;4,315.00,466.02,234.00,9.96;4,315.00,477.98,234.00,9.96;4,315.00,489.93,157.09,9.96">Given that T H→R and T H→S are predefined and not learned, we can assume that the same network f (x; θ) maps each image to three label spaces simultaneously without interfering training process:</s></p><formula xml:id="formula_4" coords="4,335.03,510.60,213.98,11.92">fH (x; θ) → H, fS (x; θ) → S, fR (x; θ) → R.<label>(6)</label></formula><p><s coords="4,315.00,535.19,63.47,9.96;4,397.33,535.19,30.46,9.96;4,446.65,535.19,26.64,9.96;4,492.15,535.19,24.68,9.96;4,535.69,535.19,13.31,9.96;4,315.68,547.81,86.48,8.74">Corresponding ground truths labels are ŷH (x), ŷS (x), ŷR (x).</s><s coords="4,414.68,547.15,134.33,9.96;4,315.00,559.10,119.10,9.96">Thus we can study accuracy metrics in each label space.</s></p><p><s coords="4,315.00,577.63,40.85,8.81">Metrics.</s><s coords="4,366.53,577.03,145.40,10.32;4,511.93,575.31,11.84,6.73;4,511.93,582.52,12.91,6.12;4,530.74,577.03,18.26,9.96;4,315.00,588.99,35.34,10.32;4,354.72,588.99,194.28,9.96">Let the test dataset D = {x i } N D i=1 contain N D images for which label function ŷ is defined.</s><s coords="4,315.00,600.94,234.00,9.96;4,315.00,612.90,99.55,9.96">The accuracy of classification in label space X after t epochs is estimated as:</s></p><formula xml:id="formula_5" coords="4,342.77,634.98,206.23,30.44">A(X , t) = 1 N D N D i=1 I( fX (x i ; θ t ) = ŷX (x i )).<label>(7)</label></formula><p><s coords="4,315.00,676.73,139.20,9.96">We normalize A(X , t) ∈ [0, 100].</s></p><p><s coords="4,315.00,694.66,234.00,9.96;4,315.00,706.62,234.00,9.96;4,315.00,718.57,131.50,9.96">Given the accuracy of recognizing hyponyms A(H, t), accuracy of recognizing random superclasses A(R, t) can be estimated theoretically.</s><s coords="4,450.75,718.57,98.25,9.96;4,315.00,730.53,130.34,9.96">We derive and validate exact formula in Appendix A.</s></p><p><s coords="5,63.00,74.49,234.00,9.96;5,63.00,86.45,234.00,9.96;5,63.00,98.40,234.00,9.96;5,63.00,110.36,83.96,9.96">Absolute accuracy can be misleading indicator of convergence since it varies a lot (as recognizing superclasses is inherently easier due to higher probability of guessing correctly).</s><s coords="5,151.38,110.36,145.62,9.96;5,63.00,122.31,215.27,10.32">To address this, we introduce relative accuracy A R (X , t) at epoch t for assessment.</s><s coords="5,282.47,122.31,14.53,9.96;5,63.00,134.27,52.89,9.96;5,115.89,139.41,3.01,6.12;5,119.40,134.27,177.61,9.96;5,63.00,146.22,11.68,9.96">Let T = argmax t A(X , t), then relative accuracy is defined as:</s></p><formula xml:id="formula_6" coords="5,133.90,155.85,163.10,22.31">A R (X , t) = A(X , t) A(X , T ) ,<label>(8)</label></formula><p><s coords="5,63.00,186.18,158.29,10.32">which ensures that A R (X , t) ∈ [0, 1].</s></p><p><s coords="5,63.00,204.12,234.00,9.96;5,63.00,216.07,234.00,9.96;5,63.00,228.03,77.10,9.96">Expression (8) does not account for different probabilities of random guessing that depends on superclass sizes distribution.</s><s coords="5,146.14,228.03,150.85,9.96;5,63.00,239.98,219.77,10.32">To address this, we also introduce the relative gain in accuracy, G R (X , t), defined as:</s></p><formula xml:id="formula_7" coords="5,115.44,261.90,181.56,22.31">G R (X , t) = A(X , t) -B(X ) A(X , T ) -B(X ) ,<label>(9)</label></formula><p><s coords="5,63.00,296.06,234.00,9.96;5,63.00,308.01,142.47,9.96">where B(X ) is the accuracy of a random guess, taking into account sets imbalance.</s><s coords="5,212.29,308.01,84.71,9.96;5,63.00,319.97,234.00,9.96;5,63.00,331.92,137.16,9.96">This baseline accuracy can be estimated as B(X ) = EA(R, 0), i.e. expected accuracy before training.</s><s coords="5,204.27,331.92,92.73,9.96;5,63.00,343.88,234.00,9.96;5,63.00,355.83,234.00,10.32;5,63.00,367.79,234.00,9.96;5,63.00,379.74,216.00,10.32">In the next section we show that estimates of accuracy gain for random superclasses G R (R) and for hyponyms G R (H) are identical, allowing us to measure impact of hypernymy relation fairly by comparing A R (R) against A R (S).</s></p><p><s coords="5,63.00,397.67,234.00,9.96;5,63.00,409.63,234.00,9.96;5,63.00,421.59,234.00,9.96;5,63.00,433.54,234.00,9.96;5,63.00,445.50,234.00,9.96;5,63.00,457.45,234.00,9.96;5,63.00,469.41,27.47,9.96">Though observation of accelerated formation of hypernym features from the perspective of different accuracy metrics are very practical and intuitive manifestation of hypernym bias, these metrics are not integral (need arbitrary choices of hypernym level), and importantly they doesn't generalize to describe features of deeper layers.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" coords="5,63.00,496.01,224.65,8.81;5,89.09,507.96,126.11,8.81">Assessing evolution of class hierarchical structure of the manifold</head><p><s coords="5,63.00,528.87,234.00,9.96;5,63.00,540.83,234.00,9.96;5,63.00,552.78,164.39,9.96">Assuming that features lie near low-dimensional manifold, we evaluate how well this feature manifold aligns with WordNet graph on each epoch.</s><s coords="5,235.64,552.78,61.36,9.96;5,63.00,564.74,234.00,9.96;5,63.00,576.69,234.00,9.96;5,63.00,588.65,234.00,9.96;5,63.00,600.60,201.27,9.96">Our approach has three major steps: a) define the distances between classes in feature space, b) define distances between classes in WordNet graph and c) compare two distance matrices via cophenetic correlation coefficient.</s></p><p><s coords="5,63.00,619.13,118.06,8.81">Graph in feature space.</s><s coords="5,185.83,618.53,111.18,9.96;5,63.00,630.49,234.00,9.96;5,63.00,642.44,97.14,9.96">To estimate distances between feature sets within manifold we adapt approach from <ref type="bibr" coords="5,86.09,642.44,69.61,9.96" target="#b20">(Jin et al., 2020)</ref>.</s><s coords="5,164.41,642.44,132.59,9.96;5,63.00,654.40,234.00,9.96;5,63.00,666.35,216.23,9.96">First we sample 2 × K training examples per each of C classes, and divide them into non-overlapping query and support sets Q and S:</s></p><formula xml:id="formula_8" coords="5,70.68,687.05,226.32,16.25">Q = U c = {u c } K C , S = V c = {v c } K C ,<label>(10)</label></formula><p><s coords="5,63.00,718.57,25.48,9.96">where</s></p><formula xml:id="formula_9" coords="5,63.00,717.67,234.00,23.18">U c ∩ V c = ∅, ∀c ∈ [0, C]. Assuming suitable metric, we define distance d f (u ci , V cj ) between query</formula><p><s coords="5,315.00,74.49,74.13,10.32;5,392.55,74.49,76.41,9.96;5,471.17,73.58,6.49,6.12;5,481.88,74.49,67.12,9.96;5,315.00,87.11,7.61,9.65;5,326.83,86.45,141.76,9.96">feature of class c i and support set V cj of another class c j as minimum individual distance:</s></p><formula xml:id="formula_10" coords="5,369.69,105.58,179.31,17.36">d f (c i , V cj ) = min v∈V c j d f (u c , v).<label>(11)</label></formula><p><s coords="5,315.00,138.05,234.00,9.96;5,315.00,150.00,93.86,10.32;5,412.27,150.00,76.36,9.96;5,490.84,149.10,6.49,6.12;5,501.53,150.00,47.47,9.96;5,315.00,161.96,85.67,9.96">Next we measure probability that distance between query point of class c i and support set V cj is less than predefined radius r:</s></p><formula xml:id="formula_11" coords="5,337.60,181.09,211.40,26.67">P r (c i , c j ) = P (d f (u ci , V cj ) &lt; r) = (12) = E u∼U c i [I{d f (u, V cj ) &lt; r}] ,<label>(13)</label></formula><p><s coords="5,315.00,217.97,67.83,10.32;5,385.04,217.06,6.49,6.12;5,392.82,217.97,137.94,9.96">where I{d f (u, V cj ) &lt; r} is the indicator function.</s></p><p><s coords="5,315.00,235.90,206.34,9.96">Finally we estimate similarity of two classes as:</s></p><formula xml:id="formula_12" coords="5,355.91,253.13,193.09,26.29">ρ(c i , c j ) = 1 r max rmax 0 P r (c i , c j ) dr,<label>(14)</label></formula><p><s coords="5,315.00,287.03,49.20,10.32;5,368.02,287.03,85.91,9.96">where r max is maximum radius.</s></p><p><s coords="5,315.00,304.96,234.00,10.32;5,315.00,316.92,234.00,9.96;5,315.00,328.87,87.71,9.96">The value ρ(c i , c j ) ∈ [0, 1] represents mutual cover when i ̸ = j and self-cover when i = j, as described by <ref type="bibr" coords="5,329.15,328.87,69.22,9.96" target="#b20">Jin et al. (2020)</ref>.</s><s coords="5,408.08,328.87,140.92,10.32;5,315.00,340.83,114.34,9.96;5,429.34,339.92,6.22,6.12;5,439.76,340.83,109.24,9.96;5,315.00,352.78,94.74,9.96;5,410.10,351.88,6.49,6.12;5,421.84,352.78,127.16,9.96;5,315.00,364.74,233.99,9.96;5,315.00,376.70,156.42,9.96;5,473.64,375.79,6.49,6.12;5,481.42,376.70,2.77,9.96">Similarity ρ(c i , c j ) = 1 indicates that every query feature u ci is within zero distance to at least one feature v cj in support set of the class j, low values indicate that there are many query features that are far from any features in V cj .</s><s coords="5,490.68,376.70,58.32,9.96">We construct</s></p><formula xml:id="formula_13" coords="5,315.00,388.65,116.62,10.32">similarity matrix A = [a ij ]</formula><p><s coords="5,433.29,388.65,115.71,10.32;5,315.00,400.61,234.00,9.96;5,315.00,412.56,140.28,9.96">, where a ij = ρ(c i , c j ) and treat its elements as probability that edge between i-th and j-th node of the graph exists.</s><s coords="5,459.38,412.56,89.61,9.96;5,315.00,424.52,148.15,10.32;5,467.75,424.52,79.60,9.96">Transformation from similarity to a distance matrix D f is straightforward:</s></p><formula xml:id="formula_14" coords="5,377.39,445.05,167.19,10.32">D F = [d F (i, j)]] = 1 -A. (<label>15</label></formula><formula xml:id="formula_15" coords="5,544.57,445.05,4.43,9.96">)</formula><p><s coords="5,315.00,472.16,171.61,8.81">Label distance in WordNet graph.</s><s coords="5,491.42,471.56,57.58,9.96;5,315.00,483.52,234.00,9.96;5,315.00,495.47,98.85,9.96">Distance matrix between class labels according to WordNet structure can be defined as:</s></p><formula xml:id="formula_16" coords="5,394.46,516.01,154.54,10.32">D W = [d W (i, j)] ,<label>(16)</label></formula><p><s coords="5,315.00,536.54,234.00,10.32;5,315.00,548.50,87.00,9.96">where d w (i, j) is the shortest path between synset i and j in the graph.</s><s coords="5,408.81,548.50,140.19,9.96;5,315.00,560.45,133.44,9.96">With this definition we can use full (not tree) WordNet graph.</s></p><p><s coords="5,315.00,578.98,96.28,8.81">Graph comparison.</s><s coords="5,418.01,578.39,130.99,9.96;5,315.00,590.34,153.33,9.96">To compare graphs we utilize Cophenetic Correlation Coefficient:</s></p><formula xml:id="formula_17" coords="5,334.12,625.17,210.45,33.19">CCC = i&lt;j dW (i, j) dF (i, j) i&lt;j dW (i, j) 2 i&lt;j dF (i, j) 2 , (<label>17</label></formula><formula xml:id="formula_18" coords="5,544.57,635.36,4.43,9.96">)</formula><formula xml:id="formula_19" coords="5,317.01,686.72,231.99,23.88">dW (i, j) = d W (i, j) -D W , dF (i, j) = d F (i, j) -D F ,<label>(18)</label></formula><p><s coords="5,315.00,718.57,42.81,10.32;5,363.42,718.57,35.82,10.32;5,404.84,718.57,144.16,9.96;5,315.00,730.53,189.07,10.32;5,509.26,730.53,35.41,10.32;5,546.23,730.53,2.77,9.96">where D F and D W are the mean of all pairwise distances in the original distance matrices D F and D W .</s></p><p><s coords="6,63.00,74.49,234.00,9.96;6,63.00,86.45,65.93,9.96">CCC ∈ [-1, 1] measures linear correlation of two distance matrices.</s></p><p><s coords="6,63.00,104.98,36.33,8.81">Metric.</s><s coords="6,107.10,104.38,189.89,9.96;6,63.00,116.33,234.00,9.96;6,63.00,128.29,234.00,9.96;6,63.00,140.24,163.91,9.96">Examples are not distributed uniformly in feature space in the course of training: fixed volume contains different number of sample points and distance should be adjusted accordingly.</s><s coords="6,231.75,140.24,65.25,9.96;6,63.00,152.20,234.00,9.96;6,63.00,164.15,234.00,9.96;6,63.00,176.11,67.18,9.96">As can be seen from Figure <ref type="figure" coords="6,120.64,152.20,9.96,9.96" target="#fig_2">2a</ref> with the use of Euclidean metric in feature space there is no trend to increase mutual to self-cover ratio.</s><s coords="6,136.64,176.11,160.36,9.96;6,63.00,188.07,234.00,9.96;6,63.00,200.02,234.00,9.96;6,63.00,211.98,234.00,9.96;6,63.00,223.93,177.90,9.96">This suggests the potential issues in the measurement of similarity, as the Euclidean metric likely captures the properties of the entire manifold rather than specific characteristics of individual class manifolds and their mutual relationships.</s><s coords="6,245.15,223.93,51.84,9.96;6,315.00,480.07,120.29,8.81">In this work Lower level hypernyms.</s><s coords="6,442.56,479.47,106.44,9.96;6,315.00,491.43,234.00,9.96;6,315.00,503.38,234.00,9.96;6,315.00,515.34,46.51,9.96">We demonstrate the robustness of the observed phenomenon to different specifications of S with the use of different levels of hypernymy tree.</s></p><p><s coords="6,315.00,533.87,193.85,8.81">Architectures and hyper parameters.</s><s coords="6,520.98,533.27,28.02,9.96;6,315.00,545.22,234.00,9.96;6,315.00,557.18,234.00,9.96;6,315.00,569.13,234.00,9.96">In experiments we used the ResNet-50, ResNet-18 <ref type="bibr" coords="6,533.23,545.22,15.77,9.96;6,315.00,557.18,54.46,9.96" target="#b15">(He et al., 2016)</ref>, ViT-B/16 (Alexey Dosovitskiy, 2020) and MobileNet-V3 <ref type="bibr" coords="6,397.38,569.13,90.54,9.96" target="#b17">(Howard et al., 2019)</ref> architectures.</s><s coords="6,315.00,581.09,234.00,9.96;6,315.00,593.04,184.56,9.96">All models were trained with the use of augmentation, starting from randomly initialized weights.</s><s coords="6,503.85,593.04,45.15,9.96;6,315.00,605.00,234.00,9.96;6,315.00,616.95,234.00,9.96;6,315.00,628.91,49.60,9.96">Except for Vit-B/16, the training parameters gave results comparable with the state of the art results for these architectures.</s><s coords="6,369.94,628.91,179.06,9.96;6,315.00,640.87,234.00,9.96;6,315.00,652.82,211.53,9.96">For neural collapse analysis, we carefully reproduced all the parameters used by <ref type="bibr" coords="6,489.96,640.87,59.04,9.96;6,315.00,652.82,27.67,9.96" target="#b26">Papyan et al. (2020)</ref> and trained ResNet-152 for 350 epochs.</s><s coords="6,535.17,652.82,13.84,9.96;6,315.00,664.78,234.00,9.96;6,315.00,676.73,29.59,9.96">We did not perform a learning rate search and fixed it to be 0.1.</s><s coords="6,350.61,676.73,198.39,9.96;6,315.00,688.69,234.00,9.96;6,315.00,700.64,94.76,9.96">For the greedy hypernym classifier we report metrics on validation set, while neural collapse is evaluated on the train set.</s><s coords="6,413.92,700.64,135.08,9.96;6,315.00,712.60,193.62,9.96">To compute CCC we embedded features into a 10-dimensional feature space.</s></p><p><s coords="6,315.00,730.53,234.00,9.96;7,63.00,74.49,234.00,9.96;7,63.00,86.45,88.12,9.96">Details on superclass formation with the use of Word-Net tree and training parameters can be found in Appendix Appendix B.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" coords="7,63.00,111.92,62.97,8.81">Results</head><p><s coords="7,63.00,133.29,141.64,8.81">Greedy hypernym classifier.</s><s coords="7,211.48,132.69,85.52,9.96;7,63.00,144.65,234.00,9.96;7,63.00,156.60,234.00,9.96;7,63.00,168.56,92.85,9.96">Table <ref type="table" coords="7,239.68,132.69,4.98,9.96" target="#tab_1">1</ref> shows final absolute accuracy, and Figure <ref type="figure" coords="7,197.20,144.65,9.96,9.96" target="#fig_1">1a</ref> displays relative accuracy curves in hyponym, hypernym and random superclass label spaces.</s><s coords="7,162.24,168.56,134.76,9.96;7,63.00,180.51,234.00,9.96;7,63.00,192.47,234.00,9.96;7,63.00,204.42,103.66,9.96">Across all architectures during initial epochs of training relative accuracy for hypernyms increases significantly faster than for the rest of considered label spaces.</s><s coords="7,172.33,204.42,124.67,9.96;7,63.00,216.38,234.00,9.96;7,63.00,228.33,234.00,9.96;7,63.00,240.29,234.00,9.96">From the table, it is evident that the recognition accuracy of superclasses formed with the use of the WordNet tree is significantly higher compared to random superclasses of the same size.</s><s coords="7,63.00,252.24,234.00,9.96;7,63.00,264.20,197.42,9.96">However, the final hypernym classification accuracy strongly depends on the size of the network.</s><s coords="7,266.84,264.20,30.16,9.96;7,63.00,276.15,234.00,9.96;7,63.00,288.11,234.00,9.96;7,63.00,300.06,157.63,9.96">For instance, ResNet-50 outperforms ResNet-18 by 61% in terms of error reduction for hypernyms, compared to a 43% improvement for hyponyms.</s><s coords="7,229.47,300.06,67.53,9.96;7,63.00,312.02,234.00,9.96;7,63.00,323.97,117.66,9.96">This should be taken into account when using interpretation through the lens of simplicity bias.</s><s coords="7,187.92,323.97,109.08,9.96;7,63.00,335.93,193.70,9.96">Also hypernym accuracy never fully plateaus until the end of training.</s><s coords="7,261.00,335.93,36.00,9.96;7,63.00,347.88,234.00,9.96;7,63.00,359.84,79.26,9.96">Figure <ref type="figure" coords="7,292.02,335.93,4.98,9.96" target="#fig_3">3</ref> shows relative accuracy gain computed according to (9) for ResNet-50.</s><s coords="7,147.39,359.84,149.61,9.96">The metric behaves identically for</s></p><formula xml:id="formula_20" coords="7,63.00,634.89,212.61,10.32">A R (S n ) against A R (R n ) (instead of A R (H 1000 )).</formula><p><s coords="7,63.00,652.82,234.00,9.96;7,63.00,664.78,234.00,9.96;7,63.00,676.73,233.99,9.96;7,63.00,688.69,39.87,9.96">As an indicator of the training convergence with respect to number of hypernyms we considered the number of epochs required to reach 95% of the maximum accuracy.</s><s coords="7,107.29,688.69,159.42,9.96">The results are depicted in Figure <ref type="figure" coords="7,258.97,688.69,3.87,9.96">4</ref>.</s></p><p><s coords="7,63.00,706.62,234.00,9.96;7,63.00,718.57,234.00,9.96;7,63.00,730.53,200.93,9.96">As observed, the recognition accuracy of hypernyms converges significantly faster, although smaller hypernyms tend to take longer to reach convergence.</s><s coords="7,268.15,730.53,28.85,9.96;7,315.00,271.78,41.94,9.96;7,376.84,271.78,172.16,9.96;7,315.00,283.74,185.87,9.96;7,315.00,319.60,234.00,9.96;7,315.00,331.56,57.21,10.32;7,372.71,331.56,176.29,9.96;7,315.00,344.18,12.41,9.65;7,327.91,343.51,87.67,9.96">For in-Figure <ref type="figure" coords="7,349.20,271.78,3.87,9.96">4</ref>: 95% accuracy convergence period (in epochs) for different number of hypernyms stance, ResNet-50 training reaches 95% accuracy in 6 epochs for S 3 , while for randomly formed superclasses R 3 , it takes 64 epochs.</s><s coords="7,420.69,343.51,128.32,9.96;7,315.00,355.47,234.00,9.96;7,315.00,367.42,162.97,9.96">Interestingly, the effect of accelerated hypernym learning is more salient in larger ResNet-50 than in smaller ResNet-18.</s><s coords="7,482.24,367.42,66.76,9.96;7,315.00,379.38,234.00,9.96;7,315.00,391.33,234.00,9.96;7,315.00,403.29,234.00,9.96;7,315.00,415.24,234.00,9.96;7,315.00,427.20,91.25,9.96">In contrast, the convergence rate for hypernyms is approximately equal for both networks (orange lines in Figure <ref type="figure" coords="7,499.33,391.33,3.87,9.96">4</ref>), whereas the features required for hyponym classification consistently take longer to converge (gray lines on the same graph) in ResNet-50.</s></p><p><s coords="7,315.00,445.13,233.99,9.96;7,315.00,457.09,233.99,9.96;7,315.00,469.04,234.00,9.96;7,315.00,481.00,234.00,9.96;7,315.00,492.95,51.56,9.96">Although our choice of grouping synsets within one WordNet level was arbitrary, the absence of such grouping and the choice of a different convergence threshold do not affect the conclusions obtained in this section.</s><s coords="7,370.81,492.95,178.20,9.96;7,315.00,504.91,234.00,9.96;7,315.00,516.86,171.39,9.96">More experiments, including experiments with animals hypernyms, different synset grouping strategies can be found in Appendix C.</s></p><p><s coords="7,315.00,535.39,79.27,8.81">Neural collapse.</s><s coords="7,398.64,534.80,150.36,9.96;7,315.00,546.75,234.00,9.96;7,315.00,558.71,64.73,10.32;7,384.59,558.71,164.41,9.96;7,315.00,570.66,74.08,9.96">In Figure <ref type="figure" coords="7,442.11,534.80,4.98,9.96" target="#fig_4">5</ref> we plot graphs demonstrating that some properties of neural collapse are observed in S 3 hypernym label space on early iterations of training.</s><s coords="7,395.34,570.66,153.65,9.96;7,315.00,582.62,141.54,9.96">UMAP embeddings of penultimate features are shown in Figure <ref type="figure" coords="7,448.79,582.62,3.87,9.96" target="#fig_1">1</ref>.</s><s coords="7,464.86,582.62,84.14,9.96;7,315.00,594.57,234.00,9.96;7,315.00,606.53,55.61,9.96">We provide all the graphs as well as more UMAP visualizations in the Appendix C.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" coords="7,315.00,636.53,185.55,8.81;7,341.09,648.48,192.26,8.81">Manifold hierarchical structure, layer-wise and data frequency analysis</head><p><s coords="7,315.00,671.35,234.00,8.81;7,315.00,683.30,49.44,8.81">Alignment with WordNet graph and layer-wise dynamics.</s><s coords="7,372.33,682.71,176.66,9.96;7,315.00,694.66,234.00,9.96;7,315.00,706.62,234.00,9.96;7,315.00,718.57,32.09,9.96">It is well-known that layers of convolutional neural networks converge faster than the entire neural network <ref type="bibr" coords="7,403.24,706.62,87.75,9.96" target="#b29">(Raghu et al., 2017;</ref><ref type="bibr" coords="7,494.84,706.62,54.17,9.96;7,315.00,718.57,27.51,9.96">Wang et al., 2023b)</ref>.</s><s coords="7,354.35,718.57,194.65,9.96;7,315.00,730.53,233.99,9.96;8,315.00,397.33,234.00,9.96;8,315.00,409.28,176.64,9.96">Residual skip connections can possibly help transmit less distorted low-level feature information to   in hypernym label spaces (Figure <ref type="figure" coords="8,461.46,397.33,8.02,9.96" target="#fig_5">6c</ref>), which show layers' redness for hypernym classification.</s><s coords="8,498.76,409.28,50.23,9.96;8,315.00,421.24,234.00,9.96;8,315.00,433.19,234.00,9.96;8,315.00,445.15,75.75,9.96">We observe that probes from higher levels consistently and similarly provide better accuracy for both hyponym and hypernym spaces.</s><s coords="8,394.84,445.15,154.16,9.96;8,315.00,457.10,234.00,9.96;8,315.00,469.06,234.00,9.96;8,315.00,481.01,156.17,9.96">In no experiment do we observe that hypernym classification based on intermediate layers can be performed with accuracy comparable to classification based on the higher layer.</s><s coords="8,476.40,481.01,72.59,9.96;8,315.00,492.97,234.00,9.96;8,315.00,504.92,172.12,9.96">Thus, we cannot conclude that knowledge sufficient for recognizing hypernyms is contained in the first layers.</s></p><p><s coords="8,315.00,523.45,97.77,8.81">Frequency analysis.</s><s coords="8,419.43,522.86,129.57,9.96;8,315.00,534.81,234.00,9.96;8,315.00,546.77,234.00,9.96;8,315.00,558.72,234.00,9.96;8,315.00,570.68,234.00,9.96">The Frequency Principle <ref type="bibr" coords="8,532.12,522.86,16.88,9.96;8,315.00,534.81,52.38,9.96" target="#b46">(Xu et al., 2019)</ref> is often linked to image frequencies, suggesting that hypernyms could be recognized using low frequencies, which are learned early in training, while hyponyms might require high frequencies learned later.</s></p><p><s coords="8,315.00,582.63,234.00,9.96;8,315.00,594.59,234.00,9.96;8,315.00,606.54,234.00,9.96">We tested this hypothesis and conclude that metrics in hypernyms label spaces do not exhibit a lesser dependency on high frequencies in the first phase of training.</s></p><p><s coords="8,315.00,624.47,234.00,9.96;8,315.00,636.43,234.00,9.96;8,315.00,648.38,42.61,9.96">These and some other experimental results, the description of the probing methods are given in Appendix C.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4" coords="8,315.00,673.90,98.99,8.81">Generalization</head><p><s coords="8,315.00,694.66,234.00,9.96;8,315.00,706.62,234.00,9.96">There are no formal grounds to believe that the results should generalize beyond the conducted experiments.</s></p><p><s coords="8,315.00,718.57,234.00,9.96;8,315.00,730.53,183.38,9.96">To address this concern, we performed additional experiments to further validate our findings.</s></p><p><s coords="9,63.00,75.09,59.57,8.81">CIFAR-100.</s><s coords="9,131.17,74.49,165.84,9.96;9,63.00,86.45,234.00,9.96;9,63.00,98.40,234.00,9.96;9,63.00,110.36,31.15,9.96">CIFAR-100 <ref type="bibr" coords="9,185.13,74.49,111.88,9.96" target="#b21">(Krizhevsky et al., 2009)</ref> comprises 60 000 32 × 32 images distributed across 100 classes, which are further grouped into 20 superclasses.</s><s coords="9,101.40,110.36,32.77,10.32;9,138.93,110.36,158.07,9.96;9,63.00,122.31,100.83,10.32;9,168.65,122.31,128.34,9.96;9,63.00,134.27,76.59,10.32;9,140.09,134.27,2.77,9.96">Let S 20 represent the hypernym label space (superclasses) and R 20 denote a random label space isomorphic to S 20 .</s></p><p><s coords="9,63.00,152.20,234.00,9.96;9,63.00,164.15,137.03,9.96">We trained ResNet-18 and achieved 77% validation accuracy in hyponym label space.</s></p><p><s coords="9,63.00,182.09,234.00,9.96;9,63.00,194.04,234.00,9.96;9,63.00,206.00,195.02,10.32">We employed the greedy classifier model described in Section 3.1 and observed accelerated growth of A R (S 20 ) compared to A R (R 20 ), as expected.</s></p><p><s coords="9,63.00,224.53,47.57,8.81">DBPedia.</s><s coords="9,115.30,223.93,181.71,9.96;9,63.00,235.89,218.69,9.96">DBPedia <ref type="bibr" coords="9,157.59,223.93,100.04,9.96" target="#b22">(Lehmann et al., 2015)</ref> is a text classification dataset containing 630,000 samples.</s><s coords="9,289.53,235.89,7.47,9.96;9,63.00,247.84,234.00,9.96;9,63.00,259.80,35.96,9.96;9,98.96,264.00,3.97,6.12;9,103.43,259.80,193.57,9.96;9,63.00,271.75,28.78,9.96">It includes 14 classes, which we grouped into 5 hypernyms (S 5 ) based on the first level of the DBPedia ontology.</s><s coords="9,96.08,271.75,200.92,9.96;9,63.00,283.71,234.00,9.96;9,63.00,295.66,212.60,9.96">We fine-tuned a pretrained BERT-base model, and the test errors were as follows: 0.48% for random superclass labels, and 0.37% for hypernym labels.</s><s coords="9,279.85,295.66,17.15,9.96;9,63.00,307.62,234.00,9.96;9,63.00,319.57,234.00,10.32;9,63.00,331.53,129.67,9.96">The relative accuracy curves demonstrate that the difference between A(R 5 ) and A(S 5 ) emerges during the early iterations, as predicted.</s><s coords="9,199.39,331.53,97.61,9.96;9,63.00,343.48,21.85,9.96">Hypernym bias is evident.</s></p><p><s coords="9,63.00,361.42,234.00,9.96;9,63.00,373.37,234.00,9.96;9,63.00,385.33,109.56,9.96">Further details on the CIFAR-100 and DBPedia experiments, including relative accuracy curves, are provided in Appendix C.1.4.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" coords="9,63.00,418.92,200.82,10.57;9,83.17,432.87,86.32,10.57">DISCUSSION AND FUTURE DIRECTIONS</head><p><s coords="9,63.00,462.13,132.87,8.81">Hierarchical classification.</s><s coords="9,207.91,461.54,89.09,9.96;9,63.00,473.49,234.00,9.96;9,63.00,485.45,234.00,9.96">To the best of our knowledge, the WordNet graph has not been widely adopted in state-of-the-art classification algorithms.</s></p><p><s coords="9,63.00,497.40,234.00,9.96;9,63.00,509.36,234.00,9.96;9,63.00,521.31,130.21,9.96">Our experiments suggest that this may be because the hyponym-hypernym relationships are implicitly learned by the network itself.</s><s coords="9,199.76,521.31,97.24,9.96;9,63.00,533.27,234.00,9.96;9,63.00,545.22,234.00,9.96;9,63.00,557.18,234.00,9.96;9,63.00,569.13,234.00,9.96;9,63.00,581.09,234.00,9.96;9,63.00,593.04,123.01,9.96">Our results show that when the loss function incorporates multiple crossentropy losses from different hierarchy levels (as in <ref type="bibr" coords="9,63.00,557.18,125.40,9.96" target="#b33">(Redmon and Farhadi, 2017</ref>)), the contributions of higher-level hierarchy components are significant only during the early iterations of training, with a much lower impact in later stages.</s></p><p><s coords="9,63.00,610.98,176.99,9.96;9,315.00,355.44,234.00,9.96;9,315.00,367.39,234.00,9.96;9,315.00,379.35,40.97,9.96">For example, <ref type="bibr" coords="9,125.35,610.98,114.65,9.96" target="#b6">Brust and Denzler (2019)</ref>  Investigating the theoretical connections between hypernym bias and other known biases is a promising direction.</s><s coords="9,362.43,379.35,186.57,9.96;9,315.00,391.30,234.00,9.96;9,315.00,403.26,234.00,9.96;9,315.00,415.21,234.00,9.96;9,315.00,427.17,30.43,9.96">Establishing these relationships would not only deepen the theoretical understanding of hypernym bias but also bridge practical findings with theoretical models, which often lack strong empirical validation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" coords="9,315.00,462.01,119.18,10.57">CONCLUSIONS</head><p><s coords="9,315.00,491.43,234.00,9.96;9,315.00,503.38,234.00,9.96;9,315.00,515.34,234.00,9.96;9,315.00,527.29,234.00,9.96;9,315.00,539.25,110.73,9.96">The primary objective of our work was to establish that hypernym bias is a consistent phenomenon across different neural network architectures trained in practical settings, which other works on biases and hierarchical learning often lack.</s><s coords="9,430.09,539.25,118.91,9.96;9,315.00,551.20,234.00,9.96;9,315.00,563.16,234.00,9.96;9,315.00,575.11,234.00,9.96;9,315.00,587.07,107.77,9.96">We also aimed to introduce an experimental framework that provides a systematic and quantifiable approach to studying biases in practical scenarios, which can serve as a foundation for future research in this area.</s><s coords="9,426.87,587.07,122.13,9.96;9,315.00,599.02,234.00,9.96;9,315.00,610.98,234.00,9.96;9,315.00,622.93,234.00,9.96;9,315.00,634.89,45.82,9.96">To the best of our knowledge we are also the first to connect the training bias in the early stage with neural collapse in the terminal training stage by estimating neural collapse in different label spaces.</s><s coords="9,365.08,634.89,183.92,9.96;9,315.00,646.84,196.53,9.96">Therefore the notion of hypernym bias can may contribute to research of neural collapse.</s><s coords="9,515.84,646.84,33.15,9.96;9,315.00,658.80,234.00,9.96;9,315.00,670.75,234.00,9.96;9,315.00,682.71,234.00,9.96;9,315.00,694.66,234.00,9.96;9,315.00,706.62,74.14,9.96">Our experiments, which did not reveal parallels with existing phenomena, were designed to demonstrate that hypernym bias cannot be trivially attributed to other known factors, such as layer-wise training dynamics or data frequency biases.</s><s coords="9,395.62,706.62,153.39,9.96;9,315.00,718.57,234.00,9.96;9,315.00,730.53,199.32,9.96">The lack of clear parallels in these cases underscores that hypernym bias is a distinct phenomenon requiring independent investigation.</s><s coords="13,63.00,163.49,114.77,10.32;13,177.78,162.58,6.31,6.12;13,177.78,168.79,12.91,6.12;13,195.38,163.49,353.62,9.96">To see this, let D = {x i } N i=1 be the dataset with N examples and A(X , t) ∈ [0, 100] in every label space X .</s><s coords="13,63.00,175.44,486.00,10.32;13,63.00,187.40,116.28,9.96">Accuracy A(H, t) is essentially an estimation of probability P H (x; θ t ) that classifier f assigns correct hyponym label ŷH (x) to an image x:</s></p><formula xml:id="formula_21" coords="13,199.25,200.45,345.33,12.28">A(H, t)/100 ≈ P H (x; θ t ) = P ( fH (x; θ t ) = ŷH (x)), (<label>19</label></formula><formula xml:id="formula_22" coords="13,544.57,202.41,4.43,9.96">)</formula><p><s coords="13,63.00,221.45,164.81,12.28">where fH (x; θ t ) is the predicted label.</s></p><p><s coords="13,63.00,241.34,486.00,9.96;13,63.00,253.30,64.84,9.96">Assume that we use classifier f to greedily predict the label in the random superclass label space R, accordingly to Section 3.1.</s><s coords="13,135.73,251.33,236.35,12.28">Let fR (x; θ t ) be a superclass label that is predicted.</s><s coords="13,379.98,253.30,169.02,10.32;13,63.00,265.25,486.00,9.96;13,63.00,277.21,51.51,9.96">The probability P (ŷ R (x) = r) that a randomly chosen image x belongs to a superclass r can be estimated according to the size of the superclass in the dataset:</s></p><formula xml:id="formula_23" coords="13,252.73,288.32,291.84,23.22">P (ŷ R = r) ≈ P r ≜ |D r | N D , (<label>20</label></formula><formula xml:id="formula_24" coords="13,544.57,294.40,4.43,9.96">)</formula><p><s coords="13,63.00,319.15,25.48,9.96">where</s></p><formula xml:id="formula_25" coords="13,91.80,319.15,260.09,10.32">D r = {x ∈ D | ŷR (x) = r} is a set of examples with label r.</formula><p><s coords="13,63.00,337.09,306.24,10.32;13,373.36,337.09,175.64,9.96;13,63.00,349.04,123.16,9.96">Since hyponyms are assigned to r independently, prior probability P R that greedy hypernym classifier predicts correct label of superclass r:</s></p><formula xml:id="formula_26" coords="13,226.68,361.02,317.89,12.28">P R ( fR (x) = ŷR (x)|ŷ R (x) = r) = P r . (<label>21</label></formula><formula xml:id="formula_27" coords="13,544.57,362.98,4.43,9.96">)</formula><p><s coords="13,63.00,388.87,486.00,9.96;13,63.00,400.83,486.00,10.32;13,63.00,412.78,370.77,9.96">The probability of assigning a correct label in the superclass label space given a hyponym recognition probability P H (x; θ t ), can be estimated by considering two possible events: (a) correctly classifying the hyponym, or (b) misclassifying the hyponym but still assigning the correct superclass label by chance.</s></p><formula xml:id="formula_28" coords="13,203.92,443.25,345.08,22.13">P R (x; θ t ) = P H (x; θ t ) + (1 -P H (x; θ t )) r∈R P 2 r ,<label>(22)</label></formula><p><s coords="13,63.00,480.49,486.00,10.32;13,63.00,492.45,279.55,9.96">P R (x; θ t ) gives good estimate of A(R, t) in real experiments (Figure <ref type="figure" coords="13,363.24,480.49,4.43,9.96">7</ref>) except at the beginning of training, due to the nonuniform behavior of the randomly initialized network.</s></p><p><s coords="13,63.00,709.34,486.00,9.96;13,63.00,721.29,217.32,9.96">Figure <ref type="figure" coords="13,94.29,709.34,3.87,9.96">7</ref>: Experimentally observed accuracy in random superclasses label space and theoretically estimation of the same, given accuracy in hyponym label space.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="14,63.00,75.09,108.95,8.81">A.2 Neural Collapse</head><p><s coords="14,63.00,96.04,354.39,9.96">Here we briefly overview Neural Collapse (NC) proposed by <ref type="bibr" coords="14,326.26,96.04,86.78,9.96" target="#b26">Papyan et al. (2020)</ref>.</s><s coords="14,421.79,96.04,127.22,9.96;14,63.00,108.00,391.30,9.96">Originally NC is defined only in hyponym label space (used for training), so we extend it to the hypernym label spaces.</s></p><p><s coords="14,63.00,126.53,122.79,8.81">Hyponym Label Spaces.</s><s coords="14,193.62,125.93,57.74,10.32;14,251.36,125.02,4.11,6.12;14,260.41,125.93,288.58,9.96;14,63.00,137.88,119.73,9.96">Let h i,c ∈ R p be a p-dimensional feature vector, where c is one of |X | classes, where X is the label space.</s><s coords="14,188.20,137.88,360.80,9.96;14,63.00,149.84,121.81,9.96">In the original ImageNet hyponym label space (X = H) each class corresponds to a hyponym, so |H| = 1000.</s><s coords="14,191.55,149.84,88.34,9.96;14,279.89,148.93,21.89,6.12;14,306.37,149.84,79.11,9.96;14,385.48,148.93,11.56,6.12;14,401.62,149.84,147.38,9.96;14,63.00,161.79,130.05,9.96">The matrix W ∈ R |H|×p and vector b ∈ R |H| represent the weights and bias of the last fully-connected layer.</s><s coords="14,198.80,161.79,54.84,10.32;14,257.89,161.79,184.43,10.32;14,446.57,161.79,102.43,9.96;14,63.00,171.79,486.00,12.28;14,63.00,185.70,90.53,10.32">An image x i is mapped to its feature representation h i by all but one layers of the network f and the predicted label fH (x i ) is determined by the index of the largest element in the vector Wh i + b, as follows:</s></p><formula xml:id="formula_29" coords="14,238.32,196.35,306.26,17.21">fH (x i ) = arg max c ′ ⟨w c ′ , h i ⟩ + b c ′ , (<label>23</label></formula><formula xml:id="formula_30" coords="14,544.57,198.31,4.43,9.96">)</formula><p><s coords="14,63.00,220.84,25.48,9.96">where</s></p><formula xml:id="formula_31" coords="14,91.80,219.93,113.90,11.23">w c ′ is the c ′ -th row of W.</formula><p><s coords="14,63.00,238.77,175.72,10.32;14,238.72,238.77,218.24,10.32">The global mean is defined as µ G ≜ Ave i,c {h i,c }, and the train class means are defined as:</s></p><formula xml:id="formula_32" coords="14,233.52,262.14,311.06,10.32">µ c ≜ Ave i {h i,c }, c = 1, . . . , |H|, (<label>24</label></formula><formula xml:id="formula_33" coords="14,544.57,262.14,4.43,9.96">)</formula><p><s coords="14,63.00,284.49,159.25,9.96">where Ave is the averaging operator.</s></p><p><s coords="14,63.00,302.43,224.33,10.32;14,287.33,301.52,14.44,6.12;14,305.59,302.43,56.00,9.96">Additionally, the between-class covariance, Σ B ∈ R p×p is defined as:</s></p><formula xml:id="formula_34" coords="14,224.51,331.56,324.49,11.72">Σ B ≜ Ave c (µ c -µ G )(µ c -µ G ) ⊤ ,<label>(25)</label></formula><p><s coords="14,63.00,357.52,179.72,10.32;14,242.72,356.61,14.44,6.12;14,260.98,357.52,56.00,9.96">and the within-class covariance, Σ W ∈ R p×p is defined as:</s></p><formula xml:id="formula_35" coords="14,218.93,386.65,325.65,11.72">Σ W ≜ Ave i,c (h i,c -µ c )(h i,c -µ c ) ⊤ . (<label>26</label></formula><formula xml:id="formula_36" coords="14,544.57,388.06,4.43,9.96">)</formula><p><s coords="14,63.00,412.62,356.94,9.96">NC manifests through four key observations as described by <ref type="bibr" coords="14,328.49,412.62,87.10,9.96" target="#b26">Papyan et al. (2020)</ref>.</s></p><p><s coords="14,63.00,430.55,168.01,10.32">(NC1) Variability collapse: Σ W → 0.</s></p><p><s coords="14,63.00,451.47,340.33,10.32;14,403.33,449.39,3.65,6.12;14,403.33,457.10,6.02,6.12;14,410.19,451.47,138.81,9.96;14,63.00,467.40,72.24,9.96;14,135.24,466.49,3.65,6.12;14,142.71,467.40,159.26,9.96">Following <ref type="bibr" coords="14,107.53,451.47,87.86,9.96" target="#b26">Papyan et al. (2020)</ref>, in the Figure <ref type="figure" coords="14,262.69,451.47,4.98,9.96" target="#fig_4">5</ref> of the paper, we plot tr Σ W Σ † B /C , where tr denotes the trace operation and [•] † is the Moore-Penrose pseudoinverse.</s></p><p><s coords="14,63.00,485.33,164.75,9.96">(NC2) Convergence to simplex ETF:</s></p><formula xml:id="formula_37" coords="14,217.59,506.28,331.42,61.16">|∥µ c -µ G ∥ 2 -∥µ c ′ -µ G ∥ 2 | → 0 ∀ c, c ′ (27) ⟨ μc , μc ′ ⟩ → C C -1 δ c,c ′ - 1 C -1 ∀ c, c ′ , (<label>28</label></formula><formula xml:id="formula_38" coords="14,544.57,551.20,4.43,9.96">)</formula><p><s coords="14,63.00,576.55,50.43,9.96;14,120.30,574.43,33.12,6.73;14,117.39,582.69,38.45,6.73;14,160.85,576.55,146.33,9.96">where μc = (µc-µ G ) ∥µc-µ G ∥2 are the renormalized class means.</s><s coords="14,63.00,595.56,322.05,9.96">Following the original paper, we plot the following metrics in Section C.2:</s></p><p><s coords="14,70.19,621.90,478.80,9.96;14,82.92,633.85,438.77,9.96">1. Norms equality level measured by two ratios: a) the standard deviation to the average length of class means and b) the standard deviation of the length of the rows of the weight matrix to their average length:</s></p><formula xml:id="formula_39" coords="14,217.59,656.20,331.42,11.26">β µ = Std c (∥µ c -µ G ∥ 2 ) /Avg c (∥µ c -µ G ∥ 2 ) ,<label>(29)</label></formula><formula xml:id="formula_40" coords="14,241.74,679.20,302.83,11.26">β w = Std c (∥w c ∥ 2 ) /Avg c (∥w c ∥ 2 ) , (<label>30</label></formula><formula xml:id="formula_41" coords="14,544.57,679.20,4.43,9.96">)</formula><p><s coords="14,82.92,697.78,40.64,10.32;14,127.38,697.78,212.92,9.96">where w c is the c-th row of W (classifier of the c-th class).</s></p><p><s coords="14,70.19,718.57,469.59,9.96;14,539.78,722.78,4.84,6.12;14,545.13,718.57,3.87,9.96;14,82.92,730.53,305.90,9.96;14,388.82,734.73,5.76,6.12;14,395.26,730.53,6.64,9.96">2. Angles equality level measured as the standard deviation of cosines of the angles between class means (α µ ) and the standard deviation of cosines of angles between rows of W (α w ).</s></p><p><s coords="15,63.00,74.49,159.22,9.96">(NC3) Convergence to self-duality:</s></p><formula xml:id="formula_42" coords="15,255.76,102.78,293.24,30.40">W ⊤ ∥W∥ F - Ṁ ∥ Ṁ∥ F F → 0,<label>(31)</label></formula><p><s coords="15,63.00,139.18,134.89,12.20">where Ṁ = [µ c -µ G , c = 1, . . .</s><s coords="15,199.55,141.73,34.40,9.30;15,233.95,140.15,16.05,6.12;15,254.11,141.06,294.89,9.96;15,63.00,153.01,51.48,9.96">, C] ∈ R p×C is the matrix obtained by stacking the class means into the columns of a matrix.</s><s coords="15,118.92,153.01,179.71,10.32">Here, δ c,c ′ is the Kronecker delta symbol.</s></p><p><s coords="15,63.00,170.95,250.39,9.96">We report value, estimated by equation (31), in Figure <ref type="figure" coords="15,305.65,170.95,3.87,9.96" target="#fig_4">5</ref>.</s></p><p><s coords="15,63.00,188.88,246.51,9.96">(NC4) Simplification to Nearest Class Centroid (NCC):</s></p><formula xml:id="formula_43" coords="15,211.02,211.16,333.55,15.24">arg max c ′ ⟨w c ′ , h⟩ + b c ′ → arg min c ′ ∥h -µ c ′ ∥ 2 . (<label>32</label></formula><formula xml:id="formula_44" coords="15,544.57,211.16,4.43,9.96">)</formula><p><s coords="15,63.00,236.75,386.96,9.96">We report the proportion of mismatch between two ways of label estimation in Figure <ref type="figure" coords="15,442.21,236.75,3.87,9.96" target="#fig_4">5</ref>.</s></p><p><s coords="15,63.00,255.28,126.34,8.81">Hypernym Label Spaces.</s><s coords="15,196.26,254.69,352.74,9.96;15,63.00,266.64,73.34,9.96">To estimate NC in the hypernym label space S, we compute the mean of each superclass s ∈ S:</s></p><formula xml:id="formula_45" coords="15,267.30,297.07,281.70,10.32">µ s ≜ Ave c∈s {µ c }.<label>(33)</label></formula><p><s coords="15,63.00,321.52,263.89,10.32;15,326.89,320.61,24.85,6.12;15,356.25,321.52,181.54,10.32;15,542.31,321.52,6.69,9.96;15,63.00,333.47,165.67,9.96">And we assume that the superclass weight matrix W s ∈ R |Sn|×p consists of |S n | rows, where each row w s is obtained by averaging the rows of W:</s></p><formula xml:id="formula_46" coords="15,266.08,363.90,282.92,10.32">w s ≜ Ave c∈s {w c },<label>(34)</label></formula><p><s coords="15,63.00,388.35,180.54,10.32;15,243.54,389.02,34.25,9.65">and similarly for the bias vector b s ≜ Ave c∈s {b c }.</s></p><p><s coords="15,63.00,406.28,121.34,10.32;15,184.84,406.28,364.15,10.32;15,63.00,418.24,155.55,9.96">We can then use {µ s }, W s , and b s = [b s ] instead of {µ s }, W, and b as defined above, to estimate Neural Collapse in the hypernym spaces S.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="15,63.00,446.35,196.14,10.57">B EXPERIMENTAL DETAILS</head><p><s coords="15,63.00,471.73,486.00,9.96">In this section, we provide additional details on the experimental setup for the experiments discussed in Section 4.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="15,63.00,498.36,225.27,8.81">B.1 Architectures and Training Parameters</head><p><s coords="15,63.00,519.28,486.00,9.96;15,63.00,531.24,486.00,9.96;15,63.00,543.19,58.48,9.96">The primary conclusions are based on experiments with ResNet; however, we also include results for ViT and MobileNet V3-L 1.0 to demonstrate that the findings generalize beyond convolutional or parameter inefficient architectures.</s><s coords="15,125.92,543.19,339.73,9.96">Table <ref type="table" coords="15,153.31,543.19,4.98,9.96" target="#tab_4">2</ref> summarizes key parameters of the neural networks used in this study.</s><s coords="15,63.00,671.67,240.81,9.96">All architectures were trained using 224 × 224 images.</s><s coords="15,309.11,671.67,239.89,9.96;15,63.00,683.62,113.07,9.96">For ResNet-18, ResNet-50 and ViT-B/16 the following hyperparameters we used:</s></p><p><s coords="15,72.96,709.88,60.09,9.96">• 200 epochs;</s></p><p><s coords="15,72.96,730.53,139.88,9.96">• Cosine learning rate schedule;</s></p><p><s coords="16,72.96,74.49,234.35,9.96">• SGD optimizer with an initial learning rate of 0.05;</s></p><p><s coords="16,72.96,95.92,166.00,9.96">• AutoAugment <ref type="bibr" coords="16,147.94,95.92,86.59,9.96" target="#b9">(Cubuk et al., 2018)</ref>;</s></p><p><s coords="16,72.96,117.35,215.45,9.96">• Loss function with Jensen-Shannon divergence.</s></p><p><s coords="16,63.00,144.00,486.00,9.96;16,63.00,155.96,223.05,9.96">MobileNet was trained for 600 epochs using RMSprop, with noise added to the learning rate, a batch size of 512, warmup, and Exponential Moving Average (EMA).</s></p><p><s coords="16,63.00,173.89,209.00,9.96">ViT training resulted in relatively low accuracy.</s><s coords="16,276.43,173.89,272.58,9.96;16,63.00,185.84,332.98,9.96">Nevertheless, the presence of hypernym bias under suboptimal training conditions supports the hypothesis that the phenomenon is general.</s></p><p><s coords="16,63.00,203.78,316.57,9.96">ResNet-152 was trained in neural collapse settings <ref type="bibr" coords="16,285.37,203.78,85.35,9.96" target="#b26">(Papyan et al., 2020</ref>):</s></p><p><s coords="16,72.96,230.43,87.99,9.96">• No augmentation;</s></p><p><s coords="16,72.96,251.86,169.88,9.96">• 600 images per class in the train set;</s></p><p><s coords="16,72.96,273.29,86.79,9.96">• Batch size of 256;</s></p><p><s coords="16,72.96,294.72,60.09,9.96">• 350 epochs;</s></p><p><s coords="16,72.96,316.14,204.20,9.96">• SGD optimizer with initial learning rate 0.1;</s></p><p><s coords="16,72.96,337.57,310.21,9.96">• Learning rate is annealed by a factor of 10 at 1/2 and 3/4 of epochs.</s></p><p><s coords="16,63.00,364.23,486.00,9.96;16,63.00,376.18,208.53,9.96">These hyperparameters achieve nearly 100% accuracy on training set, which is necessary for NC to manifest, although validation accuracy is low in this case.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="16,63.00,403.24,219.21,8.81">B.2 Constructing Hypernym Label Spaces</head><p><s coords="16,63.00,424.93,118.75,8.81">High-Level Hypernyms.</s><s coords="16,186.17,424.33,250.12,9.96;16,63.00,649.06,486.00,9.96;16,63.00,661.02,86.17,10.32;16,149.67,661.02,2.77,9.96">At the top level of the WordNet tree, there are 9 synsets: Since the synsets have an uneven distribution of classes, we used the following groupings to form a top-level hypernym space, S 3 :</s></p><p><s coords="16,70.19,687.67,156.32,9.96;16,70.19,709.10,130.01,9.96;16,70.19,730.53,127.49,9.96">1. {1-7} (miscellaneous, 80 classes), 2. {8} (artifacts, 522 classes), 3. {9} (animals, 398 classes).</s></p><p><s coords="17,63.00,75.09,147.01,8.81">Low-Level Hypernym spaces.</s><s coords="17,216.15,74.49,332.84,9.96">The low-level hypernym superclasses were created according WordNet tree.</s><s coords="17,63.00,86.45,486.00,9.96;17,63.00,98.40,486.00,9.96;17,63.00,110.36,22.13,9.96">Similar to the 3 top-level hypernym superclasses, we aimed to maintain approximately equal class saturation in ImageNet classes, and some superclasses were formed by grouping several synonym sets at the same hierarchical level.</s><s coords="17,89.56,110.36,165.64,9.96">For instance, to obtain 7 superclasses:</s></p><p><s coords="17,72.96,136.55,239.10,9.96">• The "miscellaneous" superclass remained unchanged.</s></p><p><s coords="17,72.96,157.05,476.04,9.96;17,82.92,169.00,42.94,9.96">• The "animals" superclass was divided into 3 new superclasses based on the lower levels of the WordNet hierarchy:</s></p><p><s coords="17,90.11,187.37,117.56,9.96;17,90.11,201.46,150.09,9.96;17,90.11,215.55,326.25,9.96">1. Chordates (207 classes), 2. Domestic animals (123 classes), 3. Others (including invertebrates (61 classes) and game birds (7 classes)).</s></p><p><s coords="17,63.00,242.18,368.89,9.96">Similarly, the "artifacts" superclass was divided into the following three superclasses:</s></p><p><s coords="17,70.19,268.37,140.80,9.96;17,70.19,288.87,106.71,9.96;17,70.19,309.37,378.85,9.96">1. Instrumentality (358 classes), 2. Covering (85 classes), 3. Others (including categories like "commodity" and "decoration", totaling 79 classes).</s></p><p><s coords="17,63.00,335.56,486.00,9.96;17,63.00,347.52,374.97,9.96">For a more granular categorization, we divided the "animals" superclass into 18 new superclasses, such as "domestic cat", "reptile", and "terrier", each containing an average of 22 ImageNet classes.</s></p><p><s coords="17,63.00,365.45,265.24,9.96">The grouping procedure was performed only once in all cases.</s><s coords="17,332.51,365.45,216.49,9.96;17,63.00,377.41,321.78,9.96">This was done to maintain consistency in the sizes of the hypernym sets, ensuring that the reported metrics are comparable.</s><s coords="17,389.57,377.41,159.42,9.96;17,63.00,389.36,426.55,9.96">Highly imbalanced superclasses tend to yield higher accuracy regardless of their number, as can be seen from equations in Section A.1.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="17,63.00,417.40,217.56,10.57">C ADDITIONAL EXPERIMENTS</head><p><s coords="17,63.00,443.32,192.19,8.81">C.1 Greedy Hypernym Classification</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="17,63.00,464.81,178.29,8.81">C.1.1 Confusion Matrix Evolution</head><p><s coords="17,63.00,484.41,486.00,9.96;17,63.00,496.37,323.53,9.96">A straightforward way to observe the manifestation of hypernym bias is by plotting a confusion matrix with class labels arranged according to the structure of the WordNet hierarchy.</s><s coords="17,391.03,496.37,157.98,9.96;17,63.00,508.32,486.00,9.96;17,63.00,520.28,91.56,9.96">In this case, the order of classes was determined by performing a depth-first traversal of the tree, ensuring that sibling classes appear consecutively in the confusion matrix.</s><s coords="17,158.75,520.28,390.25,9.96;17,63.00,532.23,50.80,9.96">We visualize the confusion matrices at the 1st, 5th, and 200th epochs of training ResNet-50 in Figure <ref type="figure" coords="17,106.05,532.23,3.87,9.96" target="#fig_7">8</ref>.</s></p><p><s coords="17,63.00,550.17,485.99,9.96;17,63.00,562.12,129.18,9.96">In the early stages, the block-diagonal structure is apparent, although the WordNet tree only partially mirrors the misclassification patterns.</s><s coords="17,196.60,562.12,313.61,9.96">This can be observed in the structural discontinuities within the blocks.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="17,63.00,587.40,111.79,8.81">C.1.2 Residual Error</head><p><s coords="17,63.00,607.00,486.00,9.96;17,63.00,618.95,205.62,10.32;17,269.15,618.95,52.07,9.96">To provide a clearer understanding of the training dynamics of ResNet-50, as shown in Figures <ref type="figure" coords="17,495.73,607.00,4.98,9.96" target="#fig_1">1</ref> and<ref type="figure" coords="17,525.36,607.00,3.87,9.96" target="#fig_3">3</ref>, we introduce the residual relative error metric, E R , defined as:</s></p><formula xml:id="formula_47" coords="17,237.82,646.45,311.18,22.31">E R (X , t) = 100 -A(X , t) 100 -A(X , T ) -1,<label>(35)</label></formula><p><s coords="17,63.00,676.73,397.31,9.96">where A(X , t) is the accuracy at epoch t, and A(X , T ) is the final accuracy after T epochs.</s></p><p><s coords="17,63.00,694.66,379.15,9.96">Figure <ref type="figure" coords="17,95.57,694.66,4.98,9.96" target="#fig_8">9</ref> illustrates the residual relative error over the course of ResNet-50 training.</s><s coords="17,449.99,694.66,99.01,9.96;17,63.00,706.62,405.45,9.96">The graph shows that the error is significantly higher for hypernym recognition during the first 10 to 20 epochs.</s><s coords="17,476.00,706.62,73.00,9.96;17,63.00,718.57,486.00,9.96;17,63.00,730.53,384.13,9.96">After this initial phase, the residual relative error becomes similar across all three label spaces, indicating that the majority of the improvement in hypernym recognition accuracy occurs early in the training process.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="18,63.00,286.57,175.99,8.81">C.1.3 Different Hypernym Spaces</head><p><s coords="18,63.00,306.64,127.49,8.81">Animal Hypernym Space.</s><s coords="18,194.72,306.05,354.28,9.96;18,63.00,318.00,73.19,9.96">We examined the recognition of images within the "animals" hypernym label space using ResNet-50.</s><s coords="18,140.50,318.00,408.50,9.96;18,63.00,329.96,87.51,9.96">This analysis is motivated by the well-known hierarchical structure of species, which resembles a tree-like hierarchy.</s><s coords="18,154.75,329.96,394.25,9.96;18,63.00,341.91,268.92,9.96">Although the WordNet tree has known limitations, this structure is likely to provide a more accurate description for this portion of the ImageNet dataset.</s><s coords="18,336.36,341.91,160.16,9.96;18,63.00,493.49,486.00,9.96;18,63.00,505.44,360.45,9.96">The results are presented in Table <ref type="table" coords="18,488.78,341.91,3.87,9.96" target="#tab_5">3</ref>. Comparing the results in Table <ref type="table" coords="18,200.39,493.49,4.98,9.96" target="#tab_5">3</ref> with those for the entire set of classes (Figure <ref type="figure" coords="18,407.81,493.49,3.87,9.96">4</ref>), we observe that for a random division into 7 superclasses, 95% accuracy is achieved in 109 epochs for both cases.</s><s coords="18,427.83,505.44,121.17,9.96;18,63.00,517.40,404.33,9.96">However, when utilizing the animal hierarchy, training converges faster, reaching the target in 20 epochs compared to 25.</s></p><p><s coords="18,63.00,535.93,148.01,8.81">Unbalanced hypernym space.</s><s coords="18,217.83,535.33,331.16,9.96;18,63.00,547.29,486.00,9.96;18,63.00,559.24,395.87,9.96">To demonstrate that intuitive manual grouping of synsets inside the same hypernym level of the tree does not affect the conclusions, we also evaluated superclasses derived directly from one of the levels of the WordNet hierarchy without additional grouping for class balance.</s><s coords="18,465.13,559.24,83.88,9.96;18,63.00,571.20,337.29,9.96;19,63.00,396.85,163.82,10.32;19,227.32,396.85,2.77,9.96">This resulted in 72 unbalanced superclasses, with the corresponding results presented in Table <ref type="table" coords="18,392.54,571.20,3.87,9.96" target="#tab_6">4</ref>.  random label space isomorphic to S 20 .</s></p><p><s coords="19,63.00,414.78,485.99,9.96;19,63.00,426.73,209.80,9.96">We trained cifar-adapted ResNet-18 with augmentation and cosine annealing of learning rate and achieved 77% validation accuracy in the hyponym label space.</s></p><p><s coords="19,63.00,444.67,485.99,9.96;19,63.00,456.62,91.17,9.96">We employed the greedy classifier model described in Section 3.1 to estimate relative accuracy in hypernym and random label spaces.</s><s coords="19,159.43,456.62,389.57,10.32;19,63.00,468.58,125.00,9.96">The results demonstrate an accelerated growth of A R (S 20 ) compared to A R (R 20 ), which aligns with our expectations.</s><s coords="19,192.35,468.58,356.66,9.96;19,63.00,480.53,65.84,9.96">The manifestation of hypernym bias is evident but less pronounced than in Figure <ref type="figure" coords="19,63.00,480.53,9.96,9.96" target="#fig_1">1a</ref> (ImageNet).</s><s coords="19,133.28,480.53,161.21,9.96">This discrepancy is fully explainable:</s></p><p><s coords="19,70.19,506.18,7.75,9.96">1.</s><s coords="19,82.92,506.18,466.08,9.96;19,82.92,518.14,207.42,9.96">The hierarchical structure of CIFAR-100 is less well-defined compared to WordNet, with loosely distinct superclasses such as vehicles_1 and vehicles_2.</s></p><p><s coords="19,70.19,537.51,383.86,9.96">2. As noted in Section 4.1, larger hypernyms tend to exhibit a more pronounced bias.</s><s coords="19,460.56,537.51,88.44,9.96;19,82.92,549.46,150.07,9.96">In CIFAR-100, each superclass comprises only 5 labels.</s></p><p><s coords="19,63.00,575.11,486.00,9.96;19,63.00,587.07,207.24,9.96">In this setup, the difference in dynamics between hypernyms and random superclasses is most evident on the training set, where the estimates are less noisy.</s><s coords="19,275.53,587.07,273.47,10.32;19,63.00,599.02,44.15,9.96">This is particularly noticeable when A R (X ) ≈ 0.6 as shown in Figure <ref type="figure" coords="19,94.43,599.02,8.48,9.96" target="#fig_9">10</ref>.</s></p><p><s coords="19,63.00,616.95,486.00,9.96;19,63.00,628.91,17.71,9.96">We also observe a consistent manifestation of hypernym bias across a variety of training parameters on CIFAR-100.</s></p><p><s coords="19,63.00,647.44,47.57,8.81">DBPedia.</s><s coords="19,114.94,646.84,293.95,9.96">DBPedia is a text classification dataset containing 630,000 samples.</s><s coords="19,413.25,646.84,135.75,9.96;19,63.00,658.80,127.60,9.96;19,190.60,663.00,3.97,6.12;19,195.07,658.80,309.43,9.96">It includes 14 classes, which we grouped into 5 hypernyms (S 5 ) based on the first level of the DBPedia ontology as shown in Table <ref type="table" coords="19,496.76,658.80,3.87,9.96" target="#tab_7">5</ref>.</s></p><p><s coords="19,63.00,676.73,486.00,9.96;19,63.00,688.69,53.92,9.96;19,116.92,687.78,10.20,6.12;19,127.61,688.69,2.77,9.96">We fine-tuned a pretrained BERT-base model on DBPedia for 3 epochs using a batch size of 32 and a learning rate of 2 • 10 -5 .</s><s coords="19,134.80,688.69,414.19,9.96;19,63.00,700.64,140.35,9.96">The test errors were as follows: 0.67% for hyponym labels, 0.48% for random superclass labels, and 0.37% for hypernym labels.</s><s coords="19,209.42,700.64,339.58,10.32;19,63.00,712.60,262.03,10.32">The relative accuracy curves demonstrate that the difference between A(R 5 ) and A(S 5 ) emerges during the early iterations, as predicted.</s><s coords="19,329.45,712.60,113.85,9.96">Hypernym bias is evident.</s></p><p><s coords="19,63.00,730.53,411.19,9.96;20,63.00,437.43,435.01,9.96">Thus, we demonstrated the generalizability of our approach to two domains (images and text)  As can be seen from the figure we do not observe convergence to ETF in the hypernym label space.</s></p><p><s coords="20,63.00,455.96,105.37,8.81">UMAP visualization.</s><s coords="20,173.28,455.36,375.72,9.96;20,63.00,467.32,413.10,9.96">In Figure <ref type="figure" coords="20,217.48,455.36,8.48,9.96" target="#fig_12">12</ref>, we show UMAP embeddings of the penultimate features of two versions of trained ResNet-152: one trained with and one trained without horizontal flip augmentation.</s></p><p><s coords="20,63.00,485.25,485.99,9.96;20,63.00,497.20,136.60,9.96">As shown in Figure <ref type="figure" coords="20,148.41,485.25,8.48,9.96" target="#fig_12">12</ref>, data augmentation prevents the neural network from reaching 100% accuracy, and Neural Collapse (NC) is not observed.</s><s coords="20,206.11,497.20,200.83,9.96">Similarly, NC is absent on the validation set.</s><s coords="20,413.45,497.20,135.55,9.96;20,63.00,509.16,486.00,9.96;20,63.00,521.11,57.06,9.96">The graphs in Figure <ref type="figure" coords="20,511.92,497.20,4.98,9.96" target="#fig_4">5</ref> of and Figure <ref type="figure" coords="20,94.24,509.16,9.96,9.96" target="#fig_10">11</ref> similarly to those in <ref type="bibr" coords="20,197.15,509.16,89.24,9.96" target="#b26">(Papyan et al., 2020)</ref>, display only moderate convergence towards neural collapse in ImageNet.</s><s coords="20,126.35,521.11,422.65,9.96;20,63.00,533.07,234.07,9.96">However, NC is evident in UMAP embeddings shown in Figure <ref type="figure" coords="20,411.83,521.11,3.87,9.96" target="#fig_1">1</ref>, suggesting complete removal of hyponymy information from penultimate features.</s><s coords="20,304.24,533.07,244.76,9.96;20,63.00,545.02,486.00,9.96;20,63.00,556.98,68.01,9.96">Yet, when we plot the UMAP embeddings of the class centers, the hypernym structure remains preserved until the final epoch, although information gradually decays (see Figure <ref type="figure" coords="20,114.41,556.98,8.30,9.96" target="#fig_13">13</ref>).</s><s coords="20,63.00,574.91,272.39,9.96">This is likely due to averaging, which effectively reduces noise.</s><s coords="20,339.87,574.91,209.13,9.96;20,63.00,586.87,221.47,9.96">We anticipate that training a larger network for a longer period will result in full NC on ImageNet.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="20,63.00,614.63,124.94,8.81">C.3 Manifold Distances</head><p><s coords="20,63.00,636.00,486.00,9.96;20,63.00,647.95,204.67,9.96">Here we briefly compare the method of defining class distances based on mutual covers in feature space, as described in Section 3.2, with other approaches.</s><s coords="20,271.86,647.95,277.14,9.96;20,63.00,659.91,486.00,9.96">Interestingly, these simpler methods also exhibit high cophenetic correlation coefficient (CCC) between the distances in feature space and hyponym distance in the WordNet graph.</s></p><p><s coords="20,63.00,677.84,24.56,9.96;20,87.56,676.93,2.52,6.12;20,87.56,683.14,8.75,6.12;20,100.48,677.84,354.07,9.96">Let h l i,c be ResNet-50 feature of layer l in response to an image with hyponym label c<ref type="foot" coords="20,447.32,676.89,3.97,6.16" target="#foot_2">foot_2</ref> .</s><s coords="20,460.04,677.84,88.96,9.96;20,63.00,691.24,254.56,9.96;20,317.56,690.34,2.52,6.12;20,317.56,696.54,8.75,6.12;20,329.87,691.24,219.12,9.96">We use UMAP with default parameters to create 10-dimensional embeddings ϕ l i,c of the features, these embeddings are used further.</s><s coords="20,63.00,709.77,109.32,8.81">Class means distance.</s><s coords="20,177.83,709.18,206.72,10.32;20,389.80,709.18,159.19,9.96;21,63.00,601.49,204.02,9.96">We can construct the class distance matrix D F using distances between class means  Results of all methods are reported in Table <ref type="table" coords="21,259.27,601.49,3.87,9.96" target="#tab_8">6</ref>.</s></p><p><s coords="21,63.00,619.42,485.99,9.96;21,63.00,631.38,283.46,9.96">From the table, method based on mutual cover provides higher CCC in bottom layers, while class means distances preserve more information about hypernym relation in top layers.</s><s coords="21,350.76,631.38,198.24,9.96;21,63.00,643.33,219.34,9.96">Mutual cover distances, though, provide CCC that gradually increase from bottom to top layers.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="21,63.00,670.10,99.15,8.81">C.4 Linear Probes</head><p><s coords="21,63.00,691.07,285.75,9.96">We experimented with two methods of training the linear probes:</s></p><p><s coords="21,70.19,718.57,183.66,9.96">1. Additional fully-connected layers.</s><s coords="21,261.82,718.57,262.07,9.96">This technique was proposed by <ref type="bibr" coords="21,410.66,718.57,108.88,9.96" target="#b0">Alain and Bengio (2016)</ref>.</s><s coords="21,531.85,718.57,17.15,9.96;21,82.92,730.53,466.07,9.96;22,82.92,469.66,145.36,9.96">The additional layers are trained simultaneously with the entire network, but gradients from the probes do not propagate into the network itself.</s></p><p><s coords="22,70.19,489.65,106.18,9.96">2. Few-shot learning.</s><s coords="22,184.55,489.65,364.46,9.96;22,82.92,501.60,264.26,9.96">In this approach, features from several examples per class are collected during a specific phase of training and used to train a linear model.</s><s coords="22,354.38,501.60,194.62,9.96;22,82.92,513.56,323.09,9.96">This probing method has been employed in previous studies, such as (Alexey <ref type="bibr" coords="22,233.86,513.56,79.80,9.96" target="#b1">Dosovitskiy, 2020;</ref><ref type="bibr" coords="22,317.70,513.56,83.88,9.96" target="#b30">Raghu et al., 2021)</ref>.</s><s coords="22,412.56,513.56,136.44,9.96;22,82.92,525.51,36.63,9.96">We applied two types of linear learners:</s></p><p><s coords="22,92.88,543.49,456.12,9.96;22,102.84,555.44,30.43,9.96">• An analytical solution using the least squares method with regularization as proposed by <ref type="bibr" coords="22,495.38,543.49,53.62,9.96;22,102.84,555.44,26.09,9.96" target="#b30">Raghu et al. (2021)</ref>.</s><s coords="22,92.88,569.40,92.77,9.96">• Logistic regression.</s></p><p><s coords="22,63.00,596.38,486.00,9.96;22,63.00,608.34,486.00,9.96;22,63.00,620.29,37.37,9.96">The advantage of using additional layers trained simultaneously with the network is that the entire training set can be fully utilized, though the adaptation is influenced by the continuously changing weights of the neural network.</s><s coords="22,106.71,620.29,442.29,9.96;22,63.00,632.25,90.27,9.96">In contrast, few-shot learning requires more memory but provides a solution tailored to the specific state of the network.</s></p><p><s coords="22,63.00,650.18,287.98,10.32;22,353.84,650.18,195.16,9.96;22,63.00,662.13,59.64,10.32;22,125.93,662.13,152.32,10.32;22,278.75,662.13,2.77,9.96">In all cases, we trained the probes in the hyponym label space H 1000 and evaluated accuracy in both the hypernym label space S 3 and the hyponym label space H 1000 .</s><s coords="22,285.77,662.13,75.01,10.32;22,364.07,662.13,184.92,9.96;22,63.00,674.09,234.26,9.96">Training in H 1000 provides a regularization effect and is more effective in hypernym labels too, as shown in Table <ref type="table" coords="22,289.51,674.09,3.87,9.96" target="#tab_9">7</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="22,63.00,699.08,126.44,8.81">C.4.1 Additional Layers</head><p><s coords="22,63.00,718.57,486.00,9.96;22,63.00,730.53,486.00,9.96;23,63.00,510.13,273.01,9.96">The linear layers were trained for ResNet-18 using the same optimizer and parameters as the entire network, but with cross-entropy loss as the objective function for all intermediate layers, without any additional loss  We followed the recommendations of Alain and Bengio (2016):</s></p><p><s coords="23,72.96,535.66,280.65,9.96">• A linear layer is added at the end of each convolutional block.</s></p><p><s coords="23,72.96,555.40,252.76,9.96">• The weights of the linear layer are randomly initialized.</s></p><p><s coords="23,72.96,575.14,476.04,9.96;23,82.92,587.10,161.41,9.96">• To reduce the number of parameters, the feature maps are downsampled in both height and width using average pooling by a specified factor.</s></p><p><s coords="23,72.96,606.84,209.97,9.96">• Evaluation is conducted on the validation set.</s></p><p><s coords="23,63.00,632.37,476.20,9.96">We conducted experiments using feature maps at various resolutions using different average pulling windows.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="23,63.00,657.25,162.40,8.81">C.4.2 Few-Shot Linear Learner</head><p><s coords="23,63.00,676.73,485.99,9.96;23,63.00,688.69,245.93,9.96">For few-shot training, we use 10 examples per class sampled from a portion of the validation set, and evaluate the classifier on the remaining part of the validation set.</s></p><p><s coords="23,63.00,706.62,485.99,9.96">Initially, we followed the approach described in <ref type="bibr" coords="23,281.19,706.62,89.66,9.96" target="#b30">(Raghu et al., 2021)</ref>, which employs an analytical solution.</s></p><p><s coords="23,63.00,718.57,486.00,9.96;23,63.00,730.53,340.69,9.96">Additional investigations, described further, revealed that the analytical solution performed significantly worse in the early layers of the neural network compared to trainable linear layers.</s><s coords="23,410.01,730.53,138.99,9.96;24,63.00,200.81,486.00,9.96;24,63.00,212.77,61.81,9.96">We found that iterative logistic regression yielded slightly more robust results, which are reported in the paper, though it is still inferior to trained layers.</s></p><p><s coords="24,63.00,231.30,39.79,8.81">ResNet.</s><s coords="24,107.70,230.70,441.31,9.96;24,63.00,242.66,35.69,9.96">For ResNets we used features obtained similarly to trained probes with medium size average pooling window.</s></p><p><s coords="24,63.00,261.19,22.58,8.81">ViT.</s><s coords="24,89.34,260.59,459.66,9.96;24,63.00,272.55,309.04,9.96">For ViT, to connect probes we tested the averaged token embedding as suggested by <ref type="bibr" coords="24,468.53,260.59,80.47,9.96" target="#b7">Chen et al. (2020)</ref> and classification token embedding only as done in <ref type="bibr" coords="24,283.50,272.55,84.12,9.96" target="#b30">(Raghu et al., 2021)</ref>.</s><s coords="24,376.29,272.55,172.71,9.96;24,63.00,284.50,233.46,9.96">Classification token worked significantly better and we report results with the usage of it only.</s><s coords="24,300.85,284.50,248.15,9.96;24,63.00,296.46,86.73,10.32">The transformer-decoder processes a sequence of discrete input tokens, x 1 , . . .</s><s coords="24,151.40,297.12,15.04,9.65;24,166.94,296.46,266.86,9.96">, x n , and generates a d-dimensional embedding for each position.</s><s coords="24,438.43,296.46,110.57,9.96;24,63.00,308.41,342.61,9.96;24,405.61,307.50,3.97,6.12;24,410.07,309.08,16.05,8.74">The decoder consists of a stack of L blocks, with the l-th block producing an intermediate embedding, h 1 , . . .</s><s coords="24,427.79,309.08,10.16,8.74;24,437.95,307.50,2.52,6.12;24,441.01,308.41,94.88,9.96">, h l , each of dimension d.</s></p><p><s coords="24,63.00,326.34,184.33,9.96;24,247.33,325.44,2.52,6.12;24,253.71,326.34,44.38,9.96">The ViT block updates the input tensor h l as follows:</s></p><p><s coords="24,234.12,350.81,5.98,8.74;24,240.10,348.74,2.52,6.12;24,245.93,350.14,71.37,9.96;24,317.30,348.74,2.52,6.12;24,320.36,350.81,6.64,8.74;24,234.84,367.34,5.27,8.74;24,240.10,365.26,2.52,6.12;24,245.93,367.34,16.26,8.74;24,262.19,365.26,2.52,6.12;24,267.46,366.67,110.56,9.96;24,378.02,365.26,2.52,6.12;24,381.08,367.34,6.64,8.74;24,224.28,383.86,5.74,8.74;24,230.02,381.79,12.65,6.12;24,245.93,383.86,15.78,8.74;24,261.71,381.79,2.52,6.12;24,266.98,383.20,90.82,9.96;24,357.80,381.79,2.52,6.12;24,360.86,383.86,10.52,8.74">n l = layer_norm(h l ), a l = h l + multihead_attention(n l ), h l+1 = a l + mlp(layer_norm(a l )).</s></p><p><s coords="24,63.00,414.12,363.07,9.96;24,426.07,413.22,2.52,6.12;24,426.07,419.31,3.97,6.12;24,434.33,414.12,114.67,9.96;24,63.00,426.08,140.10,9.96">The probe is taken based on the first normalized classification token embedding n l 0 at each layer l, where 0 is the index of classification token.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="24,63.00,453.14,74.43,8.81">C.4.3 Results</head><p><s coords="24,63.00,474.05,62.75,8.81">Comparison.</s><s coords="24,131.42,473.45,417.58,9.96;24,63.00,485.41,33.78,9.96">For ResNet-18, training probes provide significantly better results than few-shot learning (Figure <ref type="figure" coords="24,80.18,485.41,8.30,9.96" target="#fig_1">14</ref>).</s><s coords="24,101.21,485.41,327.90,9.96">Logistic probes are slightly better than those based on analytical solutions.</s></p><p><s coords="24,63.00,704.68,295.86,9.96">Figure <ref type="figure" coords="24,94.68,704.68,8.48,9.96" target="#fig_1">14</ref>: Comparison of probes types for fully trained ResNet-18.</s><s coords="24,364.04,704.68,184.95,9.96;24,63.00,716.64,395.80,9.96">Small, balance, and large trainable probes were obtained using features with large, medium and small pooling windows, respectively.</s><s coords="24,464.18,716.64,84.82,9.96;24,63.00,728.59,166.78,9.96">The horizontal axis represents the normalized layer depth.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="26,63.00,426.58,203.09,10.57">D Complexity and infrastructure</head><p><s coords="26,63.00,451.72,272.05,9.96">For experiments we used NVIDIA A6000 and RTX 3090 gpus.</s></p><p><s coords="26,63.00,469.65,396.30,9.96">Our framework consists of three components, each with its own computational complexity:</s></p><p><s coords="26,70.19,495.56,135.00,9.96">1. Greedy classifier model.</s><s coords="26,212.83,495.56,305.57,9.96">Involves single argmax computation over logits during forward pass.</s><s coords="26,526.03,495.56,22.96,9.96;26,82.92,507.51,296.77,9.96">Mappings between labelspaces are efficiently managed using hash tables.</s><s coords="26,384.11,507.51,164.89,9.96;26,82.92,519.47,37.10,9.96">Computational burden is negligible in practice.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,63.00,696.95,86.77,7.92;1,149.78,695.26,6.92,5.22;1,158.73,696.95,140.38,7.92;1,63.00,706.92,236.12,7.92;1,63.00,716.88,236.11,7.92;1,63.00,726.84,30.27,7.92"><head/><label/><figDesc><div><p><s coords="1,63.00,696.95,86.77,7.92;1,149.78,695.26,6.92,5.22;1,158.73,696.95,140.38,7.92;1,63.00,706.92,236.12,7.92;1,63.00,716.88,38.90,7.92">Proceedings of the 28 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2025, Mai Khao, Thailand.</s><s coords="1,106.06,716.88,82.57,7.92">PMLR: Volume 258.</s><s coords="1,192.79,716.88,106.32,7.92;1,63.00,726.84,30.27,7.92">Copyright 2025 by the author(s).</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,63.00,282.65,486.00,9.96;2,63.00,294.61,486.00,9.96;2,63.00,306.56,486.00,9.96;2,63.00,318.52,472.16,9.96;2,93.40,72.00,425.18,196.20"><head>Figure 1 :</head><label>1</label><figDesc><div><p><s coords="2,63.00,282.65,117.79,9.96">Figure 1: Hypernym bias.</s><s coords="2,189.13,282.65,359.87,9.96;2,63.00,294.61,63.91,9.96">(a) Relative accuracy during training for ResNet-18, ResNet-50, ViT-B/16, and MobileNet-V3.</s><s coords="2,131.43,294.61,311.24,9.96">Hypernyms reach near maximum recognition accuracy on early epochs.</s><s coords="2,447.20,294.61,101.80,9.96;2,63.00,306.56,295.21,9.96">(b) UMAP embeddings of ResNet-152 penultimate layer features in neural collapse settings.</s><s coords="2,362.58,306.56,186.42,9.96;2,63.00,318.52,123.25,9.96">Color shows hyponymy distance relative to anchor class (&lt;tabby cat&gt;).</s><s coords="2,190.69,318.52,344.48,9.96">The dynamics can be interpreted as top-to-bottom hierarchical label clustering</s></p></div></figDesc><graphic coords="2,93.40,72.00,425.18,196.20" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,63.00,397.01,234.00,9.96;6,63.00,408.96,234.00,9.96;6,63.00,420.92,234.00,9.96;6,63.00,432.87,65.96,9.96;6,63.00,456.66,234.00,9.96;6,63.00,468.61,234.00,9.96;6,63.00,480.57,234.00,9.96;6,63.00,492.52,234.00,9.96;6,63.00,504.48,234.00,9.96;6,63.00,516.43,234.00,9.96;6,63.00,528.39,234.00,9.96;6,63.00,540.34,98.69,10.32;6,166.53,540.34,130.47,9.96;6,63.00,552.30,234.00,9.96;6,63.00,564.25,233.99,9.96;6,63.00,576.21,116.34,9.96;6,66.61,244.61,226.77,137.95"><head>Figure 2 :</head><label>2</label><figDesc><div><p><s coords="6,63.00,397.01,234.00,9.96;6,63.00,408.96,234.00,9.96;6,63.00,420.92,234.00,9.96;6,63.00,432.87,65.96,9.96;6,63.00,456.66,234.00,9.96;6,63.00,468.61,234.00,9.96;6,63.00,480.57,234.00,9.96;6,63.00,492.52,234.00,9.96;6,63.00,504.48,234.00,9.96">Figure 2: Mean of mutual-and self-covers of the features of penultimate ResNet-50 layer in the course of training: original feature space (a) and UMAP embeddings space (b)we address the issue by utilizing the formally grounded UMAP algorithm<ref type="bibr" coords="6,145.39,468.61,98.35,9.96" target="#b25">(McInnes et al., 2018)</ref>, which approximates a manifold where the data is uniformly distributed and effectively normalizes distances according to local connectivity before embedding optimization.</s><s coords="6,63.00,516.43,234.00,9.96;6,63.00,528.39,234.00,9.96;6,63.00,540.34,98.69,10.32;6,166.53,540.34,48.43,9.96">We transform the feature space into an M-dimensional embedding space, where the usage of Euclidean metric d(u, v) = ∥u -v∥ 2 is justified.</s><s coords="6,222.43,540.34,74.57,9.96;6,63.00,552.30,234.00,9.96;6,63.00,564.25,233.99,9.96;6,63.00,576.21,116.34,9.96">The dynamics of self-to-mutual cover ratio in UMAP embedding space depicted in Figure2b, exhibit expected behavior of an increasing cover difference.</s></p></div></figDesc><graphic coords="6,66.61,244.61,226.77,137.95" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,63.00,543.53,234.00,9.96;7,63.00,555.48,47.50,9.96;7,63.00,575.11,234.00,9.96;7,63.00,587.07,234.00,9.96;7,63.00,599.02,234.00,9.96;7,63.00,610.98,234.00,9.96;7,63.00,622.93,234.00,9.96;7,63.00,634.89,212.61,10.32;7,80.79,379.97,198.39,149.11"><head>Figure 3 :</head><label>3</label><figDesc><div><p><s coords="7,63.00,543.53,234.00,9.96;7,63.00,555.48,47.50,9.96">Figure 3: Relative accuracy gain during training of ResNet-50.</s><s coords="7,63.00,575.11,234.00,9.96;7,63.00,587.07,107.82,9.96">random superclasses and hyponyms showing its invariance to label imbalance.</s><s coords="7,177.94,587.07,119.06,9.96;7,63.00,599.02,234.00,9.96;7,63.00,610.98,234.00,9.96;7,63.00,622.93,234.00,9.96;7,63.00,634.89,212.61,10.32">It clearly demonstrates accelerated dynamics of learning hypernyms, but more importantly it justifies drawing conclusions about hypernym vs hyponym training dynamics by comparing A R (S n ) against A R (R n ) (instead of A R (H 1000 )).</s></p></div></figDesc><graphic coords="7,80.79,379.97,198.39,149.11" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,91.23,293.77,429.53,9.96;8,79.23,151.93,453.54,127.39"><head>Figure 5 :</head><label>5</label><figDesc><div><p><s coords="8,91.23,293.77,429.53,9.96">Figure 5: Estimation of NC1, NC3 and NC4 properties of neural collapse in different label spaces</s></p></div></figDesc><graphic coords="8,79.23,151.93,453.54,127.39" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,63.00,667.48,234.00,9.96;8,63.00,679.44,234.00,9.96;8,63.00,691.39,234.00,9.96;8,63.00,703.35,158.40,9.96;8,66.61,433.36,226.77,219.68"><head>Figure 6 :</head><label>6</label><figDesc><div><p><s coords="8,63.00,667.48,234.00,9.96;8,63.00,679.44,211.97,9.96">Figure 6: Cophenetic correlation coefficient for Word-Net graph and mutual covers distance matrix (a).</s><s coords="8,279.15,679.44,17.85,9.96;8,63.00,691.39,234.00,9.96;8,63.00,703.35,158.40,9.96">Linear probing of several checkpoints of ResNet-50 in hyponym (b) and hypernym (c) spaces</s></p></div></figDesc><graphic coords="8,66.61,433.36,226.77,219.68" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="18,63.00,243.44,485.99,9.96;18,63.00,255.39,62.05,9.96;18,65.43,72.00,481.08,156.99"><head>Figure 8 :</head><label>8</label><figDesc><div><p><s coords="18,63.00,243.44,250.72,9.96">Figure 8: Confusion matrix of ResNet-50 during training.</s><s coords="18,318.13,243.44,230.86,9.96;18,63.00,255.39,62.05,9.96">In early stages of training block diagonal structure is clearly visible.</s></p></div></figDesc><graphic coords="18,65.43,72.00,481.08,156.99" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="19,149.00,366.13,309.57,9.96;19,135.92,72.00,340.16,279.69"><head>Figure 9 :</head><label>9</label><figDesc><div><p><s coords="19,149.00,366.13,309.57,9.96">Figure 9: Residual relative error over the course of ResNet-50 training.</s></p></div></figDesc><graphic coords="19,135.92,72.00,340.16,279.69" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="20,63.00,210.68,486.00,9.96;20,63.00,222.64,473.04,9.96;20,75.15,72.00,461.69,124.24"><head>Figure 10 :</head><label>10</label><figDesc><div><p><s coords="20,63.00,210.68,197.00,9.96">Figure 10: Generalization of hypernym bias.</s><s coords="20,264.39,210.68,284.61,9.96;20,63.00,222.64,212.12,9.96">Relative accuracy in different labelspaces for CIFAR-10 (a,b) and DBPedia (c) classifiers during course of training.</s><s coords="20,279.55,222.64,256.49,9.96">Graph for DBPedia shows only first 750 training iterations</s></p></div></figDesc><graphic coords="20,75.15,72.00,461.69,124.24" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="21,63.00,484.95,486.00,9.96;21,63.00,496.90,65.06,10.32;21,131.88,496.90,150.36,10.32;21,286.25,496.90,191.90,10.32;21,481.96,496.90,67.04,9.96;21,63.00,508.86,79.27,10.32;21,146.27,508.86,14.94,9.96;21,111.60,72.00,388.80,398.50"><head>Figure 11 :</head><label>11</label><figDesc><div><p><s coords="21,63.00,484.95,486.00,9.96;21,63.00,496.90,65.06,10.32;21,131.88,496.90,150.36,10.32;21,286.25,496.90,191.90,10.32;21,481.96,496.90,67.04,9.96;21,63.00,508.86,79.27,10.32;21,146.27,508.86,14.94,9.96">Figure 11: Train and validation accuracy during training in NC settings (a), level of angle equality between class means α µ (b) and rows of weights matrix α w (c), level of norm equality of class means β µ (d) and rows of weights matrix β w (e).</s></p></div></figDesc><graphic coords="21,111.60,72.00,388.80,398.50" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="21,63.00,542.38,27.28,9.65;21,90.28,540.81,2.30,6.12;21,93.07,542.35,52.99,9.68;21,146.06,542.38,16.44,10.50;21,163.01,541.72,98.75,10.32;21,265.57,541.72,264.16,9.96;21,63.00,559.65,486.00,9.96;21,63.00,571.60,281.30,10.32;21,344.30,570.70,6.22,6.12;21,351.52,572.27,9.26,8.74;21,361.13,570.70,6.49,6.12;21,368.91,571.60,180.09,9.96;21,63.00,583.56,340.11,10.32"><head/><label/><figDesc><div><p><s coords="21,63.00,542.38,27.28,9.65;21,90.28,540.81,2.30,6.12;21,93.07,542.35,52.99,9.68;21,146.06,542.38,16.44,10.50;21,163.01,541.72,98.75,10.32;21,265.57,541.72,264.16,9.96">d f (c, c ′ ) = ∥µ c -µ c ′ ∥ 2 , where class means µ c is average embeddings of class c (similarly to equation (24)).</s><s coords="21,63.00,560.25,240.67,8.81">Correlation from direct comparison of examples.</s><s coords="21,307.95,559.65,241.05,9.96;21,63.00,571.60,281.30,10.32;21,344.30,570.70,6.22,6.12;21,351.52,572.27,9.26,8.74;21,361.13,570.70,6.49,6.12;21,368.91,571.60,180.09,9.96;21,63.00,583.56,340.11,10.32">We can omit materializing class distances by computing correlation coefficient directly between individual distances d f (u ci , v cj ) between examples of different classes in the feature (embedding) space and distances d w (c i , c j ) in the WordNet graph.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="22,63.00,425.86,486.00,9.96;22,63.00,437.81,358.62,9.96;22,135.90,72.00,340.20,339.41"><head>Figure 12 :</head><label>12</label><figDesc><div><p><s coords="22,63.00,425.86,486.00,9.96;22,63.00,437.81,84.41,9.96">Figure 12: UMAP embeddings of penultimate features for 50 training and 50 validation examples per class in the end of training.</s><s coords="22,151.84,437.81,269.78,9.96">The color indicates the WordNet distance to the anchor class.</s></p></div></figDesc><graphic coords="22,135.90,72.00,340.20,339.41" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="23,63.00,312.25,486.00,9.96;23,63.00,324.20,224.93,9.96;23,87.30,72.00,437.38,225.80"><head>Figure 13 :</head><label>13</label><figDesc><div><p><s coords="23,63.00,312.25,438.15,9.96">Figure 13: UMAP embeddings of the class means of the penultimate features on different epochs.</s><s coords="23,506.99,312.25,42.00,9.96;23,63.00,324.20,224.93,9.96">The color indicates the WordNet distance to the anchor class.</s></p></div></figDesc><graphic coords="23,87.30,72.00,437.38,225.80" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="26,63.00,359.67,486.00,9.96;26,63.00,371.62,486.00,9.96;26,63.00,383.58,486.00,9.96;26,63.00,395.53,133.02,9.96;26,107.58,72.00,396.83,273.22"><head>Figure 16 :</head><label>16</label><figDesc><div><p><s coords="26,63.00,359.67,486.00,9.96;26,63.00,371.62,81.01,9.96">Figure 16: Examples of images from classes with different hyponymy distances and varying amounts of highfrequency content.</s><s coords="26,151.52,371.62,397.48,9.96;26,63.00,383.58,486.00,9.96;26,63.00,395.53,133.02,9.96">Each column applies the same low-pass filter parameters, producing a consistent level of detail across all classes, while rows represent different categories with increasing hyponymy distance from the class presented in the top row.</s></p></div></figDesc><graphic coords="26,107.58,72.00,396.83,273.22" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="26,70.19,539.39,291.34,9.96;26,362.24,538.49,3.97,6.12;26,366.71,540.06,21.67,8.74;26,388.39,538.49,3.97,6.12;26,392.86,540.06,8.46,8.74;26,402.03,538.49,3.97,6.12;26,406.50,539.39,39.31,9.96;26,92.88,557.33,319.68,9.96;26,92.88,571.27,270.67,9.96;26,92.88,585.22,106.37,9.96;26,92.88,599.17,80.13,9.96;26,92.88,613.12,193.38,9.96;26,82.92,631.05,466.08,9.96;26,82.92,643.00,383.63,9.96;26,70.19,662.93,478.81,9.96;26,82.92,674.88,301.24,9.96;26,92.88,692.82,57.05,9.96;26,149.93,691.91,3.97,6.12;26,157.72,692.82,124.28,9.96;26,92.88,706.77,177.48,9.96;26,92.88,720.71,110.09,9.96"><head>2.</head><label/><figDesc><div><p><s coords="26,82.92,539.39,113.27,9.96">Manifold assessment.</s><s coords="26,199.52,539.39,162.01,9.96;26,362.24,538.49,3.97,6.12;26,366.71,540.06,21.67,8.74;26,388.39,538.49,3.97,6.12;26,392.86,540.06,8.46,8.74;26,402.03,538.49,3.97,6.12;26,406.50,539.39,39.31,9.96;26,92.88,557.33,319.68,9.96;26,92.88,571.27,270.67,9.96;26,92.88,585.22,106.37,9.96;26,92.88,599.17,80.13,9.96;26,92.88,613.12,193.38,9.96;26,82.92,631.05,360.93,9.96">-Complexity: O(EL • (N log N • p + K 2 d + c 2 K 2 )), where • E and L are number of checkpoints and layers to consider respectively, • N = 2K is number of data points (query and support sets), • c is number of classes, • p is feature size, • d is number of UMAP output dimensions.GPU acceleration is utilized for computing the distance matrix and mutual covers.</s><s coords="26,448.27,631.05,100.73,9.96;26,82.92,643.00,383.63,9.96;26,70.19,662.93,7.75,9.96">In our implementation, the experiments described in the paper required approximately 40 GB of GPU memory.3.</s><s coords="26,82.92,663.53,149.13,8.81">Neuronal collapse estimation.</s><s coords="26,238.38,662.93,310.62,9.96;26,82.92,674.88,301.24,9.96;26,92.88,692.82,57.05,9.96;26,149.93,691.91,3.97,6.12;26,157.72,692.82,124.28,9.96;26,92.88,706.77,177.48,9.96;26,92.88,720.71,110.09,9.96">Main computation burden comes from penultimate feature extraction.In our implementation number of forward passes: (2 + M )SE, where• S = 6 × 10 5 (size of balanced ImageNet),• M = 1 (number of extra label spaces),• E = 350 (checkpoints).</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="27,63.00,366.01,485.99,9.96;27,63.00,377.96,320.83,10.32;27,387.43,377.96,161.57,9.96;27,63.00,389.92,206.06,9.96;27,135.92,410.67,340.15,277.88"><head>Figure 17 :</head><label>17</label><figDesc><div><p><s coords="27,63.00,366.01,485.99,9.96;27,63.00,377.96,302.66,9.96">Figure 17: Accuracy (a,b) and relative accuracy gain (c,d) of ResNet-50 in hyponym label space (a,c) and hypernym label space (b,d) with the respect to size of the blur kernel.</s><s coords="27,370.00,378.63,13.83,9.65;27,387.43,377.96,161.57,9.96;27,63.00,389.92,206.06,9.96">G R is computed with respect to accuracy on undistorted images for the same checkpoint.</s></p></div></figDesc><graphic coords="27,135.92,410.67,340.15,277.88" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17" coords="27,63.00,702.99,486.00,9.96;27,63.00,714.95,320.83,10.32;27,387.43,714.95,161.57,9.96;27,63.00,726.90,203.29,9.96"><head>Figure 18 :</head><label>18</label><figDesc><div><p><s coords="27,63.00,702.99,486.00,9.96;27,63.00,714.95,302.66,9.96">Figure 18: Accuracy (a,b) and relative accuracy gain (c,d) of ViT-B/16 in hyponym label space (a,c) and hypernym label space (b,d) with the respect to size of the blur kernel.</s><s coords="27,370.00,715.61,13.83,9.65;27,387.43,714.95,161.57,9.96;27,63.00,726.90,203.29,9.96">G R is computed with respect to accuracy on undistorted images for the same checkpoint</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="25,107.58,149.15,396.85,221.33"><head/><label/><figDesc><div><p/></div></figDesc><graphic coords="25,107.58,149.15,396.85,221.33" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,142.30,81.35,327.41,57.88"><head>Table 1 :</head><label>1</label><figDesc><div><p><s coords="8,214.45,81.35,227.09,9.96;8,142.30,92.65,13.83,9.96;8,240.47,92.65,229.23,9.96;8,142.30,105.01,86.22,10.32">Top-1 recognition accuracy for different label spaces Set ResNet-50 ResNet-18 ViT-B/16 MobileNet-V3 hyponym, A(H 1000 )</s></p></div></figDesc><table coords="8,142.30,105.01,327.41,34.23"><row><cell/><cell>79</cell><cell>69.8</cell><cell>75.6</cell><cell>75.6</cell></row><row><cell>hypernym, A(S 3 )</cell><cell>97.7</cell><cell>96.3</cell><cell>97.1</cell><cell>97.2</cell></row><row><cell>random, A(R 3 )</cell><cell>87.6</cell><cell>83</cell><cell>86.3</cell><cell>86.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,63.00,99.07,234.00,81.69"><head/><label/><figDesc><div><p><s coords="13,63.00,121.64,341.04,10.32">Let f H (x; θ t ) denote a classifier trained in the hyponym label space H at epoch t.</s><s coords="13,408.11,121.64,140.89,9.96;13,63.00,133.60,486.00,9.96;13,63.00,145.55,317.42,9.96">If A(H, t) represents the absolute classification accuracy in H, then the classification accuracy in a random superclass label space, A(R, t), can be theoretically estimated based on the sizes of the subsets within R = {r}.</s></p></div></figDesc><table coords="10,63.00,99.07,234.00,81.69"><row><cell>In ICML 2019 Workshop on Identifying and Under-A METHODOLOGY</cell><cell>5. If you used crowdsourcing or conducted research</cell></row><row><cell>The paper was prepared with the financial support of the Ministry of Science and Higher Education of the Russian Federation, grant agreement No. FSRF-2023-0003, "Fundamental principles of building of noise-immune systems for space and satellite com-munications, relative navigation, technical vision and aerospace monitoring" standing Deep Learning Phenomena. 1. For all models and algorithms presented, check if specification of all dependencies, including (c) (Optional) Anonymized source code, with [Yes, see Appendix D] (time, space, sample size) of any algorithm. (b) An analysis of the properties and complexity [Yes, see Section 3] ting, assumptions, algorithm, and/or model. (a) A clear description of the mathematical set-you include: A.1 Relation of A(R, t) and A(H, t)</cell><cell>with human subjects, check if you include: pant compensation. [Not Applicable] pants and the total amount spent on partici-(c) The estimated hourly wage paid to partici-cable] (IRB) approvals if applicable. [Not Appli-with links to Institutional Review Board (b) Descriptions of potential participant risks, pants and screenshots. [Not Applicable] (a) The full text of instructions given to partici-</cell></row><row><cell>external libraries.[No]</cell><cell/></row><row><cell>2. For any theoretical claim, check if you include:</cell><cell/></row><row><cell>(a) Statements of the full set of assumptions of</cell><cell/></row><row><cell>all theoretical results. [Yes]</cell><cell/></row><row><cell>(b) Complete proofs of all theoretical results.</cell><cell/></row><row><cell>[Yes, see Appendix A]</cell><cell/></row><row><cell>(c) Clear explanations of any assumptions. [Yes]</cell><cell/></row><row><cell>3. For all figures and tables that present empirical</cell><cell/></row><row><cell>results, check if you include:</cell><cell/></row><row><cell>(a) The code, data, and instructions needed to</cell><cell/></row><row><cell>reproduce the main experimental results (ei-</cell><cell/></row><row><cell>ther in the supplemental material or as a</cell><cell/></row><row><cell>URL). [No]</cell><cell/></row><row><cell>(b) All the training details (e.g., data splits, hy-</cell><cell/></row><row><cell>perparameters, how they were chosen). [Yes,</cell><cell/></row><row><cell>see Section 4, Appendix B]</cell><cell/></row><row><cell>(c) A clear definition of the specific measure or</cell><cell/></row><row><cell>statistics and error bars (e.g., with respect to</cell><cell/></row><row><cell>the random seed after running experiments</cell><cell/></row><row><cell>multiple times). [Yes, see Section 3, Ap-</cell><cell/></row><row><cell>pendix C.1.2]</cell><cell/></row><row><cell>(d) A description of the computing infrastructure</cell><cell/></row><row><cell>used. (e.g., type of GPUs, internal cluster, or</cell><cell/></row><row><cell>cloud provider). [Yes, see Appendix D]</cell><cell/></row><row><cell>4. If you are using existing assets (e.g., code, data,</cell><cell/></row><row><cell>models) or curating/releasing new assets, check if</cell><cell/></row><row><cell>you include:</cell><cell/></row><row><cell>(a) Citations of the creator If your work uses ex-</cell><cell/></row><row><cell>isting assets. [Yes]</cell><cell/></row><row><cell>(b) The license information of the assets, if ap-</cell><cell/></row><row><cell>plicable. [Not Applicable]</cell><cell/></row><row><cell>(c) New assets either in the supplemental mate-</cell><cell/></row><row><cell>rial or as a URL, if applicable. [Not Applica-</cell><cell/></row><row><cell>ble]</cell><cell/></row><row><cell>(d) Information about consent from data</cell><cell/></row><row><cell>providers/curators. [Not Applicable]</cell><cell/></row><row><cell>(e) Discussion of sensible content if applicable,</cell><cell/></row><row><cell>e.g., personally identifiable information or of-</cell><cell/></row><row><cell>fensive content. [Not Applicable]</cell><cell/></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="15,129.63,574.27,352.75,81.44"><head>Table 2 :</head><label>2</label><figDesc><div><p><s coords="15,250.55,574.27,150.46,9.96">Networks used in the experiments.</s></p></div></figDesc><table coords="15,129.63,586.17,352.75,69.54"><row><cell>Name</cell><cell cols="3">Basic Block #Params (M) Top-1 Accuracy (%)</cell></row><row><cell>ResNet-18</cell><cell>Convolution</cell><cell>12.0</cell><cell>69.8</cell></row><row><cell>ResNet-50</cell><cell>Convolution</cell><cell>25.0</cell><cell>79.0</cell></row><row><cell>ResNet-152</cell><cell>Convolution</cell><cell>60.2</cell><cell>65.0</cell></row><row><cell>ViT-B/16</cell><cell>Attention</cell><cell>86.0</cell><cell>75.6</cell></row><row><cell cols="2">MobileNet V3-L 1.0 Convolution</cell><cell>5.4</cell><cell>75.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="18,63.00,372.61,486.00,105.90"><head>Table 3 :</head><label>3</label><figDesc><div><p><s coords="18,104.68,372.61,444.32,9.96;18,63.00,384.57,175.33,9.96">Number of epochs required by ResNet-50 to achieve 95% relative accuracy in different labels spaces with varying sizes (animal images only).</s></p></div></figDesc><table coords="18,274.08,397.06,63.85,81.45"><row><cell cols="3">n S n R n</cell></row><row><cell>2</cell><cell>6</cell><cell>46</cell></row><row><cell>3</cell><cell>12</cell><cell>96</cell></row><row><cell>4</cell><cell cols="2">13 105</cell></row><row><cell>7</cell><cell cols="2">20 109</cell></row><row><cell>8</cell><cell cols="2">25 114</cell></row><row><cell>18</cell><cell cols="2">48 114</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="18,63.00,601.90,486.00,138.95"><head>Table 4 :</head><label>4</label><figDesc><div><p><s coords="18,158.45,601.90,334.67,9.96;18,244.13,613.80,123.24,9.72;18,63.00,688.69,486.00,9.96;18,63.00,700.64,97.00,9.96;18,63.00,719.17,59.57,8.81">Number of epochs to achieve 95% recognition accuracy using 72 superclasses Architecture R 72 S 72We have conducted additional experiments to provide further evidence of hypernym bias existence across different datasets and domains.CIFAR-100.</s><s coords="18,131.45,718.57,417.54,9.96;18,63.00,730.53,130.58,9.96">CIFAR-100 comprises 60 000 32x32 images distributed across 100 classes, which are further grouped into 20 superclasses.</s><s coords="18,201.87,730.53,33.11,10.32;18,240.09,730.53,265.45,10.32;18,510.64,730.53,38.37,9.96">Let S 20 represent the hypernym label space (superclasses) and R 20 denote a</s></p></div></figDesc><table coords="18,244.13,625.55,123.75,21.92"><row><cell>ResNet-50</cell><cell>114</cell><cell>31</cell></row><row><cell>ResNet-18</cell><cell>88</cell><cell>31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="20,63.00,255.31,486.00,174.15"><head>Table 5 :</head><label>5</label><figDesc><div><p><s coords="20,207.82,255.31,235.92,9.96;20,216.83,383.63,332.17,9.96;20,63.00,395.58,99.56,10.32;20,163.06,395.58,108.18,10.32;20,271.74,395.58,94.95,10.32;20,367.18,395.58,181.81,9.96;20,63.00,407.54,486.00,9.96;20,63.00,419.50,63.63,9.96">Superclasses structure according to DBPedia ontology Figure5shows kthe curves for NC1, NC3, and NC4 in the label spaces of top-level hypernyms S 3 , random superclasses R 3 , and hyponyms H 1000 .Here, in Figure11, we provide additional graphs that illustrate the level of convergence to the equiangular tight frame (ETF), as computed according to Appendix A.2.</s></p></div></figDesc><table coords="20,63.00,267.21,321.45,125.83"><row><cell>Superclass</cell><cell>Classes</cell></row><row><cell>Eukaryote</cell><cell>Artist, Athlete, OfficeHolder, Animal, Plant</cell></row><row><cell cols="2">Organization Company, EducationalInstitution</cell></row><row><cell>Place</cell><cell>NaturalPlace, Village</cell></row><row><cell>Work</cell><cell>Album, Film, WrittenWork</cell></row><row><cell>Others</cell><cell>MeanOfTransportation, Building</cell></row><row><cell>C.2 Neural collapse</cell><cell/></row><row><cell cols="2">ETF convergence estimation.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="23,63.00,352.80,486.00,149.37"><head>Table 6 :</head><label>6</label><figDesc><div><p><s coords="23,101.18,352.80,447.83,9.96;23,63.00,364.75,24.68,9.96">Cophenetic correlation coefficient of WordNet graph distances and distances between features of different labels</s></p></div></figDesc><table coords="23,63.00,382.18,413.68,119.98"><row><cell>Layer name</cell><cell cols="2">Direct comparison Class Means</cell><cell>Mutual Cover</cell></row><row><cell>Layer1 (max pool)</cell><cell>0.06</cell><cell>0.26</cell><cell>0.31</cell></row><row><cell>Layer2 (max pool)</cell><cell>0.20</cell><cell>0.44</cell><cell>0.44</cell></row><row><cell>Layer3 (max pool)</cell><cell>0.49</cell><cell>0.62</cell><cell>0.56</cell></row><row><cell>Pre-logits</cell><cell>0.58</cell><cell>0.61</cell><cell>0.59</cell></row><row><cell>Logits</cell><cell>0.59</cell><cell>0.62</cell><cell>0.60</cell></row><row><cell>Average</cell><cell>0.32</cell><cell>0.44</cell><cell>0.45</cell></row><row><cell>components.</cell><cell/><cell/><cell/></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="24,63.00,81.96,486.00,93.39"><head>Table 7 :</head><label>7</label><figDesc><div><p><s coords="24,106.45,81.96,406.43,10.32">Hypernym linear probes classification accuracy A(S 3 ) using different label spaces for training.</s><s coords="24,517.21,81.96,31.79,9.96;24,63.00,93.92,270.12,9.96;24,198.88,105.82,214.24,9.72">Results are for the ResNet-18 layers and the logistic regression probes Layer Hypernym (S 3 ) Hyponym (H 1000 )</s></p></div></figDesc><table coords="24,199.92,117.57,179.84,57.78"><row><cell>layer1</cell><cell>70,4</cell><cell>73,2</cell></row><row><cell>layer2</cell><cell>77,9</cell><cell>79,6</cell></row><row><cell>layer3</cell><cell>87,3</cell><cell>87,7</cell></row><row><cell>layer4</cell><cell>93,4</cell><cell>95,5</cell></row><row><cell>logits</cell><cell>92,6</cell><cell>95,8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,331.58,721.86,217.42,7.92;1,315.00,731.82,19.48,7.92"><p><s coords="1,331.58,721.86,217.42,7.92;1,315.00,731.82,19.48,7.92">Video of UMAP embeddings evolution is available here.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,79.58,711.90,217.41,7.92;2,63.00,721.86,234.00,7.92;2,63.00,731.82,160.39,7.92"><p><s coords="2,79.58,711.90,217.41,7.92;2,63.00,721.86,234.00,7.92;2,63.00,731.82,160.39,7.92">For example, on the 1st epoch features associated with close to anchor labels (red, orange and yellow dots) are close but not separated from each other</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="20,79.58,731.82,445.09,7.92"><p><s coords="20,79.58,731.82,445.09,7.92">For layers below pre-logit layer before estimating distances we applied global max-pooling to the feature maps</s></p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="25,63.00,74.49,486.00,9.96;25,63.00,86.45,110.80,9.96">However, we have observed that trainable layers doesn't change relative accuracy of probing and the same conclusions can be made.</s><s coords="25,179.07,86.45,369.94,9.96;25,63.00,98.40,186.28,9.96">Therefore, in the paper we present results for logistic regression probes only, but we verify conclusions for several architectures.</s></p><p><s coords="25,63.00,116.93,159.87,8.81">Probes for ViT and ResNet-18.</s><s coords="25,228.83,116.33,320.17,9.96">Figure <ref type="figure" coords="25,260.76,116.33,4.98,9.96">6</ref> present results for few-shot linear probes applied to ResNet-50.</s><s coords="25,63.00,128.29,471.89,9.96">Here, in Figure <ref type="figure" coords="25,132.36,128.29,8.48,9.96">15</ref>, we extend the analysis to include additional experiments with ViT-B/16 and ResNet-18.</s><s coords="25,63.00,431.13,305.31,9.96;25,70.19,455.64,478.81,9.96;25,82.92,467.59,104.45,9.96">The following conclusions can be drawn from the graphs in Figure <ref type="figure" coords="25,355.58,431.13,12.73,9.96">15:</ref> 1. Classification accuracy improves as the network progresses towards the top layer, for both hyponym and hypernym classification.</s><s coords="25,191.79,467.59,202.17,9.96">This trend is observed across all architectures.</s></p><p><s coords="25,70.19,486.46,478.80,9.96;25,82.92,498.41,98.10,9.96">2. During the early stages of ViT training, classification accuracy based on the initial layers is higher compared to the end of training.</s><s coords="25,186.59,498.41,362.41,9.96;25,82.92,510.37,445.25,9.96">This pattern is evident for both the hyponym and hypernym label spaces and can be attributed to the removal of excess information from the classification token as training progresses.</s></p><p><s coords="25,70.19,529.23,478.81,9.96;25,82.92,541.19,109.20,9.96">3. In all experiments, hypernym classification accuracy using intermediate layers is consistently lower than when using the top layer.</s><s coords="25,196.45,541.19,352.55,9.96;25,82.92,553.15,248.64,9.96">Therefore, we cannot conclude that the early layers of the neural network contain sufficient information for accurate hypernym recognition.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="25,63.00,578.26,160.12,8.81">C.5 Image Frequency Analysis</head><p><s coords="25,63.00,599.02,486.00,9.96;25,63.00,610.98,458.50,9.96">One can suggest that recognizing hypernyms is possible based on low image frequencies, which are learned in first phase of training, while hyponyms require high frequencies that are learned later (Figure <ref type="figure" coords="25,504.90,610.98,8.30,9.96">16</ref>).</s><s coords="25,529.58,610.98,19.42,9.96;25,63.00,622.93,486.00,9.96;25,63.00,634.89,178.64,9.96">This explanation through Frequency Principle seems intuitive and lead to interesting interpretations of the fact that infants perceive world in low frequencies.</s><s coords="25,246.08,634.89,221.58,9.96">However our experiments did't reveal this relation.</s></p><p><s coords="25,63.00,652.82,382.02,9.96">We removed high frequencies from the images using Gaussian kernel of different size.</s><s coords="25,452.34,652.82,96.66,9.96;25,63.00,664.78,485.99,9.96;25,63.00,676.73,37.68,9.96">The focus was on the accuracy of hypernym recognition at the beginning and at the end of training when using a significant level of blurring.</s><s coords="25,106.29,676.73,442.70,9.96;25,63.00,688.69,45.51,9.96">For this, we used the 10th and 195th epochs of ResNet-50 training and the 5th and 140th epochs of ViT-B/16.</s></p><p><s coords="25,63.00,706.62,486.00,9.96;25,63.00,718.57,486.00,9.96;25,63.00,730.53,66.18,9.96">As can be seen from the relative accuracy gain graph depicted in Figure <ref type="figure" coords="25,382.59,706.62,9.96,9.96">17</ref> and Figure <ref type="figure" coords="25,446.98,706.62,9.96,9.96">18</ref> there is no tendency to depend on low frequencies more in the beginning of training than in the end of training for both hypernyms and hyponyms.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,63.00,213.46,234.00,9.96;10,72.96,225.41,224.04,9.96;10,72.96,237.98,117.20,8.80" xml:id="b0">
	<monogr>
		<title level="m" type="main" coords="10,206.69,213.46,90.31,9.96;10,72.96,225.41,189.12,9.96">Understanding intermediate layers using linear classifier probes</title>
		<author>
			<persName coords=""><forename type="first">Alain</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01644</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Alain, G. and Bengio, Y. (2016). Understanding inter- mediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644.</note>
</biblStruct>

<biblStruct coords="10,63.00,253.64,234.00,9.96;10,72.96,265.60,224.04,9.96;10,72.96,277.55,224.04,9.96;10,72.96,290.12,79.02,8.80" xml:id="b1">
	<monogr>
		<title level="m" type="main" coords="10,72.96,265.60,224.04,9.96;10,72.96,277.55,140.32,9.96">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Alexey Dosovitskiy, Lucas Beyer, A. K. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.</note>
</biblStruct>

<biblStruct coords="10,63.00,305.78,234.00,9.96;10,72.96,317.74,224.03,9.96;10,72.96,329.69,224.04,9.96;10,72.96,341.65,224.04,9.96;10,72.96,353.60,224.04,9.96;10,72.96,365.56,17.71,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coords="10,262.01,329.69,35.00,9.96;10,72.96,341.65,167.17,9.96">A closer look at memorization in deep networks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,261.22,342.25,35.79,8.80;10,72.96,354.21,167.41,8.80">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="233" to="242"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Arpit, D., Jastrzębski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M. S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et al. (2017). A closer look at memorization in deep networks. In Interna- tional conference on machine learning, pages 233- 242.</note>
</biblStruct>

<biblStruct coords="10,93.99,365.56,32.23,9.96" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="raw_reference">PMLR.</note>
</biblStruct>

<biblStruct coords="10,63.00,381.83,234.00,9.96;10,72.96,393.79,224.04,9.96;10,72.96,406.35,117.20,8.80" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Benjdiraa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Alia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Koubaa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.15563</idno>
		<title level="m" coords="10,72.96,393.79,190.24,9.96">Guided frequency loss for image restoration</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Benjdiraa, B., Alia, A. M., and Koubaa, A. (2023). Guided frequency loss for image restoration. arXiv preprint arXiv:2309.15563.</note>
</biblStruct>

<biblStruct coords="10,63.00,422.01,233.99,9.96;10,72.96,433.97,224.04,9.96;10,72.96,445.92,224.04,9.96;10,72.96,457.88,102.06,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coords="10,173.53,433.97,123.47,9.96;10,72.96,445.92,72.64,9.96">An inductive bias for tabular deep learning</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Beyazit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kozaczuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fadlallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,155.62,446.53,141.38,8.80;10,72.96,458.49,81.34,8.80">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Beyazit, E., Kozaczuk, J., Li, B., Wallace, V., and Fadlallah, B. (2024). An inductive bias for tabu- lar deep learning. Advances in Neural Information Processing Systems, 36.</note>
</biblStruct>

<biblStruct coords="10,63.00,474.15,234.00,9.96;10,72.96,486.11,224.04,9.96;10,72.96,498.06,224.04,9.96;10,72.96,510.02,115.66,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coords="10,230.96,474.15,66.05,9.96;10,72.96,486.11,224.04,9.96;10,72.96,498.06,39.74,9.96">Integrating domain knowledge: using hierarchies to improve deep classifiers</title>
		<author>
			<persName coords=""><forename type="first">C.-A</forename><surname>Brust</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,135.69,498.67,161.31,8.80;10,72.96,510.63,15.86,8.80">Asian conference on pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3" to="16"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Brust, C.-A. and Denzler, J. (2019). Integrating do- main knowledge: using hierarchies to improve deep classifiers. In Asian conference on pattern recogni- tion, pages 3-16. Springer.</note>
</biblStruct>

<biblStruct coords="10,63.00,526.29,234.00,9.96;10,72.96,538.25,224.04,9.96;10,72.96,550.20,224.04,9.96;10,72.96,562.16,155.29,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coords="10,229.36,538.25,67.64,9.96;10,72.96,550.20,84.38,9.96">Generative pretraining from pixels</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,177.67,550.81,119.34,8.80;10,72.96,562.76,73.05,8.80">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1691" to="1703"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. (2020). Generative pre- training from pixels. In International conference on machine learning, pages 1691-1703.</note>
</biblStruct>

<biblStruct coords="10,231.58,562.16,32.23,9.96" xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="raw_reference">PMLR.</note>
</biblStruct>

<biblStruct coords="10,63.00,578.43,234.00,9.96;10,72.96,590.39,224.03,9.96;10,72.96,602.34,224.04,9.96;10,72.96,614.90,79.02,8.80" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m" coords="10,187.71,590.39,109.28,9.96;10,72.96,602.34,145.09,9.96">Autoaugment: Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V. (2018). Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501.</note>
</biblStruct>

<biblStruct coords="10,63.00,630.57,234.00,9.96;10,72.96,642.52,224.04,9.96;10,72.96,654.48,224.04,9.96;10,72.96,667.04,145.10,8.80" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.02058</idno>
		<title level="m" coords="10,72.96,642.52,224.04,9.96;10,72.96,654.48,219.20,9.96">Neural collapse for cross-entropy class-imbalanced learning with unconstrained relu feature model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Dang, H., Tran, T., Nguyen, T., and Ho, N. (2024). Neural collapse for cross-entropy class-imbalanced learning with unconstrained relu feature model. arXiv preprint arXiv:2401.02058.</note>
</biblStruct>

<biblStruct coords="10,63.00,682.71,233.99,9.96;10,72.96,694.66,224.03,9.96;10,72.96,706.62,224.04,9.96;10,72.96,719.18,224.04,8.80;10,72.96,730.53,92.41,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coords="10,72.96,694.66,224.03,9.96;10,72.96,706.62,219.92,9.96">Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,72.96,719.18,219.72,8.80">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page">2103091118</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fang, C., He, H., Long, Q., and Su, W. J. (2021). Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. Proceedings of the National Academy of Sciences, 118(43):e2103091118.</note>
</biblStruct>

<biblStruct coords="10,315.00,74.49,234.00,9.96;10,324.96,86.45,224.04,9.96;10,324.96,98.40,80.29,9.96" xml:id="b12">
	<monogr>
		<title level="m" type="main" coords="10,416.50,74.49,132.49,9.96;10,324.96,87.05,192.72,8.80">Wordnet. In Theory and applications of ontology: computer applications</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="231" to="243"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Fellbaum, C. (2010). Wordnet. In Theory and ap- plications of ontology: computer applications, pages 231-243. Springer.</note>
</biblStruct>

<biblStruct coords="10,315.00,114.22,234.00,9.96;10,324.96,126.17,224.04,9.96;10,324.96,138.13,224.04,9.96;10,324.96,150.08,192.05,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coords="10,361.07,138.13,183.34,9.96">Shortcut learning in deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,324.96,150.69,121.63,8.80">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="665" to="673"/>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A. (2020). Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665-673.</note>
</biblStruct>

<biblStruct coords="10,315.00,165.90,233.99,9.96;10,324.96,177.85,224.04,9.96;10,324.96,189.81,224.04,9.96;10,324.96,201.76,224.04,9.96;10,324.96,214.33,224.04,8.80;10,324.96,225.67,224.04,9.96;10,324.96,237.63,26.70,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coords="10,417.47,189.81,131.53,9.96;10,324.96,201.76,176.13,9.96">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,528.64,202.37,20.36,8.80;10,324.96,214.33,224.04,8.80;10,324.96,226.28,144.85,8.80">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="776" to="780"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Gemmeke, J. F., Ellis, D. P., Freedman, D., Jansen, A., Lawrence, W., Moore, R. C., Plakal, M., and Ritter, M. (2017). Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 776-780. IEEE.</note>
</biblStruct>

<biblStruct coords="10,315.00,253.44,234.00,9.96;10,324.96,265.40,224.04,9.96;10,324.96,277.96,224.04,8.80;10,324.96,289.31,151.82,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coords="10,527.01,253.44,21.99,9.96;10,324.96,265.40,166.55,9.96">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,513.10,266.01,35.90,8.80;10,324.96,277.96,224.04,8.80;10,324.96,289.92,79.59,8.80">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778"/>
		</imprint>
	</monogr>
	<note type="raw_reference">He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778.</note>
</biblStruct>

<biblStruct coords="10,315.00,305.13,234.00,9.96;10,324.96,317.08,224.03,9.96;10,324.96,329.64,79.02,8.80" xml:id="b16">
	<monogr>
		<title level="m" type="main" coords="10,509.24,305.13,39.76,9.96;10,324.96,317.08,150.66,9.96">Distilling the knowledge in a neural network</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.</note>
</biblStruct>

<biblStruct coords="10,315.00,344.85,234.00,9.96;10,324.96,356.81,224.04,9.96;10,324.96,368.76,224.04,9.96;10,324.96,381.32,224.04,8.80;10,324.96,392.67,186.35,9.96" xml:id="b17">
	<analytic>
		<title level="a" type="main" coords="10,420.87,368.76,110.58,9.96">Searching for mobilenetv3</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,324.96,381.32,224.04,8.80;10,324.96,393.28,104.24,8.80">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1314" to="1324"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Howard, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., Wang, W., Zhu, Y., Pang, R., Vasude- van, V., et al. (2019). Searching for mobilenetv3. In Proceedings of the IEEE/CVF international confer- ence on computer vision, pages 1314-1324.</note>
</biblStruct>

<biblStruct coords="10,315.00,408.49,234.00,9.96;10,324.96,420.44,224.03,9.96;10,324.96,432.40,224.04,9.96;10,324.96,444.35,224.04,9.96;10,324.96,456.31,27.67,9.96" xml:id="b18">
	<analytic>
		<title level="a" type="main" coords="10,361.55,420.44,187.44,9.96;10,324.96,432.40,161.82,9.96">The surprising simplicity of the early-time learning dynamics of neural networks</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,496.56,433.01,52.44,8.80;10,324.96,444.96,172.12,8.80">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17116" to="17128"/>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hu, W., Xiao, L., Adlam, B., and Pennington, J. (2020). The surprising simplicity of the early-time learning dynamics of neural networks. Advances in Neural Information Processing Systems, 33:17116- 17128.</note>
</biblStruct>

<biblStruct coords="10,315.00,472.12,233.99,9.96;10,324.96,484.08,224.04,9.96;10,324.96,496.03,224.04,9.96;10,324.96,508.60,79.02,8.80" xml:id="b19">
	<monogr>
		<title level="m" type="main" coords="10,324.96,484.08,224.04,9.96;10,324.96,496.03,141.10,9.96">Limitations of neural collapse for understanding generalization in deep learning</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakkiran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08384</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Hui, L., Belkin, M., and Nakkiran, P. (2022). Limitations of neural collapse for understanding generalization in deep learning. arXiv preprint arXiv:2202.08384.</note>
</biblStruct>

<biblStruct coords="10,315.00,523.80,234.00,9.96;10,324.96,535.76,224.04,9.96;10,324.96,547.72,224.03,9.96;10,324.96,559.67,218.34,9.96" xml:id="b20">
	<analytic>
		<title level="a" type="main" coords="10,359.16,535.76,189.84,9.96;10,324.96,547.72,224.03,9.96;10,324.96,559.67,85.62,9.96">Quantifying the generalization error in deep learning in terms of data distribution and neural network smoothness</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,419.77,560.28,70.11,8.80">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="85" to="99"/>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jin, P., Lu, L., Tang, Y., and Karniadakis, G. E. (2020). Quantifying the generalization error in deep learning in terms of data distribution and neural network smoothness. Neural Networks, 130:85-99.</note>
</biblStruct>

<biblStruct coords="10,315.00,575.49,234.00,9.96;10,324.96,587.44,191.99,9.96" xml:id="b21">
	<monogr>
		<title level="m" type="main" coords="10,510.66,575.49,38.35,9.96;10,324.96,587.44,187.39,9.96">Learning multiple layers of features from tiny images</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images.</note>
</biblStruct>

<biblStruct coords="10,315.00,603.26,234.00,9.96;10,324.96,615.21,224.04,9.96;10,324.96,627.17,224.04,9.96;10,324.96,639.12,224.04,9.96;10,324.96,651.08,195.72,9.96" xml:id="b22">
	<analytic>
		<title level="a" type="main" coords="10,507.36,627.17,41.64,9.96;10,324.96,639.12,224.04,9.96;10,324.96,651.08,63.38,9.96">Dbpediaa large-scale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,397.19,651.68,57.44,8.80">Semantic web</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="195"/>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kon- tokostas, D., Mendes, P. N., Hellmann, S., Morsey, M., Van Kleef, P., Auer, S., et al. (2015). Dbpedia- a large-scale, multilingual knowledge base extracted from wikipedia. Semantic web, 6(2):167-195.</note>
</biblStruct>

<biblStruct coords="10,315.00,666.89,234.00,9.96;10,324.96,678.85,224.04,9.96;10,324.96,690.80,224.04,9.96;10,324.96,702.76,50.36,9.96" xml:id="b23">
	<analytic>
		<title level="a" type="main" coords="10,414.48,666.89,134.53,9.96;10,324.96,678.85,224.04,9.96;10,324.96,690.80,145.17,9.96">Extraction of object hierarchy data from trained deep-learning neural networks via analysis of the confusion matrix</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Malashin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,484.95,691.41,59.73,8.80">J. Opt. Tech</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="599" to="603"/>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Malashin, R. (2016). Extraction of object hierarchy data from trained deep-learning neural networks via analysis of the confusion matrix. J. Opt. Tech., 83:599-603.</note>
</biblStruct>

<biblStruct coords="10,315.00,718.57,234.00,9.96;10,324.96,730.53,224.04,9.96" xml:id="b24">
	<monogr>
		<title level="m" type="main" coords="10,492.98,718.57,56.02,9.96;10,324.96,730.53,224.04,9.96">Do deep neural networks learn shallow learnable examples first?</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">U</forename><surname>Prabhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mangalam, K. and Prabhu, V. U. (2019). Do deep neu- ral networks learn shallow learnable examples first?</note>
</biblStruct>

<biblStruct coords="11,63.00,104.25,234.00,9.96;11,72.96,116.21,224.04,9.96;11,72.96,128.16,224.04,9.96;11,72.96,140.73,79.02,8.80" xml:id="b25">
	<monogr>
		<title level="m" type="main" coords="11,72.96,116.21,224.04,9.96;11,72.96,128.16,142.19,9.96">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">McInnes, L., Healy, J., and Melville, J. (2018). Umap: Uniform manifold approximation and pro- jection for dimension reduction. arXiv preprint arXiv:1802.03426.</note>
</biblStruct>

<biblStruct coords="11,63.00,157.93,234.00,9.96;11,72.96,169.88,224.04,9.96;11,72.96,181.84,224.04,9.96;11,72.96,193.79,189.18,9.96" xml:id="b26">
	<analytic>
		<title level="a" type="main" coords="11,268.89,157.93,28.12,9.96;11,72.96,169.88,224.04,9.96;11,72.96,181.84,95.06,9.96">Prevalence of neural collapse during the terminal phase of deep learning training</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,177.49,182.45,119.51,8.80;11,72.96,194.40,88.57,8.80">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="24652" to="24663"/>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Papyan, V., Han, X., and Donoho, D. L. (2020). Preva- lence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):24652-24663.</note>
</biblStruct>

<biblStruct coords="11,63.00,211.60,234.00,9.96;11,72.96,223.56,123.78,9.96" xml:id="b27">
	<monogr>
		<title level="m" type="main" coords="11,155.93,211.60,141.07,9.96;11,72.96,223.56,119.19,9.96">Dynamics of learning and generalization in neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pezeshki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pezeshki, M. (2022). Dynamics of learning and gener- alization in neural networks.</note>
</biblStruct>

<biblStruct coords="11,63.00,241.36,234.00,9.96;11,72.96,253.32,224.04,9.96;11,72.96,265.27,224.04,9.96;11,72.96,277.84,79.02,8.80" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.03375</idno>
		<title level="m" coords="11,255.22,241.36,41.78,9.96;11,72.96,253.32,224.04,9.96;11,72.96,265.27,145.57,9.96">Complexity matters: Dynamics of feature learning in the presence of spurious correlations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Qiu, G., Kuang, D., and Goel, S. (2024). Complex- ity matters: Dynamics of feature learning in the presence of spurious correlations. arXiv preprint arXiv:2403.03375.</note>
</biblStruct>

<biblStruct coords="11,63.00,295.04,234.00,9.96;11,72.96,306.99,224.04,9.96;11,72.96,318.95,224.04,9.96;11,72.96,330.90,224.04,9.96;11,72.96,342.86,98.86,9.96" xml:id="b29">
	<analytic>
		<title level="a" type="main" coords="11,166.58,306.99,130.42,9.96;11,72.96,318.95,224.04,9.96;11,72.96,330.90,82.06,9.96">Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,161.97,331.51,135.03,8.80;11,72.96,343.47,78.34,8.80">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Raghu, M., Gilmer, J., Yosinski, J., and Sohl- Dickstein, J. (2017). Svcca: Singular vector canon- ical correlation analysis for deep learning dynamics and interpretability. Advances in neural information processing systems, 30.</note>
</biblStruct>

<biblStruct coords="11,63.00,360.67,234.00,9.96;11,72.96,372.62,224.04,9.96;11,72.96,384.58,224.04,9.96;11,72.96,396.53,224.04,9.96;11,72.96,408.49,27.67,9.96" xml:id="b30">
	<analytic>
		<title level="a" type="main" coords="11,197.47,372.62,99.52,9.96;11,72.96,384.58,167.25,9.96">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,245.25,385.18,51.75,8.80;11,72.96,397.14,172.12,8.80">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12116" to="12128"/>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C., and Dosovitskiy, A. (2021). Do vision transformers see like convolutional neural networks? Advances in Neural Information Processing Systems, 34:12116- 12128.</note>
</biblStruct>

<biblStruct coords="11,63.00,426.30,234.00,9.96;11,72.96,438.25,224.04,9.96;11,72.96,450.21,224.04,9.96;11,72.96,462.16,224.04,9.96;11,72.96,474.12,47.59,9.96" xml:id="b31">
	<analytic>
		<title level="a" type="main" coords="11,108.02,450.21,170.65,9.96">On the spectral bias of neural networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,72.96,462.77,193.34,8.80">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5301" to="5310"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F., Bengio, Y., and Courville, A. (2019). On the spectral bias of neural networks. In International conference on machine learning, pages 5301-5310.</note>
</biblStruct>

<biblStruct coords="11,123.87,474.12,32.23,9.96" xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="raw_reference">PMLR.</note>
</biblStruct>

<biblStruct coords="11,63.00,491.92,234.00,9.96;11,72.96,503.88,224.04,9.96;11,72.96,516.44,224.04,8.80;11,72.96,527.79,74.76,9.96" xml:id="b33">
	<analytic>
		<title level="a" type="main" coords="11,221.60,491.92,75.40,9.96;11,72.96,503.88,65.85,9.96">Yolo9000: better, faster, stronger</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,164.92,504.49,132.08,8.80;11,72.96,516.44,219.92,8.80">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7263" to="7271"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Redmon, J. and Farhadi, A. (2017). Yolo9000: better, faster, stronger. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 7263-7271.</note>
</biblStruct>

<biblStruct coords="11,63.00,545.60,234.00,9.96;11,72.96,557.55,224.04,9.96;11,72.96,569.51,224.04,9.96;11,72.96,581.46,224.04,9.96;11,72.96,593.42,204.73,9.96" xml:id="b34">
	<analytic>
		<title level="a" type="main" coords="11,256.88,569.51,40.12,9.96;11,72.96,581.46,173.95,9.96">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,261.22,582.07,35.79,8.80;11,72.96,594.03,142.04,8.80">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252"/>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. (2015). Imagenet large scale visual recognition challenge. Interna- tional journal of computer vision, 115:211-252.</note>
</biblStruct>

<biblStruct coords="11,63.00,611.23,234.00,9.96;11,72.96,623.18,224.04,9.96;11,72.96,635.14,224.03,9.96;11,72.96,647.09,149.65,9.96" xml:id="b35">
	<analytic>
		<title level="a" type="main" coords="11,169.49,623.18,127.51,9.96;11,72.96,635.14,77.60,9.96">The pitfalls of simplicity bias in neural networks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tamuly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,158.59,635.74,138.40,8.80;11,72.96,647.70,81.34,8.80">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9573" to="9585"/>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shah, H., Tamuly, K., Raghunathan, A., Jain, P., and Netrapalli, P. (2020). The pitfalls of simplicity bias in neural networks. Advances in Neural Information Processing Systems, 33:9573-9585.</note>
</biblStruct>

<biblStruct coords="11,63.00,664.90,234.00,9.96;11,72.96,676.86,224.04,9.96;11,72.96,688.81,64.30,9.96" xml:id="b36">
	<analytic>
		<title level="a" type="main" coords="11,237.67,664.90,59.32,9.96;11,72.96,676.86,143.72,9.96">Tabular data: Deep learning is not all you need</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shwartz-Ziv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Armon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,226.46,677.46,70.55,8.80;11,72.96,689.42,16.47,8.80">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="84" to="90"/>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shwartz-Ziv, R. and Armon, A. (2022). Tabular data: Deep learning is not all you need. Information Fu- sion, 81:84-90.</note>
</biblStruct>

<biblStruct coords="11,63.00,706.62,234.00,9.96;11,72.96,718.57,224.04,9.96;11,72.96,730.53,184.13,9.96" xml:id="b37">
	<analytic>
		<title level="a" type="main" coords="11,269.17,706.62,27.83,9.96;11,72.96,718.57,163.04,9.96">Prototypical networks for few-shot learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,244.75,719.18,52.25,8.80;11,72.96,731.14,163.61,8.80">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Snell, J., Swersky, K., and Zemel, R. (2017). Proto- typical networks for few-shot learning. Advances in neural information processing systems, 30.</note>
</biblStruct>

<biblStruct coords="11,315.00,74.49,234.00,9.96;11,324.96,86.45,224.04,9.96;11,324.96,99.01,224.04,8.80;11,324.96,110.36,74.76,9.96" xml:id="b38">
	<analytic>
		<title level="a" type="main" coords="11,324.96,86.45,74.33,9.96">Deep image prior</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,420.86,87.05,128.14,8.80;11,324.96,99.01,219.92,8.80">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9446" to="9454"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Ulyanov, D., Vedaldi, A., and Lempitsky, V. (2018). Deep image prior. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 9446-9454.</note>
</biblStruct>

<biblStruct coords="11,315.00,126.74,234.00,9.96;11,324.96,138.70,224.04,9.96;11,324.96,150.65,224.04,9.96;11,324.96,162.61,38.92,9.96" xml:id="b39">
	<analytic>
		<title level="a" type="main" coords="11,384.23,138.70,164.77,9.96;11,324.96,150.65,12.04,9.96">Matching networks for one shot learning</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,345.17,151.26,203.83,8.80;11,324.96,163.22,18.30,8.80">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. (2016). Matching networks for one shot learn- ing. Advances in neural information processing sys- tems, 29.</note>
</biblStruct>

<biblStruct coords="11,315.00,179.00,234.00,9.96;11,324.96,190.95,224.04,9.96;11,324.96,202.91,224.04,9.96;11,324.96,215.47,198.80,8.80" xml:id="b40">
	<analytic>
		<title level="a" type="main" coords="11,361.53,190.95,187.47,9.96;11,324.96,202.91,23.12,9.96">Frequency shortcut learning in neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Veldhuis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Brune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Strisciuglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,370.51,203.51,178.49,8.80;11,324.96,215.47,194.48,8.80">NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, S., Veldhuis, R., Brune, C., and Strisciuglio, N. (2022). Frequency shortcut learning in neural net- works. In NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications.</note>
</biblStruct>

<biblStruct coords="11,315.00,231.25,234.00,9.96;11,324.96,243.21,224.04,9.96;11,324.96,255.16,224.04,9.96;11,324.96,267.72,224.04,8.80;11,324.96,279.07,203.55,9.96" xml:id="b41">
	<analytic>
		<title level="a" type="main" coords="11,367.48,243.21,181.52,9.96;11,324.96,255.16,205.49,9.96">What do neural networks learn in image classification? a frequency shortcut perspective</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Veldhuis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Brune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Strisciuglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,324.96,267.72,224.04,8.80;11,324.96,279.68,121.04,8.80">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1433" to="1442"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, S., Veldhuis, R., Brune, C., and Strisciuglio, N. (2023a). What do neural networks learn in image classification? a frequency shortcut perspective. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision, pages 1433-1442.</note>
</biblStruct>

<biblStruct coords="11,315.00,295.46,234.00,9.96;11,324.96,307.41,224.04,9.96;11,324.96,319.37,224.04,9.96;11,324.96,331.93,224.04,8.80;11,324.96,343.28,105.50,9.96" xml:id="b42">
	<analytic>
		<title level="a" type="main" coords="11,414.16,307.41,134.84,9.96;11,324.96,319.37,156.19,9.96">Egeria: Efficient dnn training with knowledge-guided layer freezing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,499.35,319.98,49.64,8.80;11,324.96,331.93,224.04,8.80;11,324.96,343.89,32.71,8.80">Proceedings of the Eighteenth European Conference on Computer Systems</title>
		<meeting>the Eighteenth European Conference on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="851" to="866"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, Y., Sun, D., Chen, K., Lai, F., and Chowd- hury, M. (2023b). Egeria: Efficient dnn training with knowledge-guided layer freezing. In Proceedings of the Eighteenth European Conference on Computer Systems, pages 851-866.</note>
</biblStruct>

<biblStruct coords="11,315.00,359.67,234.00,9.96;11,324.96,371.62,224.04,9.96;11,324.96,383.58,224.04,9.96;11,324.96,395.53,224.03,9.96;11,324.96,407.49,221.21,9.96" xml:id="b43">
	<analytic>
		<title level="a" type="main" coords="11,370.42,371.62,178.58,9.96;11,324.96,383.58,224.04,9.96;11,324.96,395.53,53.81,9.96">Error-driven incremental learning in deep convolutional neural network for large-scale image classification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,396.77,396.14,152.22,8.80;11,324.96,408.09,148.42,8.80">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="177" to="186"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Xiao, T., Zhang, J., Yang, K., Peng, Y., and Zhang, Z. (2014). Error-driven incremental learning in deep convolutional neural network for large-scale image classification. In Proceedings of the 22nd ACM inter- national conference on Multimedia, pages 177-186.</note>
</biblStruct>

<biblStruct coords="11,315.00,423.87,234.00,9.96;11,324.96,435.83,224.04,9.96;11,324.96,447.78,224.04,9.96;11,324.96,459.74,220.57,9.96" xml:id="b44">
	<analytic>
		<title level="a" type="main" coords="11,456.80,423.87,92.20,9.96;11,324.96,435.83,224.04,9.96;11,324.96,447.78,23.07,9.96">Deep frequency principle towards understanding why deeper learning is faster</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,366.67,448.39,182.32,8.80;11,324.96,460.35,78.23,8.80">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="10541" to="10550"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Xu, Z. J. and Zhou, H. (2021). Deep frequency prin- ciple towards understanding why deeper learning is faster. In Proceedings of the AAAI conference on ar- tificial intelligence, volume 35, pages 10541-10550.</note>
</biblStruct>

<biblStruct coords="11,315.00,476.13,234.00,9.96;11,324.96,488.08,224.04,9.96;11,324.96,500.64,224.04,8.80;11,324.96,511.99,91.83,9.96" xml:id="b45">
	<analytic>
		<title level="a" type="main" coords="11,508.30,476.13,40.70,9.96;11,324.96,488.08,219.86,9.96">Overview frequency principle/spectral bias in deep learning</title>
		<author>
			<persName coords=""><forename type="first">Z.-Q</forename><forename type="middle">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,324.96,500.64,224.04,8.80;11,324.96,512.60,34.36,8.80">Communications on Applied Mathematics and Computation</title>
		<imprint>
			<biblScope unit="page" from="1" to="38"/>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xu, Z.-Q. J., Zhang, Y., and Luo, T. (2024). Overview frequency principle/spectral bias in deep learning. Communications on Applied Mathematics and Com- putation, pages 1-38.</note>
</biblStruct>

<biblStruct coords="11,315.00,528.38,234.00,9.96;11,324.96,540.33,224.04,9.96;11,324.96,552.29,224.04,9.96;11,324.96,564.85,79.02,8.80" xml:id="b46">
	<monogr>
		<title level="m" type="main" coords="11,379.85,540.33,169.15,9.96;11,324.96,552.29,152.99,9.96">Frequency principle: Fourier analysis sheds light on deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Z.-Q</forename><forename type="middle">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06523</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Xu, Z.-Q. J., Zhang, Y., Luo, T., Xiao, Y., and Ma, Z. (2019). Frequency principle: Fourier analysis sheds light on deep neural networks. arXiv preprint arXiv:1901.06523.</note>
</biblStruct>

<biblStruct coords="11,315.00,580.63,234.00,9.96;11,324.96,592.59,224.04,9.96;11,324.96,604.54,224.04,9.96;11,324.96,617.11,145.10,8.80" xml:id="b47">
	<monogr>
		<title level="m" type="main" coords="11,372.61,592.59,176.38,9.96;11,324.96,604.54,219.86,9.96">Neural collapse inspired feature-classifier alignment for few-shot class incremental learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.03004</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yang, Y., Yuan, H., Li, X., Lin, Z., Torr, P., and Tao, D. (2023). Neural collapse inspired feature-classifier alignment for few-shot class incremental learning. arXiv preprint arXiv:2302.03004.</note>
</biblStruct>

<biblStruct coords="11,315.00,632.89,234.00,9.96;11,324.96,644.84,224.04,9.96;11,324.96,656.80,224.03,9.96;11,324.96,669.36,79.02,8.80" xml:id="b48">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13954</idno>
		<title level="m" coords="11,514.28,632.89,34.72,9.96;11,324.96,644.84,224.04,9.96;11,324.96,656.80,149.49,9.96">Rethink the connections among generalization, memorization and the spectral bias of dnns</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Zhang, X., Xiong, H., and Wu, D. (2020). Rethink the connections among generalization, memoriza- tion and the spectral bias of dnns. arXiv preprint arXiv:2004.13954.</note>
</biblStruct>

<biblStruct coords="11,315.00,685.14,234.00,9.96;11,324.96,697.09,224.04,9.96;11,324.96,709.05,224.04,9.96;11,324.96,721.00,130.01,9.96" xml:id="b49">
	<analytic>
		<title level="a" type="main" coords="11,541.53,685.14,7.47,9.96;11,324.96,697.09,224.04,9.96;11,324.96,709.05,178.57,9.96">A linear frequency principle model to understand the absence of overfitting in neural networks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-Q</forename><forename type="middle">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,514.89,709.66,34.11,8.80;11,324.96,721.61,64.52,8.80">Chinese Physics Letters</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">38701</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang, Y., Luo, T., Ma, Z., and Xu, Z.-Q. J. (2021). A linear frequency principle model to understand the absence of overfitting in neural networks. Chinese Physics Letters, 38(3):038701.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>