<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coords="1,107.76,82.24,71.71,14.93;1,195.49,82.24,68.04,14.93;1,279.53,84.48,106.69,11.94;1,402.22,84.48,59.18,11.94;1,477.39,84.48,26.70,11.94;1,107.76,102.17,370.06,14.93">ARMAP: SCALING AUTONOMOUS AGENTS VIA AUTOMATIC REWARD MODELING AND PLANNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-17">17 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,116.47,137.54,66.15,8.96;1,182.62,136.03,1.36,6.12"><forename type="first">Zhenfang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName coords="1,249.58,137.54,47.88,8.96;1,297.46,136.03,1.36,6.12"><forename type="first">Delin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName coords="1,347.62,137.54,34.61,8.96;1,382.23,136.03,1.36,6.12"><forename type="first">Rui</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName coords="1,113.98,176.69,51.12,8.96;1,165.10,175.19,1.36,6.12"><forename type="first">Wenjun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName coords="1,251.00,176.69,54.53,8.96"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0" coords="1,113.98,148.89,102.59,8.64;1,249.58,148.89,65.03,8.64;1,347.62,148.89,150.40,8.64">
								<note type="raw_affiliation">MIT-IBM Watson AI Lab UMass Amherst University of California , Los Angeles</note>
								<orgName type="institution" key="instit1">MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution" key="instit2">UMass Amherst</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1" coords="1,113.98,188.04,65.03,8.64;1,251.00,188.04,186.99,8.64">
								<note type="raw_affiliation">UMass Amherst UMass Amherst and MIT - IBM Watson AI Lab</note>
								<orgName type="institution" key="instit1">UMass Amherst</orgName>
								<orgName type="institution" key="instit2">UMass Amherst</orgName>
								<orgName type="institution" key="instit3">MIT</orgName>
								<orgName type="institution" key="instit4">IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coords="1,107.76,82.24,71.71,14.93;1,195.49,82.24,68.04,14.93;1,279.53,84.48,106.69,11.94;1,402.22,84.48,59.18,11.94;1,477.39,84.48,26.70,11.94;1,107.76,102.17,370.06,14.93">ARMAP: SCALING AUTONOMOUS AGENTS VIA AUTOMATIC REWARD MODELING AND PLANNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-17">17 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">EA628A04598A73FC2554278AC127AED0</idno>
					<idno type="arXiv">arXiv:2502.12130v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-02-18T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,143.87,254.18,324.27,8.64;1,143.87,265.14,133.32,8.64">Large language models (LLMs) have demonstrated remarkable capabilities across a range of text-generation tasks.</s><s coords="1,282.96,265.14,185.17,8.64;1,143.87,276.10,324.27,8.64;1,143.87,287.06,258.60,8.64">However, LLMs still struggle with problems requiring multi-step decision-making and environmental feedback, such as online shopping, scientific reasoning, and mathematical problem-solving.</s><s coords="1,405.45,287.06,62.68,8.64;1,143.87,298.02,255.73,8.64">Unlike pure text data, collecting large-scale decision-making data is challenging.</s><s coords="1,402.72,298.02,65.76,8.64;1,143.87,308.98,324.27,8.64;1,143.87,319.94,173.70,8.64">Moreover, many powerful LLMs are only accessible through APIs, which hinders their fine-tuning for agent tasks due to cost and complexity.</s><s coords="1,321.07,319.94,148.31,8.64;1,143.51,330.90,324.63,8.64;1,143.87,341.85,169.71,8.64">To address LLM agents' limitations, we propose a framework that can automatically learn a reward model from the environment without human annotations.</s><s coords="1,319.82,341.85,148.31,8.64;1,143.87,352.81,326.01,8.64">This model can be used to evaluate the action trajectories of LLM agents and provide heuristics for task planning.</s><s coords="1,143.87,363.77,324.27,8.64;1,143.87,374.73,264.13,8.64">Specifically, our approach involves employing one LLM-based agent to navigate an environment randomly, generating diverse action trajectories.</s><s coords="1,412.73,374.73,56.65,8.64;1,143.87,385.69,324.26,8.64;1,143.87,396.65,242.21,8.64">Subsequently, a separate LLM is leveraged to assign a task intent and synthesize a negative response alongside the correct response for each trajectory.</s><s coords="1,390.35,396.65,78.03,8.64;1,143.87,407.61,324.27,8.64;1,143.87,418.57,270.56,8.64">These triplets (task intent, positive response, and negative response) are then utilized as training data to optimize a reward model capable of scoring action trajectories.</s><s coords="1,419.16,418.57,48.97,8.64;1,143.87,429.53,324.27,8.64;1,143.87,440.48,153.57,8.64">This reward model can be integrated with LLM-based agents and various planning algorithms to enhance task-solving performance.</s><s coords="1,301.85,440.48,166.28,8.64;1,143.87,451.44,324.26,8.64;1,143.87,462.40,51.64,8.64">The effectiveness and generalizability of our framework are demonstrated through evaluations conducted on different agent benchmarks.</s><s coords="1,198.64,462.40,271.15,8.64;1,143.62,473.36,263.95,8.64">In conclusion, our proposed framework represents a significant advancement in enhancing LLM agents' decision-making capabilities.</s><s coords="1,410.58,473.36,57.56,8.64;1,143.87,484.32,324.27,8.64;1,143.87,495.28,324.26,8.64;1,143.87,506.24,101.87,8.64">By automating the learning of reward models, we overcome the challenges of data scarcity and API limitations, potentially revolutionizing the application of LLMs in complex and interactive environments.</s><s coords="1,248.85,506.24,219.29,8.64;1,143.87,517.20,324.27,8.64;1,143.87,528.16,68.90,8.64">This research paves the way for more sophisticated AI agents capable of tackling a wide range of real-world problems requiring multi-step decision-making.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" coords="1,108.30,562.58,97.69,10.37">INTRODUCTION</head><p><s coords="1,108.00,588.20,396.00,8.64;1,108.00,599.16,396.00,8.64;1,108.00,610.12,396.00,8.64;1,108.00,621.08,396.83,8.64;1,107.64,632.04,139.32,8.64">Developing AI agents capable of perceiving environments, understanding instructions, and acting to accomplish a wide range of tasks in interactive settings <ref type="bibr" coords="1,349.45,599.16,57.27,8.64" target="#b4">(Brooks, 1986</ref>) have many real-world applications, including virtual human assistants <ref type="bibr" coords="1,298.79,610.12,73.06,8.64" target="#b45">(Reed et al., 2022;</ref><ref type="bibr" coords="1,374.36,610.12,89.45,8.64" target="#b6">Casheekar et al., 2024)</ref>, business process management <ref type="bibr" coords="1,194.90,621.08,99.90,8.64" target="#b25">(Kirchdorfer et al., 2024)</ref>, and robotic process automation <ref type="bibr" coords="1,430.58,621.08,74.25,8.64" target="#b44">(Rana et al., 2023;</ref><ref type="bibr" coords="1,107.64,632.04,66.69,8.64" target="#b1">Ahn et al., 2022;</ref><ref type="bibr" coords="1,176.82,632.04,65.85,8.64" target="#b38">Palo et al., 2023)</ref>.</s></p><p><s coords="1,107.69,648.97,396.31,8.64;1,108.00,659.93,397.65,8.64;1,108.00,670.89,306.24,8.64">The recent advent of large generative models has revolutionized numerous applications, such as question answering <ref type="bibr" coords="1,188.53,659.93,88.05,8.64" target="#b43">(Rajpurkar et al., 2016</ref>), text summarization <ref type="bibr" coords="1,367.57,659.93,89.58,8.64">(Hermann et al., 2015)</ref>, and multimodal understanding <ref type="bibr" coords="1,195.44,670.89,74.78,8.64" target="#b7">(Chen et al., 2015;</ref><ref type="bibr" coords="1,272.69,670.89,74.67,8.64" target="#b14">Goyal et al., 2017;</ref><ref type="bibr" coords="1,349.84,670.89,60.05,8.64" target="#b67">Yu et al., 2016)</ref>.</s><s coords="1,417.34,670.89,86.67,8.64;1,108.00,681.85,396.00,8.64;1,108.00,692.81,397.74,8.64">However, while these models excel in text comprehension and generation tasks, their performance in decision-making scenarios-such as online shopping and scientific reasoning falls relative short of human capabilities.</s></p><p><s coords="2,108.00,244.48,396.00,8.64;2,108.00,255.44,218.38,8.64">Figure <ref type="figure" coords="2,136.28,244.48,3.84,8.64">1</ref>: In Fig. <ref type="figure" coords="2,179.00,244.48,4.94,8.64">1</ref> (a), we show that it is difficult for LLM agents to generate multi-step plans in an interactive environment to achieve the instruction goal.</s><s coords="2,329.48,255.44,174.51,8.64;2,108.00,266.39,396.00,8.64;2,108.00,277.35,39.53,8.64">However, it is relatively easy for an LLM to learn a reward model that can evaluate whether the trajectories meet the task instructions, as shown in Fig. <ref type="figure" coords="2,126.13,277.35,14.42,8.64">1 (b</ref>).</s><s coords="2,150.61,277.35,353.74,8.64;2,108.00,288.31,141.11,8.64">In Fig. <ref type="figure" coords="2,179.45,277.35,4.94,8.64">1</ref> (c), we show that a learned reward model can be used to guide the default policy models to improve action planning.</s><s coords="3,108.00,85.34,64.37,8.64">difficult to scale.</s><s coords="3,175.32,85.34,328.68,8.64;3,107.64,96.30,210.87,8.64">In this paper, we introduce an automated method to learn multi-modal reward models without relying on state-of-the-art LLMs for guidance.</s><s coords="3,321.44,96.30,182.56,8.64;3,108.00,107.26,369.76,8.64">Furthermore, previous work has not considered integrating the learned reward models with various planning algorithms for problem-solving.</s></p><p><s coords="3,107.69,124.19,253.76,8.64">The process of learning the reward model involves three steps.</s><s coords="3,364.79,124.19,139.22,8.64;3,108.00,135.15,396.34,8.64;3,108.00,146.11,299.63,8.64">Initially, we utilize an LLM-based agent (e.g., <ref type="bibr" coords="3,157.11,135.15,80.04,8.64" target="#b12">Dubey et al. (2024)</ref>) to navigate in the environments, aiming to achieve a randomly proposed intent while collecting extensive action trajectory demonstrations.</s><s coords="3,410.73,146.11,93.27,8.64;3,108.00,157.07,396.00,8.64;3,108.00,168.03,83.93,8.64">Subsequently, the LLM model examines the collected trajectories and proposes a refined intent that the sampled trajectories actually accomplish.</s><s coords="3,196.32,168.03,307.68,8.64;3,108.00,178.99,114.38,8.64">Additionally, we prompt the LLM to generate negative trajectories that fail to achieve the intended task.</s><s coords="3,225.47,178.99,278.54,8.64;3,108.00,189.95,397.65,8.64;3,108.00,200.91,396.00,8.64;3,108.00,211.86,132.28,8.64">Finally, based on the synthetic data (intents, positive trajectories, and negative trajectories) collected, we train a customized reward model using widely adopted visionlanguage models such as VILA <ref type="bibr" coords="3,241.54,200.91,70.64,8.64" target="#b33">(Lin et al., 2023)</ref> to evaluate whether the user's intent has been fulfilled by the action trajectories.</s><s coords="3,243.40,211.86,260.60,8.64;3,108.00,222.82,396.00,8.64;3,108.00,233.78,29.62,8.64">With this automatic reward model, we enhance the performance of LLM-based agents in conjunction with various planning algorithms such as best of n, reflexion, and MCTS.</s></p><p><s coords="3,108.00,250.72,396.00,8.64;3,108.00,261.68,396.00,8.64;3,108.00,272.64,306.25,8.64">In summary, we introduce a novel framework ARMAP (autonomous Agents from automatic Reward Modeling And Planning) for LLM-based agents incorporating an automatic reward model that evaluates task completion, analogous to mental simulation in human cognition.</s><s coords="3,417.24,272.64,86.76,8.64;3,108.00,283.60,396.00,8.64;3,108.00,294.55,57.89,8.64">This framework offers several advantages: (1) Effectiveness: It enhances the performance of various LLM agents across different tasks.</s><s coords="3,168.97,294.55,335.03,8.64;3,108.00,305.51,397.74,8.64">(2) Flexibility: It eliminates the need for fine-tuning the LLMs themselves and allows for optimization of custom reward targets during inference, enabling more controllable generation.</s></p><p><s coords="3,107.67,316.47,396.33,8.64;3,108.00,327.43,338.66,8.64">(3) Practicality: The training of the automatic reward model does not rely on labor-intensive labeling or state-of-the-art commercial LLMs, making it more feasible and widely applicable.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" coords="3,108.30,356.69,102.90,10.37">RELATED WORK</head><p><s coords="3,108.00,382.26,98.74,8.96">LLMs for Agent tasks.</s><s coords="3,216.70,382.65,287.30,8.64;3,108.00,393.61,396.83,8.64;3,108.00,404.57,176.08,8.64">Our research is related to deploying large language models (LLMs) as agents for decision-making tasks in interactive environments <ref type="bibr" coords="3,359.63,393.61,69.16,8.64" target="#b35">(Liu et al., 2023;</ref><ref type="bibr" coords="3,431.72,393.61,73.11,8.64" target="#b76">Zhou et al., 2023;</ref><ref type="bibr" coords="3,108.00,404.57,86.38,8.64" target="#b50">Shridhar et al., 2020;</ref><ref type="bibr" coords="3,197.20,404.57,82.51,8.64" target="#b57">Toyama et al., 2021)</ref>.</s><s coords="3,288.18,404.57,215.82,8.64;3,108.00,415.53,396.00,8.64;3,108.00,426.49,191.79,8.64">Earlier works, such as <ref type="bibr" coords="3,381.58,404.57,73.91,8.64">(Yao et al., 2023a)</ref>, fine-tuned models like BERT <ref type="bibr" coords="3,186.89,415.53,83.27,8.64" target="#b11">(Devlin et al., 2019)</ref> for decision-making in simplified environments, such as online shopping or mobile phone manipulation.</s><s coords="3,302.90,426.49,201.10,8.64;3,108.00,437.45,396.00,8.64;3,108.00,448.41,162.38,8.64">With the advent of large language models <ref type="bibr" coords="3,473.38,426.49,30.62,8.64;3,108.00,437.45,49.42,8.64" target="#b5">(Brown et al., 2020;</ref><ref type="bibr" coords="3,160.67,437.45,84.13,8.64" target="#b37">OpenAI et al., 2024)</ref>, it became feasible to perform decision-making tasks through zero-shot or few-shot in-context learning.</s><s coords="3,273.48,448.41,230.51,8.64;3,108.00,459.37,397.25,8.64;3,108.00,470.33,25.22,8.64">To better assess the capabilities of LLMs as agents, several models have been developed <ref type="bibr" coords="3,228.02,459.37,76.75,8.64" target="#b10">(Deng et al., 2024;</ref><ref type="bibr" coords="3,307.59,459.37,76.76,8.64" target="#b62">Xiong et al., 2024;</ref><ref type="bibr" coords="3,387.17,459.37,73.94,8.64" target="#b21">Hong et al., 2023;</ref><ref type="bibr" coords="3,463.92,459.37,41.33,8.64;3,108.00,470.33,21.02,8.64" target="#b63">Yan et al., 2023)</ref>.</s><s coords="3,136.19,470.33,367.82,8.64;3,108.00,481.28,342.21,8.64">Most approaches <ref type="bibr" coords="3,204.44,470.33,75.81,8.64" target="#b75">(Zheng et al., 2024;</ref><ref type="bibr" coords="3,282.38,470.33,69.30,8.64" target="#b10">Deng et al., 2024)</ref> provide the agent with observation and action history, and the language model predicts the next action via in-context learning.</s><s coords="3,453.29,481.28,51.95,8.64;3,108.00,492.24,396.00,8.64;3,108.00,503.20,306.68,8.64">Additionally, some methods <ref type="bibr" coords="3,169.16,492.24,80.18,8.64" target="#b72">(Zhang et al., 2023;</ref><ref type="bibr" coords="3,251.97,492.24,59.86,8.64" target="#b32">Li et al., 2023;</ref><ref type="bibr" coords="3,314.47,492.24,72.27,8.64" target="#b54">Song et al., 2024)</ref> attempt to distill trajectories from state-of-the-art language models to train more effective policy models.</s><s coords="3,417.78,503.20,86.39,8.64;3,108.00,514.16,397.24,8.64;3,108.00,525.12,235.39,8.64">In contrast, our paper introduces a novel framework that automatically learns a reward model from LLM agent navigation, using it to guide the agents in making more effective plans.</s><s coords="3,108.00,541.67,68.35,8.96">LLM Planning.</s><s coords="3,183.50,542.06,277.87,8.64">Our paper is also related to planning with large language models.</s><s coords="3,468.52,542.06,37.13,8.64;3,108.00,553.02,396.00,8.64;3,108.00,563.97,22.86,8.64">Early researchers <ref type="bibr" coords="3,149.21,553.02,84.47,8.64" target="#b5">(Brown et al., 2020)</ref> often prompted large language models to directly perform agent tasks.</s><s coords="3,134.28,563.97,369.72,8.64;3,108.00,574.93,189.63,8.64">Later, <ref type="bibr" coords="3,160.50,563.97,68.03,8.64" target="#b64">Yao et al. (2022)</ref> proposed ReAct, which combined LLMs for action prediction with chain-of-thought prompting <ref type="bibr" coords="3,224.03,574.93,69.23,8.64" target="#b60">(Wei et al., 2022)</ref>.</s><s coords="3,301.32,574.93,203.93,8.64;3,108.00,585.89,396.00,8.64;3,108.00,596.85,192.82,8.64">Several other works <ref type="bibr" coords="3,385.10,574.93,75.38,8.64">(Yao et al., 2023b;</ref><ref type="bibr" coords="3,463.18,574.93,42.07,8.64;3,108.00,585.89,22.24,8.64" target="#b17">Hao et al., 2023;</ref><ref type="bibr" coords="3,132.33,585.89,67.56,8.64" target="#b74">Zhao et al., 2023;</ref><ref type="bibr" coords="3,201.98,585.89,67.01,8.64" target="#b41">Qiao et al., 2024)</ref> have focused on enhancing multi-step reasoning capabilities by integrating LLMs with tree search methods.</s><s coords="3,305.01,596.85,198.99,8.64;3,108.00,607.81,100.32,8.64">Our model differs from these previous studies in several significant ways.</s><s coords="3,213.35,607.81,290.65,8.64;3,108.00,618.77,396.17,8.64;3,108.00,629.73,320.58,8.64">First, rather than solely focusing on text generation tasks, our pipeline addresses multi-step action planning tasks in interactive environments, where we must consider not only historical input but also multimodal feedback from the environment.</s><s coords="3,433.92,629.73,70.25,8.64;3,108.00,640.69,396.00,8.64;3,108.00,651.65,396.00,8.64;3,108.00,662.60,340.92,8.64">Additionally, our pipeline involves automatic learning of the reward model from the environment without relying on human-annotated data, whereas previous works rely on prompting-based frameworks that require large commercial LLMs like <ref type="bibr" coords="3,226.37,662.60,116.02,8.64">GPT-4 (OpenAI et al., 2024)</ref> to learn action prediction.</s><s coords="3,452.01,662.60,53.23,8.64;3,107.64,673.56,281.28,8.64">Furthermore, ARMAP supports a variety of planning algorithms beyond tree search.</s></p><p><s coords="3,108.00,690.11,121.29,8.96">Learning from AI Feedback.</s><s coords="3,232.38,690.50,271.61,8.64;3,108.00,701.46,397.25,8.64;3,108.00,712.42,265.71,8.64">In contrast to prior work on LLM planning, our approach also draws on recent advances in learning from AI feedback <ref type="bibr" coords="3,313.98,701.46,70.03,8.64" target="#b2">(Bai et al., 2022;</ref><ref type="bibr" coords="3,387.22,701.46,67.75,8.64" target="#b31">Lee et al., 2023;</ref><ref type="bibr" coords="3,458.18,701.46,47.07,8.64;3,108.00,712.42,23.15,8.64" target="#b69">Yuan et al., 2024;</ref><ref type="bibr" coords="3,134.69,712.42,84.54,8.64" target="#b48">Sharma et al., 2024;</ref><ref type="bibr" coords="3,222.76,712.42,68.59,8.64" target="#b39">Pan et al., 2024;</ref><ref type="bibr" coords="3,294.89,712.42,74.35,8.64">Koh et al., 2024b)</ref>.</s><s coords="3,379.93,712.42,124.07,8.64;3,108.00,723.38,396.00,8.64;4,333.77,285.53,8.79,10.41">These studies initially prompt state-of-the-art large language models to generate text responses that adhere to predefined principles ...</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,145.87,151.42,60.09,6.69">Sample Trajectories</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,118.76,166.37,22.88,6.69;4,179.52,165.08,39.49,6.69">Actions Environment</head><p><s coords="4,269.33,115.68,72.59,6.69">Refine Task Instructions</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,271.73,90.24,64.07,6.69">Sampled Trajectories</head><p><s coords="4,260.04,131.05,94.98,5.95;4,257.30,138.87,100.42,5.95;4,256.99,146.69,101.04,5.95">I am looking for straight leg jeans in sandblast light color, also in 40w x 34l size, and price lower than 170 dollars.</s><s coords="4,108.00,319.04,202.75,8.64">Figure <ref type="figure" coords="4,136.14,319.04,3.82,8.64">2</ref>: The pipeline of our ARMAP framework.</s><s coords="4,313.83,319.04,190.17,8.64;4,108.00,330.00,396.00,8.64;4,108.00,340.96,80.33,8.64">We first generate an initial task instruction using LLMs with in-context learning and sample trajectories aligned with the initial language instructions in the environment.</s><s coords="4,193.67,340.96,310.32,8.64;4,108.00,351.92,237.14,8.64">Next, we use the LLM to summarize the sampled trajectories and generate refined task instructions that better match these trajectories.</s><s coords="4,348.26,351.92,155.74,8.64;4,108.00,362.88,396.00,8.64;4,108.00,373.84,33.02,8.64">We then modify specific actions within the trajectories to perform new actions in the environment, collecting negative trajectories in the process.</s><s coords="4,144.15,373.84,359.85,8.64;4,108.00,384.80,377.76,8.64">Using the refined task instructions, along with both positive and negative trajectories, we train a lightweight reward model to distinguish between matching and non-matching trajectories.</s><s coords="4,488.82,384.80,15.18,8.64;4,108.00,395.75,376.51,8.64">The learned reward model can then collaborate with various LLM agents to improve task planning.</s><s coords="4,108.00,465.34,273.60,8.64">and then potentially fine-tune the LLMs with reinforcement learning.</s><s coords="4,384.71,465.34,119.29,8.64;4,108.00,476.30,232.55,8.64">Like previous studies, we also prompt large language models to generate synthetic data.</s><s coords="4,344.12,476.30,159.88,8.64;4,108.00,487.26,396.00,8.64;4,108.00,498.22,198.88,8.64">However, unlike them, we focus not on fine-tuning a better generative model but on developing a classification model that evaluates how well action trajectories fulfill the intended instructions.</s><s coords="4,309.97,498.22,194.03,8.64;4,108.00,509.18,173.75,8.64">This approach is simpler, requires no reliance on state-of-the-art LLMs, and is more efficient.</s><s coords="4,284.83,509.18,219.16,8.64;4,108.00,520.14,385.74,8.64">We also demonstrate that our learned reward model can integrate with various LLMs and planning algorithms, consistently improving their performance.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,272.71,184.58,88.90,6.69">Sample Negative Trajectories</head><p><s coords="4,108.00,536.69,103.89,8.96">Inference-Time Scaling.</s><s coords="4,219.81,537.07,284.36,8.64;4,108.00,548.03,71.23,8.64"><ref type="bibr" coords="4,219.81,537.07,74.42,8.64" target="#b53">Snell et al. (2024)</ref> validates the efficacy of inference-time scaling for language models.</s><s coords="4,182.80,548.03,321.20,8.64;4,108.00,558.99,396.83,8.64;4,108.00,569.95,72.21,8.64">Based on inference-time scaling, various methods have been proposed, such as random sampling <ref type="bibr" coords="4,178.33,558.99,79.44,8.64">(Wang et al., 2022b)</ref> and tree-search methods <ref type="bibr" coords="4,357.16,558.99,67.78,8.64" target="#b17">(Hao et al., 2023;</ref><ref type="bibr" coords="4,427.29,558.99,77.54,8.64">Zhang et al., 2024a;</ref><ref type="bibr" coords="4,108.00,569.95,68.01,8.64" target="#b16">Guan et al., 2025)</ref>.</s><s coords="4,183.24,569.95,320.77,8.64;4,108.00,580.91,135.53,8.64">Concurrently, several works have also leveraged inference-time scaling to improve the performance of agentic tasks.</s><s coords="4,250.24,580.91,253.76,8.64;4,108.00,591.87,396.00,8.64;4,108.00,602.83,30.40,8.64"><ref type="bibr" coords="4,250.24,580.91,74.84,8.64">Koh et al. (2024b)</ref> adopts a training-free approach, employing MCTS to enhance policy model performance during inference and prompting the LLM to return the reward.</s><s coords="4,144.18,602.83,359.82,8.64;4,108.00,613.79,160.13,8.64"><ref type="bibr" coords="4,144.18,602.83,64.33,8.64" target="#b15">Gu et al. (2024)</ref> introduces a novel speculative reasoning approach to bypass irreversible actions by leveraging LLMs or VLMs.</s><s coords="4,273.20,613.79,230.80,8.64;4,108.00,624.75,146.80,8.64">It also employs tree search to improve performance and prompts an LLM to output rewards.</s><s coords="4,261.79,624.75,242.21,8.64;4,108.00,635.71,347.35,8.64"><ref type="bibr" coords="4,261.79,624.75,64.14,8.64" target="#b15">Yu et al. (2024)</ref> proposes Reflective-MCTS to perform tree search and fine-tune the GPT model, leading to improvements in <ref type="bibr" coords="4,378.51,635.71,72.58,8.64">Koh et al. (2024a)</ref>.</s><s coords="4,462.08,635.71,43.67,8.64;4,107.67,646.66,397.00,8.64;4,108.00,657.62,150.09,8.64"><ref type="bibr" coords="4,462.08,635.71,43.67,8.64;4,107.67,646.66,26.96,8.64" target="#b40">Putta et al. (2024)</ref> also utilizes MCTS to enhance performance on web-based tasks such as <ref type="bibr" coords="4,432.75,646.66,71.92,8.64">Yao et al. (2023a)</ref> and real-world booking environments.</s><s coords="4,263.72,657.62,240.28,8.64;4,108.00,668.58,210.36,8.64"><ref type="bibr" coords="4,263.72,657.62,63.92,8.64" target="#b34">Lin et al. (2025)</ref> utilizes the stepwise reward to give effective intermediate guidance across different agentic tasks.</s><s coords="4,321.47,668.58,182.53,8.64;4,108.00,679.54,180.42,8.64">Our work differs from previous efforts in two key aspects: (1) Broader Application Domain.</s><s coords="4,291.49,679.54,212.51,8.64;4,108.00,690.50,396.00,8.64;4,108.00,701.46,322.67,8.64">Unlike prior studies that primarily focus on tasks from a single domain, our method demonstrates strong generalizability across web agents, mathematical reasoning, and scientific discovery domains, further proving its effectiveness.</s><s coords="4,437.10,701.46,66.90,8.64;4,108.00,712.42,112.29,8.64">(2) Flexible and Effective Reward Modeling.</s><s coords="4,223.38,712.42,280.62,8.64;4,108.00,723.38,256.00,8.64">Instead of simply prompting an LLM as a reward model, we finetune a small scale VLM <ref type="bibr" coords="4,179.11,723.38,67.24,8.64" target="#b33">(Lin et al., 2023)</ref> to evaluate input trajectories.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" coords="5,108.30,84.04,57.02,10.37">MODEL</head><p><s coords="5,108.00,108.80,396.00,8.64;5,108.00,119.75,220.74,8.64">In this section, we provide a detailed introduction to our framework, autonomous Agents from automatic Reward Modeling And Planning (ARMAP).</s><s coords="5,331.23,119.75,172.77,8.64;5,108.00,130.71,396.00,8.64;5,108.00,141.67,45.66,8.64">The framework includes automated reward data generation in section 3.2, reward model design in section 3.3, and planning algorithms in section 3.4.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" coords="5,108.25,166.47,84.96,8.64">BACKGROUND</head><p><s coords="5,107.69,187.00,396.56,8.64;5,108.00,197.64,214.07,8.96">The planning tasks for LLM agents can be typically formulated as a Partially Observable Markov Decision Process (POMDP): (X , S, A, O, T ), where:</s></p><p><s coords="5,135.40,218.10,134.22,8.96;5,135.40,232.82,143.25,8.96;5,135.40,247.54,187.24,8.96">• X is the set of text instructions; • S is the set of environment states; • A is the set of available actions at each state;</s></p><p><s coords="5,135.40,262.25,368.60,8.96;5,143.87,273.53,196.63,8.64;5,135.40,287.93,368.95,8.96;5,143.87,299.21,126.07,8.64">• O represents the observations available to the agents, including text descriptions and visual information about the environment in our setting; • T : S × A → S is the transition function of states after taking actions, which is given by the environment in our settings.</s></p><p><s coords="5,108.00,319.36,396.00,9.65;5,108.00,330.32,206.82,9.65;5,314.82,328.74,6.31,6.12;5,314.82,334.83,15.01,6.12;5,332.83,330.63,171.17,8.64;5,108.00,341.27,395.99,9.65;5,108.00,352.55,59.68,8.64">Given a task instruction x ∈ X and the initial environment state s 0 ∈ S, planning tasks require the LLM agents to propose a sequence of actions {a n } N n=1 that aim to complete the given task, where a n ∈ A represents the action taken at time step n, and N is the total number of actions executed in a trajectory.</s><s coords="5,172.93,352.23,331.06,9.65;5,108.00,363.19,119.87,9.65">Following the n-th action, the environment transitions to state s n , and the agent receives a new observation o n .</s><s coords="5,230.91,363.51,273.26,8.64;5,108.00,374.47,167.12,8.64">Based on the accumulated state and action histories, the task evaluator determines whether the task is completed.</s></p><p><s coords="5,107.64,391.09,396.52,8.96;5,108.00,402.05,198.03,8.96">An important component of our framework is the learned reward model R, which estimates whether a trajectory h has successfully addressed the task:</s></p><formula xml:id="formula_0" coords="5,279.50,418.84,225.17,8.96">r = R(x , h),<label>(1)</label></formula><p><s coords="5,107.64,435.64,72.09,9.65;5,179.73,434.07,6.31,6.12;5,179.73,440.15,15.01,6.12;5,195.24,435.64,24.64,9.65;5,219.89,434.07,6.31,6.12;5,219.89,440.15,15.01,6.12;5,235.40,435.64,30.70,9.65;5,266.09,434.07,6.31,6.12;5,266.09,440.15,15.01,6.12;5,284.12,435.64,174.51,9.65;5,458.63,434.07,6.31,6.12;5,458.63,440.15,15.01,6.12;5,476.66,435.96,27.34,8.64;5,108.00,446.60,381.51,8.96">where h = {{a n } N n=1 , {o n } N n=0 }, {a n } N n=1 are the actions taken in the trajectory, {o n } N n=0 are the corresponding environment observations, and r is the predicted reward from the reward model.</s><s coords="5,492.61,446.92,11.74,8.64;5,108.00,457.88,395.99,8.64;5,108.00,468.84,199.42,8.64">By integrating this reward model with LLM agents, we can enhance their performance across various environments using different planning algorithms.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" coords="5,108.25,493.64,206.78,8.64">AUTOMATIC REWARD DATA GENERATION.</head><p><s coords="5,107.69,514.16,396.31,8.64;5,108.00,524.80,242.01,9.65;5,350.01,523.23,7.60,6.12;5,350.01,529.31,17.16,6.12;5,367.66,524.80,136.51,8.96;5,108.00,536.08,82.10,8.64">To train a reward model capable of estimating the reward value of history trajectories, we first need to collect a set of training language instructions {x m } M m=1 , where M represents the number of instruction goals.</s><s coords="5,194.98,535.76,260.49,8.96;5,455.47,534.19,6.12,6.12;5,455.47,540.26,7.07,6.12;5,463.04,535.76,4.98,8.74;5,468.02,534.19,7.60,6.12;5,468.02,540.27,17.16,6.12;5,488.76,536.08,15.24,8.64;5,108.00,546.72,262.12,8.96;5,370.12,545.14,6.23,6.12;5,370.12,551.22,7.07,6.12;5,377.69,546.72,4.98,8.74;5,382.67,545.14,7.60,6.12;5,382.67,551.23,17.16,6.12;5,403.46,547.04,100.78,8.64;5,108.00,558.00,55.60,8.64">Each instruction corresponds to a set of positive trajectories {h + m } M m=1 that match the instruction goals and a set of negative trajectories {h - m } M m=1 that fail to meet the task requirements.</s><s coords="5,166.68,558.00,338.97,8.64;5,108.00,568.96,227.42,8.64">This process typically involves human annotators and is time-consuming and laborintensive <ref type="bibr" coords="5,147.01,568.96,96.79,8.64" target="#b8">(Christiano et al., 2017;</ref><ref type="bibr" coords="5,246.52,568.96,84.53,8.64" target="#b42">Rafailov et al., 2024)</ref>.</s><s coords="5,339.19,568.96,149.19,8.64">As shown in Fig. <ref type="figure" coords="5,411.65,568.96,5.08,8.64">8</ref> of the Appendix.</s><s coords="5,492.15,568.96,11.85,8.64;5,108.00,579.91,396.00,8.64;5,108.00,590.87,233.85,8.64">we automate data collection by using Large Language Model (LLM) agents to navigate environments and summarize the navigation goals without human labels.</s></p><p><s coords="5,108.00,607.42,93.80,8.96">Instruction Synthesis.</s><s coords="5,204.90,607.81,299.10,8.64;5,108.00,618.77,49.70,8.64">The first step in data generation is to propose a task instruction for a given observation.</s><s coords="5,161.81,618.77,275.59,8.64">We achieve this using the in-context learning capabilities of LLMs.</s><s coords="5,441.50,618.77,62.67,8.64;5,108.00,629.73,233.91,8.64">The prompt for instruction generation is shown in Fig. <ref type="figure" coords="5,266.02,629.73,5.07,8.64" target="#fig_6">9</ref> of the Appendix.</s><s coords="5,345.00,629.73,159.01,8.64;5,108.00,640.69,396.00,8.64;5,108.00,651.65,225.78,8.64">Specifically, we provide some few-shot examples in context along with the observation of an environment state to an LLM, asking it to summarize the observation and propose instruction goals.</s><s coords="5,336.89,651.65,167.11,8.64;5,108.00,662.29,97.54,8.96;5,205.54,660.71,14.00,6.12;5,205.54,666.79,7.07,6.12;5,220.22,662.29,4.98,8.74;5,225.21,660.71,7.60,6.12;5,225.21,666.80,17.16,6.12;5,242.86,662.29,262.85,8.96">In this way, we collect a set of synthesized language instructions {x raw m } M m=1 , where M represents the total number of synthesized instructions.</s><s coords="5,107.67,679.15,91.47,8.96">Trajectory Collection.</s><s coords="5,202.22,679.22,142.05,8.96;5,344.27,677.65,14.00,6.12;5,344.27,683.73,7.07,6.12;5,361.42,679.54,142.58,8.64;5,108.00,690.50,396.00,8.64;5,108.00,701.14,10.67,8.74;5,118.68,699.57,14.00,6.12;5,118.68,705.64,7.07,6.12;5,133.36,701.14,22.72,9.65;5,156.08,699.57,7.60,6.12;5,156.08,705.65,17.16,6.12;5,176.22,701.46,178.15,8.64">Given the synthesized instructions x raw m and the environment, an LLM-based agent is instructed to take actions and navigate the environment to generate diverse trajectories {x raw m , h m } M m=0 aimed at accomplishing the task instructions.</s><s coords="5,357.47,701.14,146.88,9.65;5,108.00,712.10,184.07,9.65;5,292.07,710.52,6.31,6.12;5,292.07,716.61,15.01,6.12;5,310.75,712.10,176.94,9.65;5,487.69,710.52,6.31,6.12;5,487.69,716.61,15.01,6.12;5,503.20,712.42,2.54,8.64">Here, h m represents the m-th history trajectory, which consists of N actions {a n } N n=1 and N + 1 environment observations {o n } N n=0 .</s><s coords="5,108.00,723.06,396.00,9.65;6,107.64,85.02,189.27,9.65">Due to the limited capabilities of current LLMs, the generated trajectories h m may not always align well with the synthesized task instructions x m .</s><s coords="6,299.99,85.34,204.01,8.64;6,108.00,95.98,213.78,9.65;6,321.78,94.40,3.70,6.12;6,321.78,100.48,7.07,6.12;6,329.35,96.30,2.49,8.64">To address this, we ask the LLM to summarize the completed trajectory h m and propose a refined goal x r m .</s><s coords="6,334.93,96.30,169.06,8.64;6,108.00,108.63,74.05,8.96;6,182.05,107.06,3.70,6.12;6,182.05,113.14,7.07,6.12;6,189.62,108.63,22.72,9.65;6,212.33,106.24,10.94,6.12;6,212.33,113.33,17.16,6.12;6,229.99,108.63,213.58,9.65">This process results in a set of synthesized demonstrations {x r m , h m } Mr m=0 , where M r is the number of refined task instructions.</s><s coords="6,108.00,125.50,120.25,8.96">Pairwise Data Construction.</s><s coords="6,231.33,125.89,272.67,8.64;6,108.00,136.85,331.23,8.64">To train a reward model capable of distinguishing between good and poor trajectories, we also need trajectories that do not satisfy the task instructions.</s><s coords="6,442.33,136.85,62.92,8.64;6,107.64,147.49,210.36,8.96;6,318.01,145.91,3.70,6.12;6,318.01,151.99,7.07,6.12;6,325.57,147.49,178.43,9.65;6,108.00,158.44,334.20,9.65;6,442.20,156.87,6.23,6.12;6,442.20,162.95,7.07,6.12;6,449.77,158.44,7.52,8.74">To create these, we sample additional trajectories that differ from {x r m , h m } and do not meet the task requirements by modifying actions in h m and generating corresponding negative trajectories {h - m }.</s><s coords="6,460.80,158.76,44.45,8.64;6,107.64,169.40,221.88,9.65;6,329.52,167.83,6.12,6.12;6,329.52,173.91,7.07,6.12;6,337.09,169.40,153.59,9.65;6,490.67,167.83,6.23,6.12;6,490.67,173.91,7.07,6.12;6,498.24,169.40,7.50,8.74">For clarity, we refer to the refined successful trajectories as {x m , h + m } and the unsuccessful ones as {x m , h - m }.</s><s coords="6,107.69,180.68,396.31,8.64;6,108.00,191.64,273.75,8.64">These paired data will be used to train the reward model described in Section 3.3, allowing it to estimate the reward value of any given trajectory in the environment.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" coords="6,108.25,216.76,137.91,8.64">REWARD MODEL DESIGN.</head><p><s coords="6,108.00,236.93,129.36,8.96">Reward Model Architectures.</s><s coords="6,241.74,237.32,262.26,8.64;6,108.00,248.28,366.86,8.64">Theoretically, we can adopt any vision-language model that can take a sequence of visual and text inputs as the backbone for the proposed reward model.</s><s coords="6,479.24,248.28,24.93,8.64;6,108.00,259.24,396.00,8.64;6,108.00,270.20,397.65,8.64;6,108.00,281.16,396.00,8.64;6,108.00,292.12,85.52,8.64">In our implementation, we use the recent VILA model <ref type="bibr" coords="6,294.25,259.24,64.80,8.64" target="#b33">(Lin et al., 2023)</ref> as the backbone for reward modeling since it has carefully maintained open-source code, shows strong performance on standard visionlanguage benchmarks like <ref type="bibr" coords="6,211.52,281.16,61.33,8.64" target="#b13">(Fu et al., 2023;</ref><ref type="bibr" coords="6,275.03,281.16,71.54,8.64" target="#b14">Goyal et al., 2017;</ref><ref type="bibr" coords="6,348.74,281.16,103.23,8.64" target="#b22">Hudson &amp; Manning, 2019)</ref>, and support multiple image input.</s></p><p><s coords="6,107.69,309.05,396.66,8.64;6,106.83,319.69,397.53,9.65;6,108.00,330.97,284.78,8.64">The goal of the reward model is to predict a reward score to estimate whether the given trajectory (x m , h m ) has satisfied the task instruction or not, which is different from the original goal of VILA models that generate a series of text tokens to respond to the task query.</s><s coords="6,395.88,330.97,108.12,8.64;6,108.00,341.93,396.00,8.64;6,108.00,352.89,113.18,8.64">To handle this problem, we additionally add a fully-connected layer for the model, which linearly maps the hidden state of the last layer into a scalar value.</s></p><p><s coords="6,108.00,369.44,93.98,8.96">Optimazation Target.</s><s coords="6,207.39,369.83,298.26,8.64;6,108.00,380.78,396.00,8.64">Given the pairwise data that is automatically synthesized from the environments in Section 3.2, we optimize the reward model by distinguishing the good trajectories</s></p><formula xml:id="formula_1" coords="6,106.83,389.85,140.43,12.19">(x m , h + m ) from bad ones (x m , h - m ).</formula><p><s coords="6,250.31,391.74,253.69,8.64;6,108.00,402.70,396.00,8.64;6,108.00,413.66,316.08,8.64">Following standard works of reinforcement learning from human feedback <ref type="bibr" coords="6,147.71,402.70,100.00,8.64" target="#b3">(Bradley &amp; Terry, 1952;</ref><ref type="bibr" coords="6,250.75,402.70,70.47,8.64">Sun et al., 2023b;</ref><ref type="bibr" coords="6,321.22,402.70,8.59,8.64">a)</ref>, we treat the optimization problem of the reward model as a binary classification problem and adopt a cross-entropy loss.</s><s coords="6,427.17,413.66,73.29,8.64">Formally, we have</s></p><formula xml:id="formula_2" coords="6,183.26,429.30,321.41,14.06">L(θ) = -E (xm,h + m ,h - m ) [log σ(R θ (x m , h + m ) -R θ (x m , h - m ))],<label>(2)</label></formula><p><s coords="6,107.64,451.22,396.70,8.96;6,108.00,462.50,395.99,8.64;6,108.00,473.46,214.71,8.64">where σ is the sigmoid function and θ are the learnable parameters in the reward model R. By optimizing this target, the reward model is trained to give higher value scores to the trajectories that are closer to the goal described in the task instruction.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4" coords="6,108.25,498.58,294.77,8.64">PLANNING WITH LARGE VISION-LANGAUGE REWARD MODEL.</head><p><s coords="6,107.64,519.14,396.61,8.64;6,108.00,530.10,397.65,8.64;6,108.00,541.06,52.83,8.64">After getting the reward model to estimate how well a sampled trajectory match the given task instruction, we are able to combine it with different planning algorithms to improve LLM agents' performance.</s><s coords="6,163.92,541.06,280.29,8.64">Here, we summarize the typical algorithms we can adopt in this paper.</s></p><p><s coords="6,108.00,557.61,42.50,8.96">Best of N.</s><s coords="6,153.27,558.00,350.74,8.64;6,108.00,568.96,115.19,8.64">This is a simple algorithm that we can adopt the learned reward model to improve the LLM agents' performances.</s><s coords="6,228.65,568.64,275.35,8.96;6,108.00,579.91,396.17,8.64;6,108.00,590.87,44.16,8.64">We first prompt the LLM agent to generate n different trajectories independently and choose the one with the highest predicted reward score as the prediction for evaluation.</s><s coords="6,155.25,590.87,348.74,8.64;6,108.00,601.83,396.00,8.64;6,108.00,612.79,87.99,8.64">Note that this simple method is previously used in natural language generation <ref type="bibr" coords="6,474.79,590.87,29.20,8.64;6,108.00,601.83,53.63,8.64">(Zhang et al., 2024b)</ref> and we adopt it in the context of agent tasks to study the effectiveness of the reward model for agent tasks.</s></p><p><s coords="6,108.00,629.34,43.19,8.96">Reflexion.</s><s coords="6,157.53,629.73,346.47,8.64;6,108.00,640.69,311.20,8.64">Reflexion <ref type="bibr" coords="6,200.48,629.73,81.29,8.64" target="#b49">(Shinn et al., 2024)</ref> is a planning framework that enables large language models (LLMs) to learn from trial-and-error without additional fine-tuning.</s><s coords="6,424.30,640.69,79.70,8.64;6,108.00,651.65,327.50,8.64">Instead of updating model weights, Reflexion agents use verbal feedback derived from task outcomes.</s><s coords="6,438.59,651.65,65.41,8.64;6,108.00,662.60,396.00,8.64;6,108.00,673.56,40.36,8.64">This feedback is converted into reflective summaries and stored in an episodic memory buffer, which informs future decisions.</s><s coords="6,152.45,673.56,353.21,8.64;6,108.00,684.52,396.00,8.64;6,108.00,695.48,109.84,8.64">Reflexion supports various feedback types and improves performance across decisionmaking, coding, and reasoning tasks by providing linguistic reinforcement that mimics human self-reflection and learning.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="6,108.00,712.03,31.90,8.96">MCTS.</head><p><s coords="6,143.49,712.42,360.51,8.64;6,107.67,723.38,292.86,8.64">We also consider tree search-based planning algorithms like Monte Carlo Tree Search (MCTS) <ref type="bibr" coords="6,145.26,723.38,64.97,8.64" target="#b9">(Coulom, 2006;</ref><ref type="bibr" coords="6,213.39,723.38,77.05,8.64" target="#b52">Silver et al., 2017)</ref> to find the optimal policy.</s><s coords="6,405.62,723.38,98.39,8.64;7,108.00,85.34,397.75,8.64">There is a tree structure constructed by the algorithm, where each node represents a state and each edge signifies an action.</s></p><p><s coords="7,108.00,96.30,396.00,8.64;7,108.00,107.26,328.54,8.64">Beginning at the initial state of the root node, the algorithm navigates the state space to identify action and state trajectories with high rewards, as predicted by our learned reward model.</s></p><p><s coords="7,107.69,124.19,396.31,8.64;7,108.00,134.83,270.90,8.96">The algorithm tracks 1) the frequency of visits to each node and 2) a value function that records the maximum predicted reward obtained from taking action a in state s.</s><s coords="7,382.02,135.15,121.98,8.64;7,108.00,146.11,396.00,8.64;7,108.00,157.07,149.15,8.64">MCTS would visit and expand nodes with either higher values (as they lead to high predicted reward trajectory) or with smaller visit numbers (as they are under-explored).</s><s coords="7,260.26,157.07,243.74,8.64;7,108.00,168.03,69.73,8.64">We provide more details in the implementation details and the appendix section.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" coords="7,108.30,198.70,91.78,10.37">EXPERIMENTS</head><p><s coords="7,108.00,225.51,396.00,8.64;7,108.00,236.47,111.96,8.64">In this section, we conduct a series of experiments to demonstrate the effectiveness of the proposed framework for agent tasks.</s><s coords="7,226.58,236.47,277.42,8.64;7,108.00,247.43,372.45,8.64">First, we evaluate the framework's performance on standard agent benchmarks <ref type="bibr" coords="7,158.59,247.43,72.82,8.64">(Yao et al., 2023a;</ref><ref type="bibr" coords="7,233.91,247.43,76.90,8.64">Wang et al., 2022a;</ref><ref type="bibr" coords="7,313.31,247.43,68.73,8.64">Yao et al., 2023b)</ref>, detailed in Section 4.2.</s><s coords="7,483.56,247.43,21.69,8.64;7,107.64,258.39,396.36,8.64;7,108.00,269.34,160.98,8.64">Next, we show how customizing the reward target during inference allows us to generate more tailored action plans, as described in Section 4.3.</s><s coords="7,272.07,269.34,201.90,8.64">Finally, we conduct ablation studies in Section 4.4.</s><s coords="7,477.06,269.34,26.95,8.64;7,108.00,280.30,352.62,8.64">Before delving into the experimental results, we provide an overview of our experimental setup.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" coords="7,108.25,308.39,121.32,8.64">EXPERIMENTAL SETUP</head><p><s coords="7,108.00,329.76,62.25,8.96">Environments.</s><s coords="7,180.21,330.14,274.94,8.64">We evaluate the ARMAP framework in three different environments:</s></p><p><s coords="7,135.40,348.06,368.61,9.03;7,143.87,359.40,316.44,8.64">• Webshop is a well-known environment for online shopping <ref type="bibr" coords="7,385.73,348.45,72.14,8.64">(Yao et al., 2023a)</ref>, where the agent must search for and select products on the website to obtain a final result.</s><s coords="7,463.40,359.40,40.61,8.64;7,143.87,370.36,360.14,8.64;7,143.62,381.32,299.31,8.64">Following the setup of AgentBench <ref type="bibr" coords="7,247.49,370.36,68.84,8.64" target="#b35">(Liu et al., 2023)</ref> for LLM evaluation, we test the model on the validation split, using the default matching reward as the evaluation metric.</s></p><p><s coords="7,135.40,398.61,368.60,9.03;7,143.87,409.96,84.94,8.64">• ScienceWorld <ref type="bibr" coords="7,206.57,399.00,79.28,8.64">(Wang et al., 2022a</ref>) is an interactive benchmark designed for embodied science experiments.</s><s coords="7,233.34,409.96,271.01,8.64;7,143.87,420.92,360.13,8.64;7,143.87,431.88,128.73,8.64">It places agents in a simulated text-based environment where they must perform elementary science experiments by navigating the environment, manipulating objects, and observing outcomes.</s><s coords="7,275.52,431.88,228.47,8.64;7,143.87,442.84,250.76,8.64">The aim is to assess whether AI models can apply scientific knowledge, rather than merely retrieve or assemble information.</s><s coords="7,397.70,442.84,106.55,8.64;7,143.87,453.80,123.42,8.64">We evaluate the framework on both seen and unseen splits.</s></p><p><s coords="7,135.40,471.09,368.60,9.03;7,143.87,482.44,360.31,8.64;7,143.87,493.40,12.20,8.64">• Game of 24 is a mathematical game where the agent is given four numbers and must use arithmetic operations (addition, subtraction, multiplication, and division) to make the number 24.</s><s coords="7,158.99,493.08,346.75,8.96">For instance, given the input '3, 5, 7, 11,' one possible solution is '(7-3) * (11-5) = 24'.</s><s coords="7,143.87,504.36,360.13,8.64;7,143.87,515.32,361.87,8.64">Following <ref type="bibr" coords="7,185.66,504.36,67.14,8.64">Yao et al. (2023b)</ref>, we selected 100 challenging puzzles, specifically those indexed from 901 to 1,000, and the performance metric is the success rate across these puzzles.</s></p><p><s coords="7,143.51,526.27,361.74,8.64;7,143.87,537.23,337.20,8.64">As shown in Fig. <ref type="figure" coords="7,217.67,526.27,5.08,8.64">7</ref> of the Appendix, we use the chain-of-thought prompting technique, prompting the LLM agents to output intermediate steps followed by the final answer.</s><s coords="7,484.14,537.23,19.86,8.64;7,143.87,548.19,172.11,8.64">Each step of the solution is considered an action.</s></p><p><s coords="7,108.00,574.54,53.81,8.96">LLM Setup.</s><s coords="7,171.77,574.93,332.47,8.64;7,108.00,585.89,358.98,8.64">Our framework requires LLM models to act as agents, generating synthetic task instructions from the environment along with few-shot examples in the prompt context.</s><s coords="7,471.39,585.89,32.61,8.64;7,108.00,596.85,396.00,8.64;7,108.00,607.81,80.31,8.64">We also deploy agents to perform these synthetic tasks in the environment, collecting diverse trajectories for further analysis.</s><s coords="7,192.23,607.81,313.02,8.64;7,108.00,618.77,396.35,8.64;7,108.00,629.73,159.74,8.64">In this paper, we primarily use the Llama3-70b-instruct model <ref type="bibr" coords="7,449.63,607.81,55.62,8.64;7,108.00,618.77,22.78,8.64" target="#b12">(Dubey et al., 2024)</ref> to synthesize training data for the automatic reward models, as it is open-source, easy to deploy locally, and delivers robust performance.</s><s coords="7,270.86,629.73,233.31,8.64;7,108.00,640.69,396.00,8.64;7,108.00,651.65,249.17,8.64">We avoid state-of-the-art commercial models like GPT-4 or Gemini due to their high costs and the complexity of reproducing results caused by frequent model updates, making them less suitable for our research objectives.</s></p><p><s coords="7,107.69,668.58,396.31,8.64;7,108.00,679.54,264.01,8.64">To evaluate the performance of various LLM agents, we serve a representative set of LLM APIs locally, balancing model diversity with affordable serving costs.</s><s coords="7,377.01,679.54,127.16,8.64;7,108.00,690.50,88.82,8.64">We identify the LLMs by their model family and size.</s><s coords="7,199.88,690.50,269.68,8.64">Specifically, these are Llama70B, Llama8B, Mistral7B, and Phi3.8B.</s><s coords="7,472.00,690.50,32.00,8.64;7,108.00,701.46,396.00,8.64;7,108.00,712.42,84.81,8.64">We note that these open-source model families are frequently updated, and we provide the current model links in the Appendix A.3.</s><s coords="7,195.89,712.42,308.78,8.64;7,108.00,723.38,97.12,8.64">All models can be easily set up using the vLLM library <ref type="bibr" coords="7,421.09,712.42,78.82,8.64">(Kwon et al., 2023b</ref>) and a single H100 GPU.</s></p><p><s coords="8,108.00,84.95,41.88,8.96">Baselines.</s><s coords="8,159.84,85.34,344.16,8.64;8,108.00,95.91,397.25,9.03;8,108.00,107.26,50.60,8.64">We implement our ARMAP framework using different planning algorithms, including Reflexion, Best-of-N, and MCTS, which we denote as ARMAP-R, ARMAP-B, and ARMAP-M, respectively.</s><s coords="8,162.42,107.26,341.58,8.64;8,108.00,118.22,396.00,8.64;8,108.00,129.17,158.00,8.64">We limit the maximum number of trajectories our ARMAP can explore to 10 in the ScienceWorld and Webshop environments to systematically evaluate the pipeline's effectiveness across different LLM agent backbones.</s><s coords="8,269.11,129.17,234.89,8.64;8,108.00,139.74,205.56,9.03">We also compare the model with two baselines that do not use reward model guidance: Sampling and Greedy.</s><s coords="8,316.66,140.13,187.33,8.64;8,108.00,151.09,396.00,8.64;8,108.00,162.05,29.14,8.64">For the Game of 24 environment, we follow the setup of a previous study <ref type="bibr" coords="8,210.99,151.09,74.84,8.64">(Yao et al., 2023b)</ref> and set the maximum number of explored trajectories to 100.</s><s coords="8,143.14,161.66,360.86,9.03;8,108.00,173.01,185.43,8.64">For Sampling, we set the model temperature to 1 and sample action trajectories using chain-of-thought prompting <ref type="bibr" coords="8,221.65,173.01,67.49,8.64" target="#b61">(Wei et al., 2023)</ref>.</s><s coords="8,296.52,172.62,207.47,9.03;8,108.00,183.97,192.45,8.64">For Greedy, we set the temperature to 0, generating the action sequence with the highest probability.</s><s coords="8,303.57,183.97,200.43,8.64;8,107.64,194.93,41.78,8.64">Further implementation details are provided in the Appendix.</s><s coords="8,152.51,194.93,336.73,8.64">We will release all the code, model, and data for easy reproduction upon acceptance.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" coords="8,108.25,223.10,203.15,8.64">EFFECTIVENESS FOR REWARD PLANNING.</head><p><s coords="8,108.00,244.88,397.65,8.64;8,108.00,255.84,377.39,8.64">In this section, we investigate the effectiveness of the framework across different language models <ref type="bibr" coords="8,121.58,255.84,79.40,8.64" target="#b12">(Dubey et al., 2024;</ref><ref type="bibr" coords="8,203.46,255.84,70.69,8.64" target="#b23">Jiang et al., 2023;</ref><ref type="bibr" coords="8,276.63,255.84,75.11,8.64" target="#b0">Abdin et al., 2024)</ref> and various planning algorithms.</s><s coords="8,488.47,255.84,15.52,8.64;8,108.00,266.80,118.36,8.64">The results are shown in Table <ref type="table" coords="8,218.74,266.80,3.81,8.64" target="#tab_1">1</ref>.</s><s coords="8,231.15,266.80,230.32,8.64">Based on the table, we have the following observations.</s><s coords="8,466.25,266.80,37.92,8.64;8,108.00,277.37,396.00,9.03;8,108.00,288.72,144.21,8.64">First, our proposed pipeline is effective, as it consistently outperforms the Sampling and Greedy baselines across different planning algorithms.</s><s coords="8,255.34,288.72,248.66,8.64;8,108.00,299.68,397.25,8.64;8,108.00,310.64,285.88,8.64">Additionally, we observe that the average improvement is more significant on weaker models, such as Phi <ref type="bibr" coords="8,280.05,299.68,79.86,8.64" target="#b0">(Abdin et al., 2024)</ref> and Mistral-7B <ref type="bibr" coords="8,426.80,299.68,74.08,8.64" target="#b23">(Jiang et al., 2023)</ref>, compared to stronger models like Llama3-1-70B <ref type="bibr" coords="8,309.57,310.64,79.94,8.64" target="#b12">(Dubey et al., 2024)</ref>.</s><s coords="8,397.48,310.64,106.52,8.64;8,107.64,321.60,396.35,8.64;8,108.00,332.56,125.87,8.64">We believe this is because weaker models explore more low-reward trajectories, providing greater opportunities for the reward model to improve performance.</s></p><p><s coords="8,107.64,349.49,396.52,8.64;8,108.00,360.45,379.77,8.64">Among the three planning algorithms, MCTS performs the best on average, likely due to its superior mechanisms for identifying higher-reward trajectories and searching less-explored trajectories.</s><s coords="8,490.86,360.45,13.14,8.64;8,108.00,371.41,379.84,8.64">We also notice that Reflexion performs the worst on weaker models like Mistral7B and Phi3.8B.</s><s coords="8,490.70,371.41,13.30,8.64;8,108.00,382.37,396.00,8.64;8,108.00,393.33,310.41,8.64">We suspect this is because Reflexion was designed for ChatGPT-family-based agents and requires the LLM agent to possess strong capabilities for learning from trial and error.</s><s coords="8,424.71,393.33,79.28,8.64;8,108.00,404.29,396.17,8.64;8,108.00,415.25,324.68,8.64">Finally, we present qualitative results of different methods in Fig. <ref type="figure" coords="8,293.56,404.29,3.75,8.64" target="#fig_3">3</ref>, where it is clear that our ARMAP generates better trajectories than the baselines, aided by the guidance of automatic reward models.</s><s coords="8,435.77,415.25,69.48,8.64;8,107.64,426.20,396.36,8.64;8,108.00,437.16,267.48,8.64">In Appendix A.5, we analyze several failure cases, offer more detailed insights into the limitations of the current approach, and suggest potential improvements in reward modeling.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" coords="8,108.25,465.33,155.94,8.64">CONTROLLABLE GENERATION.</head><p><s coords="8,107.64,487.12,397.60,8.64;8,108.00,498.08,396.00,8.64;8,108.00,509.04,73.37,8.64">Another benefit of our ARMAP pipeline is that we can customize our reward targets during inference, allowing us to generate more controllable action sequences, rather than solely maximizing the predicted rewards.</s><s coords="8,184.46,509.04,319.54,8.64;8,108.00,520.00,325.68,8.64">Agent fine-tuning methods <ref type="bibr" coords="8,293.94,509.04,61.80,8.64" target="#b32">(Li et al., 2023;</ref><ref type="bibr" coords="8,358.23,509.04,70.66,8.64" target="#b70">Zeng et al., 2023)</ref> find it challenging to achieve this goal since agent behaviors are typically fixed during inference.</s><s coords="8,445.50,520.00,58.50,8.64;8,108.00,530.95,397.74,8.64">We conducted experiments in the Webshop environment to evaluate the impact of customizable reward targets.</s></p><p><s coords="8,108.00,541.59,396.00,8.96;8,108.00,552.87,124.89,8.64">In addition to the original objective of maximizing the predicted reward R(x, h), we defined two additional optimization targets.</s><s coords="8,235.98,552.87,268.36,8.64;8,108.00,563.51,271.13,8.96">First, we aimed to minimize the number of actions in the trajectory history, defining the reward target as R(x, h) -NumberOfAction(h).</s><s coords="8,382.11,563.83,121.89,8.64;8,108.00,574.47,362.30,8.96">Second, we sought to minimize the price of the target product, with a customized target of R(x, h) -PriceOfProduct(h).</s><s coords="8,473.91,574.79,30.09,8.64;8,108.00,585.75,76.24,8.64">Table <ref type="table" coords="8,498.92,574.79,5.08,8.64" target="#tab_2">2</ref> presents the results.</s><s coords="8,187.24,585.75,316.76,8.64;8,108.00,596.71,396.00,8.64;8,108.00,607.67,234.99,8.64">By applying a length penalty on the reward target for ARMAP-M, we reduced the average action length from 4.5 to 4 and the average product price from 97.9 to 69.0, while maintaining comparable performance on the default matching reward.</s><s coords="8,347.26,607.67,156.91,8.64;8,107.64,618.63,48.84,8.64">Similar performance was observed for ARMAP-B.</s><s coords="8,159.19,618.63,344.81,8.64;8,108.00,629.58,396.00,8.64;8,108.00,640.54,219.56,8.64">Additionally, we provide a qualitative example in Fig. <ref type="figure" coords="8,382.24,618.63,3.81,8.64" target="#fig_5">4</ref>. From this example, we can see that our customized reward target successfully guided the LLM agent to purchase products with fewer action steps while still finding the target product.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4" coords="8,108.25,668.71,110.31,8.64">ABLATION STUDIES.</head><p><s coords="8,107.53,690.50,312.21,8.64">We conduct ablation studies to investigate the effectiveness of the framework.</s><s coords="8,422.82,690.50,81.18,8.64;8,108.00,701.07,156.69,9.03">Specifically, we aim to answer the following questions: Q1.</s><s coords="8,267.77,701.46,236.23,8.64;8,108.00,712.42,254.97,8.64">Can we train a policy model with fully supervised learning to handle multi-step tasks from the synthesized trajectory data?</s><s coords="8,366.05,712.03,15.44,8.96">Q2.</s><s coords="8,384.57,712.42,119.43,8.64;8,108.00,723.38,367.56,8.64">Can a large, general language model be used as the reward model to perform guidance without automatic reward learning?</s><s coords="9,107.53,568.96,398.21,8.64">We conducted experiments using the ScienceWorld benchmark, and the results are shown in Table <ref type="table" coords="9,498.33,568.96,3.71,8.64" target="#tab_5">3</ref>.</s><s coords="9,107.53,579.91,397.71,8.64;9,107.64,590.87,396.35,8.64;9,108.00,601.83,375.54,8.64">When comparing our pipeline to the SFT model trained using our reward backbone VILA3B, we observed that although the policy model trained through fully supervised learning performed reasonably well (18.6), it still lagged behind the performance of our planning framework (28.3).</s><s coords="9,486.62,601.83,17.38,8.64;9,108.00,612.79,396.00,8.64;9,108.00,623.36,271.89,9.03">This suggests that learning a policy model is more challenging than learning a reward model, highlighting the effectiveness of our proposed ARMAP pipeline (answering Q1).</s></p><p><s coords="9,108.00,640.69,397.24,8.64;9,108.00,651.65,318.86,8.64">Next, we replaced our smaller 3B reward model with a much larger language model, Llama3-1-70B, and used few-shot prompting to predict the reward of the extracted trajectories.</s><s coords="9,429.95,651.65,74.05,8.64;9,108.00,662.60,396.00,8.64;9,108.00,673.56,164.27,8.64">We found that this larger model also improved performance compared to the default greedy model, demonstrating the effectiveness of our planning framework.</s><s coords="9,275.39,673.56,228.61,8.64;9,108.00,684.52,396.00,8.64;9,108.00,695.09,257.62,9.03">However, it still performed worse than our pipeline using automatic reward learning, despite the Llama3-1-70B being about 20 times larger, further showcasing the efficiency and effectiveness of our approach (answering Q2).</s></p><p><s coords="9,107.53,712.42,396.46,8.64;9,107.75,723.38,273.48,8.64">We provide additional ablation experiments in the Appendix A.2, including the data quality from various LLMs, reward modeling target and computational efficiency.</s><s coords="10,169.05,88.46,174.84,8.27;10,169.05,98.35,184.04,8.28">I'm looking for furniture to make my living room and dinning room so nice, and price lower than 300 dollars.</s><s coords="10,164.00,209.06,196.43,8.27">I need a blue women coat, and price lower than 80 dollars.</s><s coords="10,108.00,340.96,396.00,8.64;10,108.00,351.92,396.00,8.64;10,108.00,362.88,95.11,8.64">In the top example, when the search results do not meet the requirements, our ARMAP method leverages the advantage of the tree structure to backtrack and search again, thereby retrieving the appropriate target item.</s><s coords="10,206.63,362.88,297.37,8.64;10,108.00,373.84,25.78,8.64">In contrast, existing methods fail to backtrack when the target item is not found.</s><s coords="10,136.87,373.84,368.38,8.64;10,108.00,384.80,396.00,8.64;10,107.64,395.75,351.70,8.64">In the bottom example, by using the ARMAP to evaluate different states in the environment, our method is able to select the color that offers a higher reward and better meets the requirements when choosing between size and color, rather than mistakenly selecting the wrong size.</s><s coords="10,462.44,395.75,41.56,8.64;10,108.00,406.71,397.73,8.64">These two examples sufficiently demonstrate the advantages of our method compared to traditional approaches.</s><s coords="10,144.45,632.08,359.55,8.64;10,108.00,643.04,74.10,8.64">In the middle, we show our default reward can guide the LLM agent to generate a correct but long trajectory.</s><s coords="10,185.19,643.04,318.98,8.64;10,108.00,654.00,301.67,8.64">On the right, we show our framework with a customized reward target for shorter trajectories, which finds a correct and short trajectory for the target product.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="10,207.93,115.25,48.95,8.27">Back to Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" coords="10,108.30,686.37,87.08,10.37">CONCLUSION</head><p><s coords="10,107.53,712.42,396.46,8.64;10,108.00,723.38,396.00,8.64;17,108.00,514.77,151.62,8.96">We propose a framework, ARMAP, for large language model (LLM) agents to manage tasks that require multi-step decision-making and environmental feedback, such as online shopping or scientific Computational Efficiency Analysis.</s><s coords="17,262.73,515.16,243.02,8.64">We further study the data demands of the reward modelings.</s><s coords="17,107.53,526.12,284.30,8.64">We show the performance of using different amounts of training data.</s><s coords="17,395.49,526.12,108.51,8.64;17,108.00,537.07,286.17,8.64">In Table <ref type="table" coords="17,431.65,526.12,5.08,8.64">8</ref> and Table <ref type="table" coords="17,481.80,526.12,3.81,8.64">9</ref>, we selected ScienceWorld and used ARMAP-B as the experimental setting.</s><s coords="17,397.26,537.07,106.74,8.64;17,108.00,548.03,176.85,8.64">In the leftmost column, we listed the different LLMs used in our study.</s><s coords="17,288.30,548.03,216.95,8.64;17,108.00,558.99,396.00,8.64;17,108.00,569.95,40.84,8.64">In the first row, we introduced VILA-3B, VILA-13B, and LLaVA-13B, to compare the impact of different sizes and types of reward models on the final outcomes.</s><s coords="17,151.95,569.95,352.05,8.64;17,108.00,580.91,397.75,8.64">In the last two columns, we trained the reward models using 1/5 and 1/25 of the original training dataset size, respectively, to assess how varying amounts of training data affect our method.</s></p><p><s coords="17,107.67,591.87,396.33,8.64;17,108.00,602.83,21.53,8.64">(1) As seen, the effectiveness of our method continues to improve with increasing reward model sizes.</s><s coords="17,132.61,602.83,238.32,8.64">However, in the experiments with LLaMA-8B and Phi-3.8B,</s><s coords="17,373.42,602.83,130.58,8.64;17,108.00,613.79,188.83,8.64">despite using more potent reward models, there was no improvement in results.</s><s coords="17,302.28,613.79,201.71,8.64;17,108.00,624.75,285.24,8.64">We believe that in the processes of planning and reasoning, the capability of the policy model still plays a dominant role.</s><s coords="17,396.33,624.75,107.67,8.64;17,108.00,635.71,396.35,8.64;17,108.00,646.66,89.67,8.64">If the policy model is more robust, and concurrently, if we enhance the capability of the reward model, we can continuously achieve better results.</s></p><p><s coords="17,203.51,646.66,300.49,8.64;17,108.00,657.62,58.43,8.64">(2) We also observe that the performance of LLaVA-13B is not as good as VILA-13B.</s><s coords="17,169.28,657.62,334.72,8.64;17,108.00,668.58,397.24,8.64;17,108.00,679.54,151.87,8.64">We attribute this to VILA being an improved version of LLaVA, and it utilizes an interleaved image-text dataset in its training, which better aids the model in perceiving, understanding, and handling multimodal information.</s><s coords="17,262.96,679.54,139.72,8.64">Hence, VILA outperforms LLaVA.</s><s coords="17,405.16,679.54,98.84,8.64;17,107.69,690.50,396.31,8.64;17,108.00,701.46,156.14,8.64">(3) From the Table <ref type="table" coords="17,482.19,679.54,4.97,8.64">8</ref> and Table <ref type="table" coords="17,132.08,690.50,3.74,8.64">9</ref>, it is evident that regardless of whether the data is seen or unseen, increasing the model size improves the final experimental results.</s><s coords="17,267.24,701.46,237.01,8.64;17,108.00,712.42,396.00,8.64;17,108.00,723.38,114.29,8.64">If we use the results of the VILA-3B model as a benchmark and compare it with the two settings, 1/5 data and 1/25 data, it is clear that increasing the training data enhances the outcomes.</s><s coords="17,225.39,723.38,278.60,8.64;18,108.00,85.34,396.00,8.64;18,108.00,96.30,89.07,8.64">Conversely, even when using extremely limited data amounts like 1/5 or 1/25 of the original dataset, we can still achieve a capable model, and the performance does not dramatically decrease.</s></p><p><s coords="18,107.69,113.23,398.05,8.64">These results demonstrate that our method can still yield good results in a low-resource environment.</s></p><p><s coords="18,108.00,124.19,396.00,8.64;18,108.00,135.15,390.87,8.64">In other words, our approach does not rely on large volumes of data and the strong capability of large models; it is succinct and efficient, capable of performing well in extremely low-resource settings.</s><s coords="18,108.00,522.19,120.65,8.96">Overhead in Data Synthesis.</s><s coords="18,231.74,522.58,272.26,8.64;18,108.00,533.54,105.84,8.64">We calculate the tokens we have used for task instruction generation and trajectory exploration.</s><s coords="18,216.92,533.54,175.22,8.64">We summarize these overheads in Table <ref type="table" coords="18,379.62,533.54,8.35,8.64" target="#tab_9">11</ref>.</s><s coords="18,395.23,533.54,108.77,8.64;18,108.00,544.50,337.84,8.64">To provide a more intuitive comparison, we first calculated the average tokens per sample for these different tasks.</s><s coords="18,448.92,544.50,55.08,8.64;18,108.00,555.45,396.00,8.64;18,108.00,566.41,136.89,8.64">We found that although Game of 24 overall consumes the most tokens, the average number of tokens spent per Game of 24 sample is relatively the least.</s><s coords="18,247.98,566.41,256.02,8.64;18,108.00,577.37,177.94,8.64">In contrast, Webshop has the fewest total samples but the highest average number of tokens spent per sample.</s><s coords="18,289.06,577.37,167.00,8.64">ScienceWorld falls in between these two.</s><s coords="18,459.17,577.37,44.82,8.64;18,107.53,588.33,396.46,8.64;18,108.00,599.29,359.16,8.64">The reason Webshop has a higher average number of tokens compared to Game of 24 is that the environment required for Webshop is more complex, involving more diverse elements and possibilities.</s><s coords="18,108.00,712.03,296.88,8.96">Proprietary Models as Training Data Generators and Policy Models.</s><s coords="18,408.20,712.42,95.80,8.64;18,108.00,723.38,384.51,8.64">In the main content, we mainly consider using open-source models to act as training data generators and policy models.</s><s coords="18,495.60,723.38,8.41,8.64;19,108.00,85.34,395.99,8.64;19,108.00,96.30,158.67,8.64">In order to investigate the upper bounds of our proposed method, we also conduct some experiments by using powerful proprietary models.</s><s coords="19,272.04,96.30,233.61,8.64;19,108.00,107.26,396.00,8.64;19,108.00,118.22,62.17,8.64">However, to serve as the training data generator, closedsource models have several drawbacks, including high costs, limited commercial access, and lack of reproducibility.</s><s coords="19,173.27,118.22,330.73,8.64;19,108.00,129.17,31.80,8.64">In contrast, our approach achieves strong results without relying on closed-source models.</s><s coords="19,142.88,129.17,361.11,8.64;19,108.00,140.13,232.85,8.64">Given the expense associated with API-based models like GPT-4o for generating training datasets, we have opted not to pursue this method for now.</s></p><p><s coords="19,108.00,157.07,396.00,8.64;19,108.00,168.03,310.13,8.64">For API-based proprietary models serving as policy models, the high cost of GPT-4o and API access rate limitations prompted us to focus our experiments primarily on ALFWorld.</s><s coords="19,421.22,168.03,82.78,8.64;19,108.00,178.99,396.00,8.64;19,108.00,189.95,191.26,8.64">Specifically, we used GPT-4o-2024-08-06 to sample five trajectories each on ALFWorld's Dev and Std sets, then conducted experiments using our automatic reward model.</s><s coords="19,302.34,189.95,201.66,8.64;19,108.00,200.91,397.74,8.64">As shown in Table <ref type="table" coords="19,379.28,189.95,8.37,8.64" target="#tab_10">12</ref>, our reward model is able to help the powerful GPT-4o gain better performance, demonstrating the effectiveness of our framework.</s><s coords="19,108.00,356.59,125.06,8.96">Large Pretrain Model Setup.</s><s coords="19,243.03,356.98,260.98,8.64;19,108.00,367.94,158.85,8.64">We serve a diverse set of open-source LLM APIs to evaluate the effectiveness of the proposed pipeline.</s><s coords="19,272.25,367.94,231.75,8.64;19,108.00,378.90,100.75,8.64">We list all the open-source models and their weights on huggingface in table <ref type="table" coords="19,196.05,378.90,8.47,8.64" target="#tab_5">13</ref>.</s><s coords="19,214.64,378.90,289.36,8.64;19,108.00,389.86,116.72,8.64">All these models can be easily setup and reproduced with the VLLM libarary <ref type="bibr" coords="19,140.94,389.86,79.46,8.64">(Kwon et al., 2023b)</ref>.</s><s coords="19,227.81,389.86,276.19,8.64;19,108.00,400.81,45.94,8.64">We prove the effectiveness of our ARMAP framework across different LLM APIs.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="19,247.94,235.33,31.18,8.64">GPT-4o</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="19,115.96,424.38,37.48,8.64">Acronym</head><p><s coords="19,169.98,424.38,220.15,8.64;19,115.96,440.55,42.06,8.64">Model description and weight on huggingface websites Llama70B <ref type="url" coords="19,169.98,440.55,326.06,8.64" target="https://huggingface.co/hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4">https://huggingface.co/hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4</ref></s><s coords="19,115.96,451.50,37.08,8.64">Llama8B <ref type="url" coords="19,169.98,451.50,252.80,8.64" target="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct">https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct</ref></s><s coords="19,115.96,462.46,40.41,8.64">Mistral7B <ref type="url" coords="19,169.98,462.46,227.37,8.64" target="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3</ref></s><s coords="19,115.96,473.42,32.39,8.64;19,115.96,489.59,35.42,8.64">Phi3.8B <ref type="url" coords="19,169.98,473.42,216.31,8.64" target="https://huggingface.co/microsoft/Phi-3.5-mini-instruct">https://huggingface.co/microsoft/Phi-3.5-mini-instruct</ref> VILA3B <ref type="url" coords="19,169.98,489.59,233.55,8.64" target="https://huggingface.co/Efficient-Large-Model/VILA1.5-3b">https://huggingface.co/Efficient-Large-Model/VILA1.5-3b</ref></s></p><p><s coords="19,107.69,513.18,397.91,8.64">Table <ref type="table" coords="19,131.71,513.18,8.35,8.64" target="#tab_5">13</ref>: Agent models, the reward model, and their associated description on huggingface websites.</s></p><p><s coords="19,108.00,550.74,85.82,8.96">Environment Setup.</s><s coords="19,203.78,551.13,300.22,8.64;19,107.64,562.09,396.36,8.64;19,108.00,573.04,51.04,8.64">We build our environments based on the environment setup of the previous works <ref type="bibr" coords="19,135.87,562.09,70.88,8.64" target="#b35">(Liu et al., 2023;</ref><ref type="bibr" coords="19,210.24,562.09,74.27,8.64" target="#b54">Song et al., 2024;</ref><ref type="bibr" coords="19,288.00,562.09,74.38,8.64">Yao et al., 2023b;</ref><ref type="bibr" coords="19,365.88,562.09,88.38,8.64" target="#b51">Shridhar et al., 2021;</ref><ref type="bibr" coords="19,457.75,562.09,46.26,8.64;19,108.00,573.04,46.67,8.64" target="#b47">Schmidgall et al., 2024)</ref>.</s><s coords="19,162.22,573.04,341.77,8.64;19,107.64,584.00,396.36,8.64;19,108.00,594.96,66.95,8.64">For Webshop and ALFWorld environment, we start these docker environments from AgentBench <ref type="bibr" coords="19,160.95,584.00,68.48,8.64" target="#b35">(Liu et al., 2023)</ref> and implement different planning algorithms, Reflexion, Best-of-N and MCTS on it.</s><s coords="19,178.07,594.96,325.93,8.64;19,108.00,605.92,344.88,8.64">Similarly, we build our ScienceWorld, Game of 24 and AgentClinic environments from <ref type="bibr" coords="19,132.35,605.92,69.73,8.64" target="#b54">Song et al. (2024)</ref>, <ref type="bibr" coords="19,211.21,605.92,71.49,8.64">Yao et al. (2023b)</ref> and <ref type="bibr" coords="19,302.07,605.92,94.59,8.64" target="#b47">Schmidgall et al. (2024)</ref>, respectively.</s></p><p><s coords="19,108.00,629.34,121.66,8.96">Planning Algorithm Details.</s><s coords="19,239.62,629.73,264.73,8.64;19,108.00,640.69,216.25,8.64">We compare the performance of different planning algorithms by limiting their maximum explored trajectory number.</s><s coords="19,329.97,640.69,174.03,8.64;19,107.53,651.65,306.18,8.64">We set the maximum number to be 10 on Webshop and ScieneWorld in consideration of effectiveness and efficiency.</s><s coords="19,417.55,651.65,86.45,8.64;19,108.00,662.60,286.76,8.64">We set the maximum number to be 100 on Game of 24 following the setup of <ref type="bibr" coords="19,323.80,662.60,66.79,8.64">Yao et al. (2023b)</ref>.</s><s coords="19,397.66,662.60,107.58,8.64;19,107.64,673.56,396.71,8.64;19,108.00,684.52,200.84,8.64">In Webshop, ScienceWorld, ALFWorld and AgentClinic benchmarks, we only consider the top 10 available actions suggested by the LLM agent at each state to reduce search space.</s><s coords="19,311.90,684.52,192.27,8.64;19,108.00,695.48,105.90,8.64">We also set a trajectory's maximal action number length to 10 for simplicity.</s></p><p><s coords="19,108.00,712.42,273.63,8.64">For Reflexion, we set the maximum trial number to be 10 for all tasks.</s><s coords="19,384.67,712.42,120.57,8.64;19,107.64,723.38,173.59,8.64">For different tasks and models, we set the threshold of Reflexion separately.</s><s coords="19,284.34,723.38,219.67,8.64;20,108.00,85.34,395.99,8.64;20,108.00,96.30,40.39,8.64">During the iteration process, if the reward of the current trail's trajectory exceeds the threshold, the iteration will stop, and the current trail will be taken as the result.</s><s coords="20,152.59,96.30,351.41,8.64;20,107.53,107.26,356.35,8.64">If the maximum number of trials is reached, the last trial will be taken as the result in Webshop and Game of 24, while the first trial will be taken as the result in ScienceWorld.</s></p><p><s coords="20,108.00,130.53,76.44,8.96">Data Generation.</s><s coords="20,194.40,130.92,310.85,8.64;20,108.00,141.88,185.84,8.64">In total, we generate 2,436, 4,064 and 37,885 pairs of data for Webshop, ScienceWorld and Game of 24, respectively.</s><s coords="20,300.31,141.88,203.68,8.64;20,108.00,152.84,97.41,8.64">Sampled synthesized data sample can be seen in Fig. <ref type="figure" coords="20,126.42,152.84,3.77,8.64">5</ref>, Fig. <ref type="figure" coords="20,154.89,152.84,5.03,8.64">6</ref> and Fig. <ref type="figure" coords="20,197.87,152.84,3.77,8.64">7</ref>.</s><s coords="20,208.50,152.84,295.50,8.64;20,108.00,163.79,30.20,8.64">We provide the sampled prompt we use for data generation from Fig. <ref type="figure" coords="20,488.65,152.84,5.03,8.64">8</ref> to Fig. <ref type="figure" coords="20,125.98,163.79,8.14,8.64">11</ref>.</s><s coords="20,141.31,163.79,362.86,8.64;20,108.00,174.75,56.95,8.64">In Fig. <ref type="figure" coords="20,169.92,163.79,3.67,8.64">8</ref>, we show an example how we prompt the LLM to generate language instruction for ScienceWorld.</s><s coords="20,167.95,174.75,337.79,8.64">In Fig. <ref type="figure" coords="20,195.97,174.75,3.66,8.64" target="#fig_6">9</ref>, we show how we refine the language instruction to refine the instruction goal.</s><s coords="20,108.00,185.71,396.00,8.64;20,108.00,196.67,98.83,8.64">In Fig. <ref type="figure" coords="20,137.65,185.71,10.16,8.64">10</ref> and Fig. <ref type="figure" coords="20,186.22,185.71,8.47,8.64">11</ref>, we show the prompt how the LLM agent synthesizes positive and negative trajectories, respectively.</s><s coords="25,123.59,249.29,256.82,9.03">Refined Task Description: Your task is to grow an apple.</s><s coords="25,389.71,249.68,100.36,8.64;25,123.59,260.64,261.19,8.64">This will require growing several plants, and them being crosspollinated to produce fruit.</s><s coords="25,387.90,260.64,100.51,8.64;25,123.59,271.60,31.82,8.64">Seeds can be found in the kitchen.</s><s coords="25,158.50,271.60,190.42,8.64">To complete the task, focus on the grown apple.</s></p><p><s coords="25,123.59,293.52,364.82,8.64;25,123.59,304.48,56.58,8.64;25,123.59,315.43,7.47,8.64">Here is the task description you need to refine, and the corresponding trajectory is also provided: ...</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="25,123.59,336.96,110.34,8.96">Refined Task Description:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="25,123.59,415.55,220.10,8.64">Positive Trajectory Synthesis Prompt for ScienceWorld</head><p><s coords="25,123.26,436.94,365.15,9.03;25,123.59,448.28,52.45,8.64">Task Instruction: You are a helpful assistant to do some scientific experiments in an environment.</s></p><p><s coords="25,123.59,470.20,366.06,8.64;25,123.59,481.16,268.42,8.64">In the environment, there are several rooms: kitchen, foundry, workshop, bathroom, outside, living room, bedroom, greenhouse, art studio, and hallway.</s><s coords="27,108.00,427.66,396.00,8.64;27,108.00,438.62,194.58,8.64">For using few-shot prompting to guide the LLMs to predict the reward of historical trajectories, we use the following format of the few-shot prompt:</s></p><p><s coords="27,123.59,459.38,309.34,8.64">Few-shot Prompt for LLMs Directly Serving as ScienceWorld Reward Model</s></p><p><s coords="27,123.26,480.77,365.15,9.03;27,123.59,492.12,144.44,8.64">Task Instruction: You are an autonomous intelligent agent tasked with evaluating the trajectories of the past experience.</s><s coords="27,275.38,492.12,213.03,8.64;27,123.59,503.08,330.53,8.64">You will be given the history of a past experience in which you were placed in an environment and given a task to complete.</s><s coords="27,464.15,503.08,24.27,8.64;27,123.59,514.04,263.68,8.64">These tasks will be accomplished through the use of specific actions.</s><s coords="27,394.11,514.04,94.30,8.64;27,123.59,525.00,165.91,8.64">Now you are trying to evaluate the performance on a past task.</s><s coords="27,294.53,525.00,193.88,8.64;27,123.59,535.96,364.82,8.64;27,123.59,546.91,88.54,8.64">You will be given the objective of the task, the history of interaction including the observations you had and the actions you issued, and the status of the task.</s><s coords="27,217.64,546.91,272.42,8.64;27,123.59,557.87,364.82,8.64">Your goal is to think about the strategy and provided path to produce a score ranging from 0 to 1 to measure whether the objective of the task has been reached.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="27,123.59,579.79,84.22,8.64">Here are 2 examples:</head><p><s coords="27,123.59,601.32,45.94,8.96">Example1:</s></p><p><s coords="27,123.59,623.24,366.47,9.03;27,123.59,634.59,22.86,8.64">Human: You are a helpful assistant to do some scientific experiment in an environment.</s><s coords="27,149.62,634.59,178.81,8.64;28,107.53,689.82,285.93,8.64">In the environment, there are several rooms:  We show more qualitative results of our ARMAP in Fig. <ref type="figure" coords="28,333.72,689.82,9.90,8.64" target="#fig_3">13</ref> and Fig. <ref type="figure" coords="28,381.09,689.82,8.25,8.64" target="#fig_5">14</ref>.</s><s coords="28,396.56,689.82,107.45,8.64;28,108.00,700.78,396.00,8.64;28,108.00,711.73,280.02,8.64">Based on the examples, we can see that the automatic reward models in our pipeline can distinguish good trajectories from bad ones and guide LLMs to generate better trajectories to finish the tasks.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,260.70,290.07,34.64,5.20;4,257.97,222.62,44.67,5.95;4,257.83,239.25,49.34,5.95;4,257.92,253.61,41.06,5.95;4,261.85,203.09,22.88,6.69;4,318.45,203.09,39.49,6.69;4,334.22,217.71,8.79,10.41;4,285.57,274.82,73.36,5.20;4,379.92,86.67,102.17,6.69;4,371.89,110.53,50.14,6.69;4,433.47,101.32,60.01,6.69;4,431.69,118.75,63.20,6.69;4,408.01,148.35,44.44,6.69;4,468.77,166.07,9.38,6.69;4,468.84,184.88,9.38,6.69;4,462.20,156.66,22.73,6.69;4,369.08,151.02,23.88,6.69;4,374.86,159.63,14.02,6.69;4,407.64,212.00,48.85,6.69;4,471.61,228.73,9.38,6.69;4,471.68,247.54,9.38,6.69;4,465.03,219.33,22.73,6.69;4,369.61,213.69,28.49,6.69;4,377.70,222.30,14.02,6.69;4,390.86,267.69,13.85,5.20;4,390.17,274.72,15.25,5.20;4,432.08,266.70,17.71,5.20;4,433.31,273.73,15.25,5.20;4,479.99,266.90,10.06,5.20;4,472.27,273.93,25.50,5.20;4,390.37,284.42,18.27,5.20;4,385.86,291.45,27.30,5.20;4,432.43,284.23,20.70,5.20;4,429.14,291.25,27.30,5.20;4,398.81,231.57,8.79,10.41;4,471.73,232.96,8.79,10.41;4,478.77,283.73,14.45,5.20;4,472.35,290.76,27.30,5.20;4,291.13,264.82,54.27,5.20"><head/><label/><figDesc><div><p><s coords="4,308.31,264.82,37.10,5.20">16.44 to $43.61</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,108.00,319.04,396.00,8.64;10,108.00,330.00,397.74,8.64;10,108.00,340.96,396.00,8.64;10,108.00,351.92,396.00,8.64;10,108.00,362.88,396.00,8.64;10,108.00,373.84,397.24,8.64;10,108.00,384.80,396.00,8.64;10,107.64,395.75,396.36,8.64;10,108.00,406.71,397.73,8.64"><head>Figure 3 :</head><label>3</label><figDesc><div><p><s coords="10,108.00,319.04,216.10,8.64">Figure 3: Two qualitative results of the Webshop task.</s><s coords="10,327.18,319.04,176.82,8.64;10,108.00,330.00,397.74,8.64;10,108.00,340.96,396.00,8.64;10,108.00,351.92,396.00,8.64;10,108.00,362.88,95.11,8.64">The figure shows two examples utilizing the advantages of our ARMAP framework and we are able to correct errors made by existing methods.In the top example, when the search results do not meet the requirements, our ARMAP method leverages the advantage of the tree structure to backtrack and search again, thereby retrieving the appropriate target item.</s><s coords="10,206.63,362.88,297.37,8.64;10,108.00,373.84,25.78,8.64">In contrast, existing methods fail to backtrack when the target item is not found.</s><s coords="10,136.87,373.84,368.38,8.64;10,108.00,384.80,396.00,8.64;10,107.64,395.75,351.70,8.64">In the bottom example, by using the ARMAP to evaluate different states in the environment, our method is able to select the color that offers a higher reward and better meets the requirements when choosing between size and color, rather than mistakenly selecting the wrong size.</s><s coords="10,462.44,395.75,41.56,8.64;10,108.00,406.71,397.73,8.64">These two examples sufficiently demonstrate the advantages of our method compared to traditional approaches.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,261.05,452.15,24.53,5.28;10,288.92,437.50,26.81,5.28;10,288.92,442.63,30.41,5.27;10,288.92,447.74,12.54,5.28;10,188.40,492.64,20.60,4.11;10,213.46,551.82,8.47,4.10;10,213.46,555.82,10.10,4.10;10,205.15,475.81,8.14,11.52;10,112.27,475.06,74.16,6.25;10,112.27,482.41,76.32,6.25;10,112.27,489.77,44.20,6.25;10,111.98,499.97,63.31,6.25;10,112.02,548.73,66.37,6.25;10,164.83,462.02,17.81,5.92;10,140.06,507.19,30.56,6.25;10,116.74,434.27,136.07,7.23;10,120.36,441.47,128.78,7.23;10,146.12,448.67,77.36,7.22;10,317.48,452.15,25.79,5.28;10,345.03,436.52,26.87,5.27;10,345.03,441.64,32.79,5.27;10,345.03,446.75,10.32,5.28;10,375.59,452.15,27.22,5.28;10,217.75,492.50,19.52,4.11;10,213.46,499.42,8.47,4.11;10,213.46,503.42,10.10,4.10;10,213.46,507.57,6.88,4.11;10,216.23,511.74,14.81,4.10;10,112.09,523.08,66.02,6.25;10,112.09,530.44,77.62,6.25;10,112.09,537.80,63.16,6.25;10,188.87,543.35,20.54,4.11;10,205.70,526.15,8.13,11.52;10,214.71,543.47,21.69,4.10;10,112.44,570.88,83.71,6.25;10,227.54,572.53,9.02,3.71;10,140.04,555.25,26.73,6.25;10,122.78,586.44,101.43,8.40;10,398.95,436.11,39.92,5.28;10,398.95,441.23,41.99,5.28;10,398.95,446.36,12.56,5.27;10,298.80,463.28,18.96,5.91;10,406.27,462.61,47.83,5.92;10,321.01,492.59,20.54,4.10;10,337.77,475.75,8.14,11.52;10,244.88,475.00,71.30,6.25;10,244.88,482.36,71.24,6.25;10,244.88,489.71,13.96,6.25;10,244.58,499.90,59.00,6.25;10,244.58,554.55,75.77,6.25;10,345.09,492.58,19.03,4.10;10,321.63,538.25,20.54,4.11;10,338.46,521.05,8.13,11.52;10,344.56,538.37,21.69,4.10;10,244.58,571.20,53.11,6.25;10,458.63,435.98,41.97,5.28;10,458.63,441.09,22.94,5.28;10,458.63,446.22,12.55,5.27;10,337.13,502.12,8.51,4.11;10,337.13,506.12,10.12,4.10;10,266.82,508.16,25.77,6.25;10,244.71,520.27,66.02,6.25;10,244.71,527.63,76.35,6.25;10,244.71,534.99,45.27,6.25;10,337.13,553.85,8.51,4.11;10,337.13,557.86,10.12,4.10;10,337.13,562.01,6.90,4.11;10,339.90,566.17,15.25,4.10;10,337.13,570.17,9.93,4.11;10,340.60,574.33,26.05,4.10;10,244.58,562.87,72.89,6.25;10,244.71,545.56,64.60,6.25;10,447.60,492.90,20.54,4.10;10,464.35,476.08,8.14,11.52;10,370.79,473.65,71.24,6.25;10,370.79,481.00,74.09,6.25;10,370.79,488.36,29.97,6.25;10,370.79,513.40,75.75,6.25;10,471.47,492.90,21.69,4.10;10,370.79,534.91,53.09,6.25;10,370.79,524.15,72.89,6.25;10,370.79,499.88,64.60,6.25;10,436.17,452.15,27.22,5.28;10,251.06,586.44,232.06,8.40;10,120.21,508.84,2.06,12.71;10,463.29,515.59,8.47,4.10;10,463.29,519.58,10.10,4.11;10,463.29,523.75,6.87,4.10;10,466.06,527.90,15.25,4.11;10,463.29,531.91,9.91,4.10;10,466.76,536.06,26.07,4.11"><head/><label/><figDesc><div><p><s coords="10,166.07,434.27,86.74,7.23;10,120.36,441.47,128.78,7.23;10,146.12,448.67,77.36,7.22">select a 1 pound, certified organic sea salt shaker in the flavor triple blend flakes, and price lower than 30.00 dollars.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,108.00,610.17,396.00,8.64;10,108.00,621.12,396.00,8.64;10,108.00,632.08,396.00,8.64;10,108.00,643.04,396.17,8.64;10,108.00,654.00,301.67,8.64"><head>Figure 4 :</head><label>4</label><figDesc><div><p><s coords="10,108.00,610.17,365.26,8.64">Figure4: A typical example of customized reward target for shorter trajectory generation.</s><s coords="10,476.59,610.17,27.41,8.64;10,108.00,621.12,396.00,8.64;10,108.00,632.08,33.35,8.64">On the left, we show the default greedy decoding generates a long trajectory without finding the target product.</s><s coords="10,144.45,632.08,359.55,8.64;10,108.00,643.04,74.10,8.64">In the middle, we show our default reward can guide the LLM agent to generate a correct but long trajectory.</s><s coords="10,185.19,643.04,318.98,8.64;10,108.00,654.00,301.67,8.64">On the right, we show our framework with a customized reward target for shorter trajectories, which finds a correct and short trajectory for the target product.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="25,184.16,390.16,240.59,8.64"><head>Figure 9 :</head><label>9</label><figDesc><div><p><s coords="25,184.16,390.16,240.59,8.64">Figure 9: Instruction Refinement Prompt for ScienceWorld.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="28,124.84,632.04,359.24,8.64"><head>Figure 12 :</head><label>12</label><figDesc><div><p><s coords="28,124.84,632.04,359.24,8.64">Figure 12: Few-shot Prompt for LLMs Directly Serving as ScienceWorld Reward Model.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="31,112.12,265.75,79.86,9.40;31,455.77,213.41,11.17,5.50;31,455.77,218.80,13.22,5.50;31,455.77,224.18,9.16,5.50;31,459.46,229.57,40.37,5.50;31,459.46,234.96,40.37,5.50;31,455.55,253.24,11.17,5.50;31,455.55,258.63,13.22,5.50;31,455.55,264.02,9.16,5.50;31,459.24,269.41,40.37,5.50;31,459.24,274.79,40.37,5.50;31,415.06,298.27,73.61,12.55;31,218.83,296.68,69.80,12.55;31,311.20,228.76,108.04,7.05;31,311.20,235.69,71.06,7.05;31,111.56,229.30,108.31,7.06;31,111.56,236.24,65.21,7.05"><head/><label/><figDesc><div><p><s coords="31,385.06,228.76,34.18,7.05;31,311.20,235.69,71.06,7.05;31,111.56,229.30,108.31,7.06;31,111.56,236.24,65.21,7.05">with Fluoride for Cavity Protection of Teeth SprinJene Natural Fluoride Free Toothpaste for Clean and Strong Teeth</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="31,108.00,329.01,396.00,9.03;31,108.00,340.35,78.32,8.64"><head>Figure 15 :</head><label>15</label><figDesc><div><p><s coords="31,108.00,329.01,186.72,9.03">Figure 15: Failure Example from Webshop.</s><s coords="31,297.84,329.40,206.16,8.64;31,108.00,340.35,78.32,8.64">The reward model ignores certain key conditions in the task instruction.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="31,226.78,532.16,31.05,6.30;31,256.80,506.87,12.23,17.41;31,111.25,505.50,105.05,9.49;31,111.25,516.70,112.51,9.48;31,111.25,527.88,21.11,9.48;31,110.80,543.28,97.83,9.49;31,124.65,470.91,362.49,11.05;31,165.26,481.79,281.29,11.05;31,271.30,531.94,29.92,6.30;31,112.43,579.78,89.98,9.48;31,120.87,559.85,46.53,9.48;31,112.81,616.62,71.73,12.66;31,428.01,531.57,32.42,6.30;31,455.95,506.29,12.23,17.40;31,312.48,504.93,105.05,9.48;31,312.48,516.11,109.37,9.48;31,312.48,527.29,21.11,9.48;31,312.03,542.71,99.96,9.48;31,468.42,531.56,29.93,6.30;31,313.19,596.01,80.57,9.48;31,312.94,580.57,69.45,9.48;31,313.10,616.08,71.73,12.66;31,258.03,541.26,11.27,5.55;31,258.03,546.69,13.33,5.55;31,258.03,552.13,9.24,5.55;31,261.76,557.56,31.98,5.55;31,258.03,563.00,12.16,5.55;31,261.76,568.42,33.57,5.56;31,258.22,578.53,11.27,5.55;31,258.22,583.97,13.33,5.55;31,258.22,589.40,9.24,5.55;31,261.95,594.84,31.99,5.55;31,258.22,600.27,12.16,5.55;31,261.95,605.71,33.57,5.55;31,112.16,594.28,80.56,9.48;31,458.82,541.48,11.27,5.55;31,458.82,546.91,13.33,5.55;31,458.82,552.35,12.16,5.55;31,462.54,557.78,33.57,5.55;31,458.59,581.66,11.27,5.55;31,458.59,587.10,13.33,5.55;31,458.59,592.53,12.16,5.55;31,462.32,597.97,33.57,5.55;31,324.04,559.74,42.31,9.48;31,404.81,627.08,74.25,12.66;31,220.18,625.48,70.41,12.66"><head/><label/><figDesc><div><p><s coords="31,175.70,470.91,311.44,11.05;31,165.26,481.79,28.37,11.05">want to find white blackout shades that are 66 inches in width and 66 inches in height.</s><s coords="31,195.50,481.79,213.42,11.05">they need to be easy to install, and price lower than 90.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="31,108.00,656.24,396.00,9.03;31,108.00,667.59,233.83,8.64"><head>Figure 16 :</head><label>16</label><figDesc><div><p><s coords="31,108.00,656.24,194.18,9.03">Figure 16: Failure Example from Webshop.</s><s coords="31,307.19,656.63,196.80,8.64;31,108.00,667.59,233.83,8.64">The reward model misjudged the importance of different conditions, such as the size and color in this case.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,107.69,86.76,396.56,422.59"><head>Table 1 :</head><label>1</label><figDesc><div><p><s coords="9,143.49,361.31,256.56,8.64">Effectiveness of the proposed method on different benchmarks.</s><s coords="9,403.14,361.31,101.11,8.64;9,108.00,372.27,288.62,8.64">Our ARMAP framework consistently outperforms the baselines across different language models.</s></p></div></figDesc><table coords="9,150.90,86.76,310.20,422.59"><row><cell cols="3">Backbone Algorithms Webshop</cell><cell cols="4">ScienceWorld Game24 Average seen unseen</cell></row><row><cell/><cell>Sampling</cell><cell>52.0</cell><cell>53.9</cell><cell>50.6</cell><cell>9.6</cell><cell>38.0</cell></row><row><cell/><cell>Greedy</cell><cell>50.4</cell><cell>57.2</cell><cell>55.1</cell><cell>6.0</cell><cell>37.5</cell></row><row><cell>Llama70B</cell><cell>ARMAP-R</cell><cell>56.5</cell><cell>59.0</cell><cell>56.7</cell><cell>16.0</cell><cell>43.5</cell></row><row><cell/><cell>ARMAP-B</cell><cell>62.0</cell><cell>57.3</cell><cell>57.0</cell><cell>19.0</cell><cell>46.1</cell></row><row><cell/><cell>ARMAP-M</cell><cell>66.8</cell><cell>58.2</cell><cell>55.9</cell><cell>24.0</cell><cell>49.3</cell></row><row><cell/><cell>Sampling</cell><cell>56.4</cell><cell>24.5</cell><cell>20.6</cell><cell>2.0</cell><cell>27.0</cell></row><row><cell/><cell>Greedy</cell><cell>57.7</cell><cell>29.9</cell><cell>23.8</cell><cell>2.0</cell><cell>28.9</cell></row><row><cell>Llama8B</cell><cell>ARMAP-R</cell><cell>58.3</cell><cell>31.2</cell><cell>28.0</cell><cell>6.0</cell><cell>31.3</cell></row><row><cell/><cell>ARMAP-B</cell><cell>59.3</cell><cell>35.7</cell><cell>28.1</cell><cell>11.0</cell><cell>34.1</cell></row><row><cell/><cell>ARMAP-M</cell><cell>60.2</cell><cell>32.5</cell><cell>24.9</cell><cell>9.0</cell><cell>32.6</cell></row><row><cell/><cell>Sampling</cell><cell>17.7</cell><cell>18.4</cell><cell>17.1</cell><cell>1.0</cell><cell>12.2</cell></row><row><cell/><cell>Greedy</cell><cell>37.2</cell><cell>21.1</cell><cell>19.6</cell><cell>1.0</cell><cell>19.5</cell></row><row><cell>Mistral7B</cell><cell>ARMAP-R</cell><cell>54.1</cell><cell>21.7</cell><cell>19.7</cell><cell>2.0</cell><cell>25.6</cell></row><row><cell/><cell>ARMAP-B</cell><cell>54.4</cell><cell>24.5</cell><cell>21.2</cell><cell>2.0</cell><cell>26.4</cell></row><row><cell/><cell>ARMAP-M</cell><cell>58.2</cell><cell>30.0</cell><cell>23.4</cell><cell>4.0</cell><cell>29.6</cell></row><row><cell/><cell>Sampling</cell><cell>34.7</cell><cell>10.0</cell><cell>7.6</cell><cell>2.0</cell><cell>15.2</cell></row><row><cell/><cell>Greedy</cell><cell>42.4</cell><cell>9.5</cell><cell>6.5</cell><cell>2.1</cell><cell>17.5</cell></row><row><cell>Phi3.8B</cell><cell>ARMAP-R</cell><cell>53.3</cell><cell>9.6</cell><cell>7.2</cell><cell>4.0</cell><cell>21.9</cell></row><row><cell/><cell>ARMAP-B</cell><cell>52.1</cell><cell>20.0</cell><cell>17.0</cell><cell>9.0</cell><cell>26.5</cell></row><row><cell/><cell>ARMAP-M</cell><cell>53.7</cell><cell>28.3</cell><cell>24.3</cell><cell>10.0</cell><cell>30.0</cell></row><row><cell cols="2">Algorithms</cell><cell/><cell cols="3">Action↓ Price ↓ Reward ↑</cell><cell/></row><row><cell cols="2">Greedy</cell><cell/><cell>4.6</cell><cell>102.4</cell><cell>50.4</cell><cell/></row><row><cell cols="2">ARMAP-B</cell><cell/><cell>4.7</cell><cell>102.2</cell><cell>62.0</cell><cell/></row><row><cell cols="2">ARMAP-M</cell><cell/><cell>4.5</cell><cell>97.9</cell><cell>66.8</cell><cell/></row><row><cell cols="4">ARMAP-B+Length-Penalty 3.9</cell><cell>98.8</cell><cell>60.3</cell><cell/></row><row><cell cols="4">ARMAP-M+Length-penalty 4.0</cell><cell>102.1</cell><cell>65.5</cell><cell/></row><row><cell cols="3">ARMAP-B+Price-penalty</cell><cell>5.0</cell><cell>65.5</cell><cell>57.5</cell><cell/></row><row><cell cols="3">ARMAP-M+Price-penalty</cell><cell>4.3</cell><cell>69.0</cell><cell>62.4</cell><cell/></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,107.69,524.30,396.31,30.56"><head>Table 2 :</head><label>2</label><figDesc><div><p><s coords="9,142.74,524.30,141.99,8.64">Controllable Trajectory Generation.</s><s coords="9,287.82,524.30,216.19,8.64;9,108.00,535.26,294.58,8.64">We show that we can generate controllable trajectories like shorter action lengths and lower prices by customizing reward targets.</s><s coords="9,405.66,535.26,98.33,8.64;9,108.00,546.22,132.43,8.64">We use Llama70B as the default API for action prediction.</s></p></div></figDesc><table/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,107.69,86.76,396.66,233.73"><head>Table 3 :</head><label>3</label><figDesc><div><p><s coords="11,142.79,191.72,361.21,8.64;11,108.00,202.68,324.61,8.64;11,108.00,235.14,42.05,8.64">Ablation study of the proposed framework.Our ARMAP framework is more effective than directly finding a policy model and using the general LLM for reward generation.reasoning.</s><s coords="11,153.81,235.14,350.53,8.64;11,108.00,246.10,344.12,8.64">This framework allows LLM-based agents to enhance task planning by autonomously learning a reward model from the environment, without the need for human labeling.</s><s coords="11,455.22,246.10,48.78,8.64;11,108.00,257.06,396.00,8.64;11,108.00,268.02,267.38,8.64">The method utilizes pre-trained LLM agents to generate diverse action trajectories within an environment, which are then evaluated by a separate LLM based on the task's intent.</s><s coords="11,380.56,268.02,123.44,8.64;11,108.00,278.98,280.00,8.64">These evaluations help train a reward model that strengthens the agents' decision-making capabilities.</s><s coords="11,391.08,278.98,112.91,8.64;11,108.00,289.94,396.35,8.64;11,108.00,300.90,82.86,8.64">The framework enhances the performance of LLM agents in addressing complex tasks and mitigates issues related to data scarcity and API limitations.</s><s coords="11,195.18,300.90,308.82,8.64;11,108.00,311.85,392.93,8.64">Its effectiveness is demonstrated across various benchmarks, representing a significant advancement in the development of AI agents for real-world, multi-step problem-solving.</s></p></div></figDesc><table coords="11,160.35,86.76,279.35,90.01"><row><cell>Models</cell><cell>Model Base</cell><cell>ScienceWorld ( seen )</cell></row><row><cell>Greedy</cell><cell>Phi3.8B</cell><cell>9.6</cell></row><row><cell>SFT-Policy</cell><cell>VILA3B</cell><cell>18.6</cell></row><row><cell cols="2">ARMAP-B w/o R Llama70B and Phi3.8B ARMAP-M w/o R</cell><cell>16.0 26.5</cell></row><row><cell>ARMAP-B ARMAP-M</cell><cell>VILA3B and Phi3.8B</cell><cell>20.0 28.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="17,108.00,170.35,396.17,297.39"><head>Table 6 :</head><label>6</label><figDesc><div><p><s coords="17,204.48,170.35,238.26,8.64;17,108.00,205.97,396.00,8.64;17,108.00,216.93,216.40,8.64;17,108.00,233.48,396.00,9.03;17,108.00,244.82,396.00,8.64;17,108.00,255.78,396.17,8.64;17,108.00,266.74,396.17,8.64;17,108.00,277.70,365.82,8.64">Experiments of training data generated from various LLMs.outcomes, it is clear that our method does not overly rely on the capabilities of language models.In other words, our method is highly efficient and robust.Reward Modeling Target.To further investigate the optimization target of the reward model, we conduct experiments to compare the performance of pairwise comparison and binary classification as learning methods for the reward model.Specifically, in the classification setting: each input pair is treated as a positive and a negative example.The model is trained to predict a score of 1 for positive examples and 0 for negative examples.The comparative results are shown in Table7.</s><s coords="17,476.89,277.70,27.11,8.64;17,108.00,288.66,319.76,8.64">Across all settings, pairwise comparison consistently outperforms binary classification.</s><s coords="17,430.84,288.66,73.16,8.64;17,108.00,299.62,396.00,8.64;17,108.00,310.58,223.45,8.64">This confirms that pairwise comparison captures nuanced preferences more effectively than binary classification, leading to better reward modeling and overall task performance.</s></p></div></figDesc><table coords="17,177.89,339.65,256.23,128.09"><row><cell>Backbone</cell><cell>Algorithms</cell><cell cols="4">Classification Seen Unseen Seen Unseen Comparative</cell></row><row><cell>LLaMA-70B</cell><cell cols="2">ARMAP-R 57.0 ARMAP-B 47.2</cell><cell>55.4 43.3</cell><cell>59.0 57.3</cell><cell>56.7 57.0</cell></row><row><cell>LLaMA-8B</cell><cell cols="2">ARMAP-R 29.0 ARMAP-B 27.5</cell><cell>24.2 22.2</cell><cell>31.2 35.7</cell><cell>28.0 28.1</cell></row><row><cell>Mistral-7B</cell><cell cols="2">ARMAP-R 17.8 ARMAP-B 19.1</cell><cell>18.2 17.3</cell><cell>21.7 24.5</cell><cell>19.7 21.1</cell></row><row><cell>Phi-3.8B</cell><cell cols="2">ARMAP-R ARMAP-B 17.7 8.6</cell><cell>4.8 13.7</cell><cell>9.6 20.0</cell><cell>7.2 17.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="17,125.83,482.69,357.26,8.64"><head>Table 7 :</head><label>7</label><figDesc><div><p><s coords="17,161.05,482.69,322.04,8.64">Comparison of the Classification target and Comparison target on ScienceWorld.</s></p></div></figDesc><table/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="18,107.64,159.18,396.36,344.61"><head>Table 10 :</head><label>10</label><figDesc><div><p><s coords="18,107.64,360.02,396.36,9.03;18,108.00,371.37,396.00,8.64;18,108.00,382.33,396.00,8.64;18,108.00,393.29,72.53,8.64">Ablation on Visual Input.We also train a new reward model without visual information.As shown in Table10, we can see that, in different settings, the reward model with visual information performs better than the model without visual information, which shows the importance of visual context in the Webshop task.</s><s coords="18,268.52,495.15,112.08,8.64">Ablation of the visual input.</s></p></div></figDesc><table coords="18,153.66,159.18,311.04,8.64"><row><cell>Backbone</cell><cell>VILA-3B VILA-13B LLaVA-13B 1/5 Data 1/25 Data</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="18,185.43,623.32,241.14,70.31"><head>Table 11 :</head><label>11</label><figDesc><div><p><s coords="18,226.08,684.99,196.95,8.64">Tokens of data generation in three different tasks.</s></p></div></figDesc><table coords="18,185.43,623.32,241.14,46.72"><row><cell>Tasks</cell><cell>Samples</cell><cell>Tokens</cell><cell>Tokens per Sample</cell></row><row><cell>ScienceWorld</cell><cell>4064</cell><cell>2541255</cell><cell>625</cell></row><row><cell>Webshop</cell><cell>2436</cell><cell>6645746</cell><cell>2728</cell></row><row><cell>Game of 24</cell><cell>37885</cell><cell>12846182</cell><cell>339</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="19,108.25,235.33,334.08,109.70"><head>Table 12 :</head><label>12</label><figDesc><div><p><s coords="19,209.87,297.00,232.47,8.64;19,108.25,336.40,15.66,8.64">Experiments of using the proprietary model on ALFWorldA.3</s><s coords="19,134.37,336.40,119.54,8.64">IMPLEMENTATION DETAILS.</s></p></div></figDesc><table coords="19,247.94,235.33,104.17,46.72"><row><cell/><cell>Std Dev</cell></row><row><cell>Sampling</cell><cell>0.74 0.88</cell></row><row><cell>Greedy</cell><cell>0.82 0.90</cell></row><row><cell cols="2">ARMAP-B 0.84 0.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="22,123.01,92.05,367.14,627.88"><head/><label/><figDesc><div><p><s coords="22,123.59,92.05,364.82,8.64;22,123.59,103.01,364.82,8.64;22,123.59,113.97,189.64,8.64;22,123.26,124.54,326.40,9.03">Unit Portable Radio 10W 20w 4 Ohm Speaker HiFi Bluetooth Speakers 55mm Bookshelf Speakers (Color : 4 ohm 20W) [SEP] Price: $42.66 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy NowThought: I think I should click on the 'Buy Now' button to purchase the product.</s><s coords="22,205.59,207.19,282.82,8.64;22,123.59,218.15,53.51,8.64">You are a helpful assistant to do some scientific experiments in an environment.</s><s coords="22,183.18,218.15,306.47,8.64;22,123.59,229.11,364.82,8.64;22,123.59,240.07,287.09,8.64">In the environment, there are several rooms: kitchen, foundry, workshop, bathroom, outside, living room, bedroom, greenhouse, art studio, hallway You should explore the environment and find the items you need to complete the experiment.</s><s coords="22,413.77,240.07,74.64,8.64;22,123.59,251.02,86.08,8.64">You can teleport to any room in one step.</s><s coords="22,212.76,251.02,275.65,8.64;22,123.59,261.98,151.63,8.64">All containers in the environment have already been opened, you can directly get items from the containers.</s><s coords="22,195.41,327.74,224.97,8.64">Your task is to find a non-living thing in the environment.</s><s coords="22,423.50,327.74,64.91,8.64;22,123.59,338.70,72.77,8.64">Focus on the jug in the greenhouse.</s><s coords="22,199.45,338.70,260.62,8.64">When you are done, move the jug to the green box in the kitchen.</s><s coords="22,164.96,667.46,323.45,8.64;22,123.59,678.42,364.82,8.64;22,123.59,689.38,186.40,8.64;22,123.23,699.95,137.83,9.03;22,123.59,710.91,183.03,9.03">the art studio (that is open), A door to the bedroom (that is open), A door to the greenhouse (that is open), A door to the kitchen (that is open), A door to the living room (that is open), A door to the workshop (that is open) Action: teleport to the greenhouse Observation: You teleport to the greenhouse.</s><s coords="25,203.90,118.17,122.34,8.64">Your task is to grow an apple.</s><s coords="25,330.21,118.17,137.83,8.64">You can find seeds in the kitchen.</s><s coords="25,472.01,118.17,16.40,8.64;25,123.59,129.13,132.01,8.64">You should focus on the grown apple.</s></p></div></figDesc><table coords="22,123.01,135.50,367.14,540.60"><row><cell>Action: click[buy now] Figure 5: Training Data Example for Webshop. Training Data Example for ScienceWorld Task Instruction: The available actions are: open OBJ: open a container close OBJ: close a container ... wait1: task no action for a step Task Description: Positive Example: Action: look around Observation:This room is called the hallway. In it, you see: the agent, a substance called air, a picture You also see: A door to the art studio (that is open), A door to the bedroom (that is open), A door to the greenhouse (that is open), A door to the kitchen (that is open), A door to the living room (that is open), A door to the workshop (that is open) Action: teleport to the greenhouse Observation: You teleport to the greenhouse. Action: look around Observation: This room is called the greenhouse. In it, you see: the agent, a substance called air, a adult bee, a adult bee, a adult bee, a adult bee, a bee hive. The bee hive door is open. In the bee hive is: a adult bee. a flower pot 2 (containing a peach tree in the adult stage, soil), a flower pot 4 (containing a substance called water), a flower pot 5 (containing a substance called water), a flower pot 6 (containing a banana tree in the adult stage, soil), a flower pot 7 (containing a apple tree in the adult stage, soil), a jug (containing nothing), a shovel, a sink, which is turned off. In the sink is: nothing. You also see: A door to the hallway (that is open), A door to the outside (that is open) Action: focus on the jug Observation: You focus on the jug. Negative Example: Action: look around Observation:This room is called the hallway. In it, you see: the agent, a substance called air, a picture You also see: Task Description: Corresponding Trajectory: look around This room is called the hallway. In it, you see: ... open door to kitchen The door is already open. go to kitchen You move to the kitchen. A door to Example: ...</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="25,123.01,503.08,366.64,216.86"><head>Table 14 :</head><label>14</label><figDesc><div><p><s coords="26,205.59,259.99,282.82,8.64;26,123.59,292.87,366.06,8.64;26,123.59,303.83,268.42,8.64;26,123.01,369.58,291.50,8.64">You are a helpful assistant to do some scientific experiments in an In the environment, there are several rooms: kitchen, foundry, workshop, bathroom, outside, living room, bedroom, greenhouse, art studio, and hallway.You will be given a task description and a corresponding trajectory.</s><s coords="26,423.13,369.58,66.52,8.64;26,123.34,380.54,365.07,8.64;26,123.59,391.50,30.66,8.64">Based on them, you need to generate a negative sample that is similar to the correct trajectory but different from it.</s><s coords="26,158.02,391.50,332.13,8.64">The generated trajectory should not meet all requirements of the task description.</s><s coords="26,123.59,402.46,342.54,8.64">Moreover, the generated trajectory should satisfy all requirements of the environment.</s><s coords="26,203.14,446.29,285.27,8.64;26,123.59,457.25,65.03,8.64">Your task is to focus on the life stages of the apple plant, starting from earliest to latest.</s><s coords="26,191.71,457.25,121.46,8.64;27,164.58,84.12,279.75,8.64">The plants are located outside. Figure 11: Negative Trajectories Synthesis Prompt for ScienceWorld.</s><s coords="27,108.00,116.77,137.43,8.96">Reward Model Training Details.</s><s coords="27,255.39,117.16,248.61,8.64;27,108.00,128.12,188.40,8.64">The detailed hyperparameters we use for reward model during training and inference are shown in Table 14.</s><s coords="27,301.74,128.12,202.26,8.64;27,108.00,139.08,132.61,8.64">We employ identical hyperparameters for reward models of different environments.</s><s coords="27,243.69,139.08,260.30,8.64;27,108.00,150.04,226.92,8.64">For Webshop, we use checkpoint of 1100 steps in ARMAP-B, and checkpoint of 1200 steps in ARMAP-R and ARMAP-M.</s><s coords="27,229.47,351.08,193.26,8.64">Detailed hyperparameters used in reward model.</s><s coords="27,108.00,388.42,196.04,8.96">Implementation Details of Ablation baselines.</s><s coords="27,314.01,388.80,189.99,8.64;27,108.00,399.76,165.21,8.64">For SFT, we use all positive examples from the reward model training as the training data.</s><s coords="27,276.27,399.76,227.73,8.64;27,108.00,410.72,175.45,8.64">The training objective is to enable the model to predict the output of the LLM in the positive examples.</s></p></div></figDesc><table coords="25,123.01,503.08,365.40,173.02"><row><cell>The available actions are:</cell></row><row><cell>open OBJ: open a container</cell></row><row><cell>. . .</cell></row><row><cell>Based on this environment, you need to randomly propose a Task Description, which</cell></row><row><cell>concludes what you have done in this environment.</cell></row><row><cell>Here are some examples:</cell></row><row><cell>Your task is to use chemistry to create green paint. When you are done, focus on the green</cell></row><row><cell>paint.</cell></row><row><cell>Your task is to determine whether tall plant height is a dominant or recessive trait in the pea</cell></row><row><cell>plant. If the trait is dominant, focus on the red box. If the trait is recessive, focus on the green</cell></row><row><cell>box.</cell></row><row><cell>. . .</cell></row><row><cell>Once you obtain the Task Description, you need to navigate through the environment to</cell></row><row><cell>complete the instruction and generate a trajectory.</cell></row></table><note coords="25,123.59,688.99,40.96,8.96;25,123.26,699.95,366.39,9.03;25,123.59,711.30,98.22,8.64"><p><s coords="25,123.59,688.99,40.96,8.96;25,123.26,699.95,282.20,9.03">Example: Task Description: Your task is to use chemistry to create green paint.</s><s coords="25,408.55,700.34,81.10,8.64;25,123.59,711.30,98.22,8.64">When you are done, focus on the green paint.</s></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="27,123.59,634.59,366.06,85.35"><head/><label/><figDesc><div><p><s coords="27,331.56,634.59,158.09,8.64;27,123.59,645.54,263.06,8.64">kitchen, foundry, workshop, bathroom, outside, living room, bedroom, greenhouse, art studio, hallway.</s><s coords="27,392.21,645.54,96.20,8.64;27,123.59,656.50,271.32,8.64">You should explore the environment and find the items you need to complete the experiment.</s><s coords="27,398.00,656.50,90.76,8.64;28,172.84,92.05,315.58,8.64;28,123.59,103.01,366.06,8.64;28,123.59,113.97,364.82,8.64;28,123.59,124.93,366.07,8.64;28,123.59,135.89,364.82,8.64;28,123.59,146.85,63.91,8.64;28,126.08,157.42,238.45,9.03">You can teleport to any , examine OBJ: describe an object in detail, look at OBJ: describe a container's contents, read OBJ: read a note or book, move OBJ to OBJ: move an object to a container, pick up OBJ: move an object to the inventory, pour OBJ into OBJ: pour a liquid into a container, mix OBJ: chemically mix a container, teleport to LOC: teleport to a specific room, focus on OBJ: signal intent on a task object, wait: task no action for 10 steps, wait1: task no action for a step Task Description: Your task is to find a(n) non-living thing.</s><s coords="28,367.62,157.81,96.28,8.64">First, focus on the thing.</s><s coords="28,466.99,157.81,22.67,8.64;28,123.59,168.76,108.73,8.64;28,249.27,168.76,48.21,8.64">Then, move it to the purple box in living room.</s><s coords="28,300.56,168.76,189.59,8.64">Please only output one Thought and one Action.</s><s coords="28,123.28,179.72,365.12,8.64;28,123.59,190.68,364.82,8.64;28,123.59,201.64,366.21,8.64;28,123.59,212.60,364.82,8.64;28,123.59,223.56,364.82,8.64;28,123.59,234.52,366.06,8.64;28,123.59,245.48,366.06,8.64;28,123.59,256.44,366.06,8.64;28,123.23,267.39,261.80,8.64">The available actions are: open OBJ: open a container, close OBJ: close a container, activate OBJ: activate a device, deactivate OBJ: deactivate a device, connect OBJ to OBJ: connect electrical components, disconnect OBJ: disconnect electrical components, use OBJ [on OBJ]: use a device/item, look around: describe the current room, examine OBJ: describe an object in detail, look at OBJ: describe a container's contents, read OBJ: read a note or book, move OBJ to OBJ: move an object to a container, pick up OBJ: move an object to the inventory, pour OBJ into OBJ: pour a liquid into a container, mix OBJ: chemically mix a container, teleport to LOC: teleport to a specific room, focus on OBJ: signal intent on a task object, wait: task no action for 10 steps, wait1: task no action for a step.</s><s coords="28,388.13,267.39,100.28,8.64;28,123.59,278.35,85.22,8.64">Please strictly follow the format of the actions.</s><s coords="28,123.23,288.92,366.43,9.03;28,123.59,300.27,210.30,8.64">Assistant: Thought: I need to find a non-living thing, which could be an object in any room, so I should start by looking around the current room.</s><s coords="28,336.98,300.27,80.85,8.64;28,123.59,310.84,221.64,9.03">Action: look around Human: Observation: This room is called the bathroom.</s><s coords="28,348.24,311.23,140.16,8.64;28,123.59,322.19,163.82,8.64">In it, you see: the agent, a substance called air, a bathtub, which is turned off.</s><s coords="28,290.51,322.19,105.12,8.64">In the bathtub is: nothing.</s><s coords="28,398.73,322.19,89.68,8.64;28,123.26,333.15,191.34,8.64">a drawing, a glass cup (containing nothing), a sink, which is turned off.</s><s coords="28,317.70,333.15,89.29,8.64">In the sink is: nothing.</s><s coords="28,410.09,333.15,29.65,8.64">a toilet.</s><s coords="28,442.83,333.15,45.58,8.64;28,123.59,344.11,214.37,8.64">In the toilet is: A drain, which is open, a substance called water.</s><s coords="28,342.29,344.11,146.12,8.64;28,123.26,355.07,56.58,8.64">You also see: A door to the kitchen (that is open).</s><s coords="28,183.70,355.07,197.91,8.64">Please only output one Thought and one Action.</s><s coords="28,385.49,355.07,104.30,8.64;28,123.59,366.02,366.06,8.64;28,123.59,376.98,366.06,8.64;28,123.59,387.94,366.06,8.64;28,123.59,398.90,364.81,8.64;28,123.59,409.86,366.20,8.64;28,123.59,420.82,364.82,8.64;28,123.59,431.78,366.20,8.64;28,123.59,442.74,364.82,8.64;28,123.59,453.70,174.60,8.64">The available actions are: open OBJ: open a container, close OBJ: close a container, activate OBJ: activate a device, deactivate OBJ: deactivate a device, connect OBJ to OBJ: connect electrical components, disconnect OBJ: disconnect electrical components, use OBJ [on OBJ]: use a device/item, look around: describe the current room, examine OBJ: describe an object in detail, look at OBJ: describe a container's contents, read OBJ: read a note or book, move OBJ to OBJ: move an object to a container, pick up OBJ: move an object to the inventory, pour OBJ into OBJ: pour a liquid into a container, mix OBJ: chemically mix a container, teleport to LOC: teleport to a specific room, focus on OBJ: signal intent on a task object, wait: task no action for 10 steps, wait1: task no action for a step.</s><s coords="28,301.28,453.70,183.88,8.64">Please strictly follow the format of the actions.</s><s coords="28,123.23,464.27,365.18,9.03;28,123.59,475.61,20.48,8.64">Assistant: Thought: The glass cup in the bathroom is a non-living thing, so I should focus on it.</s><s coords="28,147.16,475.61,108.79,8.64;28,123.59,486.57,7.47,8.64">Action: focus on glass cup.</s><s coords="28,123.23,497.14,366.92,9.03">... Assistant: Thought: I've found the purple box, and now I need to move the glass cup into it.</s><s coords="28,123.23,508.49,152.22,8.64">Action: move glass cup to purple box.</s><s coords="28,123.59,519.06,239.23,9.03">Human: Observation: No known action matches that input.</s><s coords="28,123.59,530.02,310.02,9.03">Human: Please evaluate whether you complete the 'Task Description' or not.</s><s coords="28,123.59,541.37,257.69,8.64">Following your definitions, the task completion score is &lt;0.750&gt;</s></p></div></figDesc><table coords="28,123.59,562.90,70.32,30.94"><row><cell>Example2: ...</cell></row><row><cell>Now is your turn:</cell></row></table><note coords="27,123.59,667.46,361.82,8.64;27,123.59,678.42,366.06,8.64;27,123.59,689.38,364.82,8.64;27,123.59,700.34,364.82,8.64;27,123.59,711.30,361.61,8.64;28,123.59,92.05,49.24,8.64"><p><s coords="27,123.59,667.46,71.93,8.64">room in one step.</s><s coords="27,200.12,667.46,285.29,8.64;27,123.59,678.42,149.10,8.64">All containers in the environment have already been opened, you can directly get items from the containers.</s><s coords="27,275.77,678.42,213.88,8.64;27,123.59,689.38,364.82,8.64;27,123.59,700.34,364.82,8.64;27,123.59,711.30,361.61,8.64;28,123.59,92.05,49.24,8.64">The available actions are: open OBJ: open a container, close OBJ: close a container, activate OBJ: activate a device, deactivate OBJ: deactivate a device, connect OBJ to OBJ: connect electrical components, disconnect OBJ: disconnect electrical components, use OBJ [on OBJ]: use a device/item, look around: describe the current room</s></p></note></figure>
		</body>
		<back>

			
			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,232.49,724.22,72.01,7.20">-agent.github.io</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="16,108.30,84.04,74.33,10.37">A APPENDIX</head><p><s coords="16,108.00,110.06,279.31,8.64">In this section, we provide supplementary material for the main paper.</s></p><p><s coords="16,108.25,136.82,246.53,8.64">A.1 EXPERIMENTS ON ALFWORLD AND AGENTCLINIC.</s></p><p><s coords="16,107.53,158.05,398.12,8.64;16,108.00,169.01,396.00,8.64;16,106.68,179.97,131.66,8.64">We extend our experiment on ALFWorld <ref type="bibr" coords="16,276.87,158.05,87.74,8.64" target="#b51">(Shridhar et al., 2021)</ref>, a classic environment for House-Holding, where the agent must accomplish tasks in physical house-holding environments, like "Put a pan on the dining table".</s><s coords="16,243.47,179.97,260.53,8.64;16,108.00,190.93,396.00,8.64;16,108.00,201.89,27.58,8.64">Following the setup of AgentBench <ref type="bibr" coords="16,393.87,179.97,70.48,8.64" target="#b35">(Liu et al., 2023)</ref> for LLM evaluation, we test the model on the dev and std split, using the default success rate as the evaluation metric.</s><s coords="16,138.69,201.89,365.31,8.64;16,108.00,212.84,165.67,8.64">Specifically, we used LLaMa-3.1-70B to generate around 1600 pairs of positive and negative samples with our data generation pipeline.</s><s coords="16,276.77,212.84,228.97,8.64">Then we train a reward model with these synthesized data.</s><s coords="16,107.53,223.80,396.46,8.64;16,108.00,234.76,343.03,8.64">We evaluate our ARMAP framework on ALFWorld using various planning algorithms, including Reflexion and Best-of-N, which we refer to as ARMAP-R and ARMAP-B, respectively.</s><s coords="16,454.09,234.76,51.15,8.64;16,107.64,245.72,397.74,8.64;16,108.00,256.68,89.40,8.64">Additionally, we compare our approach with two baseline methods that do not incorporate reward model guidance: Sampling and Greedy.</s><s coords="16,200.48,256.68,116.18,8.64">The results are shown below.</s><s coords="16,319.74,256.68,184.26,8.64;16,107.64,267.64,393.34,8.64">As shown in Table <ref type="table" coords="16,396.71,256.68,3.77,8.64">4</ref>, our model still performs well in this challenging environment, which contains diverse scenes and long-horizon planning tasks.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="16,205.70,294.70,29.89,8.64">Models</head><p><s coords="16,263.04,294.70,38.95,8.64;16,107.53,397.79,396.46,8.64;16,108.00,408.75,184.06,8.64">ALFWorld We also extended our experiments to ClinicalAgent <ref type="bibr" coords="16,328.42,397.79,102.69,8.64" target="#b47">(Schmidgall et al., 2024)</ref>, an environment designed for medical decision-making tasks.</s><s coords="16,297.67,408.75,206.33,8.64;16,108.00,419.71,278.06,8.64">ClinicalAgent evaluates models on their ability to interpret clinical scenarios and make accurate, high-stakes decisions.</s><s coords="16,389.15,419.71,114.85,8.64;16,108.00,430.66,47.33,8.64;23,123.59,492.26,193.99,8.64">Results of ClinicalAgent are provided in    Instruction Generation Prompt for ScienceWorld</s></p><p><s coords="23,123.26,513.65,365.15,9.03;23,123.59,525.00,52.45,8.64">Task Instruction: You are a helpful assistant to do some scientific experiments in an environment.</s></p><p><s coords="23,123.59,546.91,366.06,8.64;23,123.59,557.87,268.42,8.64">In the environment, there are several rooms: kitchen, foundry, workshop, bathroom, outside, living room, bedroom, greenhouse, art studio, and hallway.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="23,123.28,579.79,101.64,8.64">The available actions are:</head><p><s coords="23,123.59,590.75,110.67,8.64;23,123.59,601.71,10.46,8.64">open OBJ: open a container . . .</s></p><p><s coords="23,123.01,623.63,315.54,8.64">You will be given a dialogue between you (assistant) and a human user.</s><s coords="23,448.16,623.63,40.25,8.64;23,123.59,634.59,317.75,8.64">You need to generate the task description after understanding the dialogue given to you.</s><s coords="23,445.14,634.59,43.27,8.64;23,123.59,645.54,315.37,8.64">In order to help you better generate the task description, I will give you an example below.</s></p><p><s coords="23,123.59,667.46,276.78,8.64">In this example, a human user gives a Task Description at first.</s><s coords="23,409.52,667.46,78.90,8.64;23,123.59,678.42,366.56,8.64">Then you work as an assistant and talk with user to finish the task step by step using the available actions above.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="30,108.25,397.90,135.54,8.64">A.5 FAILURE CASE ANALYSIS</head><p><s coords="30,108.00,418.42,396.00,8.64;30,108.00,429.38,153.28,8.64">In this section, we investigate the common failure cases of our framework, aiming to provide data points and insights for future research.</s></p><p><s coords="30,107.69,446.32,396.31,8.64;30,108.00,457.28,191.78,8.64">The most common error occurs when there are multiple restrictions in the instruction, the reward model overlooks some of these key conditions.</s><s coords="30,303.66,457.28,201.59,8.64;30,108.00,468.24,396.00,8.64;30,108.00,479.20,46.76,8.64">A representative example is illustrated in Fig. <ref type="figure" coords="30,492.54,457.28,8.47,8.64">15</ref>, the model focuses on price and size but ignores the details about 'Fluoride' hidden in the product description.</s></p><p><s coords="30,107.64,496.13,326.49,8.64">Another common failure mode occurs when commonsense knowledge is involved.</s><s coords="30,437.22,496.13,66.78,8.64;30,108.00,507.09,396.17,8.64;30,108.00,518.05,49.54,8.64">As demonstrated in Fig. <ref type="figure" coords="30,137.71,507.09,8.47,8.64">16</ref>, the agent was tasked with buying a blackout shade but failed to choose both the color and the size.</s><s coords="30,160.64,518.05,343.36,8.64;30,108.00,529.01,53.78,8.64">While, in everyday life, size is generally more important, the reward model prioritized color instead.</s><s coords="30,164.86,529.01,339.14,8.64;30,108.00,539.97,209.19,8.64">In Fig. <ref type="figure" coords="30,193.96,529.01,8.32,8.64">17</ref>, the reward model cannot assess the lifespan of dragonflies and chipmunks because it lacks the necessary biological knowledge.</s></p><p><s coords="30,108.00,556.52,47.47,8.96">Discussion.</s><s coords="30,158.58,556.91,332.01,8.64">The analysis of failure modes highlights the significant potential of our framework.</s><s coords="30,493.70,556.91,10.30,8.64;30,108.00,567.86,397.39,8.64;30,107.67,578.43,396.33,9.03;30,108.00,589.78,396.25,8.64;30,108.00,600.74,195.40,8.64">To improve its performance, we propose two possible strategies for improvements in reward modeling: (a) Constructing Data with Focus on Complex and Detailed Conditions: enhancing the dataset to include scenarios with higher complexity and more nuanced conditions will help the framework better handle intricate situations and edge cases.</s><s coords="30,306.55,600.35,197.46,9.03;30,108.00,611.70,396.00,8.64;30,107.64,622.66,122.17,8.64">(b) Intervening in Reward Scoring with External Knowledge: incorporating external knowledge by combining a prompted Large Language Model with the trained reward model.</s><s coords="30,232.90,622.66,271.10,8.64;30,108.00,633.62,396.00,8.64;30,108.00,644.58,57.20,8.64">This approach allows the LLM's generalized knowledge to calibrate the reward scores in a controllable manner, improving the overall accuracy and robustness of the reward model.</s></p><p><s coords="32,311.67,435.71,141.06,9.30">Observation4: You focus on the parrot egg.</s></p><p><s coords="32,311.67,466.21,160.21,9.30">Observation5: You focus on the baby chipmunk .</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,108.00,359.18,397.24,8.64;11,117.60,370.14,386.57,8.64;11,117.96,381.10,239.64,8.64" xml:id="b0">
	<monogr>
		<title level="m" type="main" coords="11,213.98,370.14,290.19,8.64;11,117.96,381.10,22.37,8.64">Phi-3 technical report: A highly capable language model locally on your phone</title>
		<author>
			<persName coords=""><forename type="first">Jyoti</forename><surname>Marah Abdin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hany</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><surname>Awadalla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ammar</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nguyen</forename><surname>Ahmad Awan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bahree</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2404.14219"/>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, and et al. Phi-3 technical report: A highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219.</note>
</biblStruct>

<biblStruct coords="11,108.00,400.87,396.00,8.64;11,117.96,411.65,386.04,8.82;11,117.96,422.60,100.17,8.82" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Anthony</forename><surname>Michael Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yevgen</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omar</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Byron</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>David</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01691</idno>
		<title level="m" coords="11,141.10,411.83,286.81,8.64">Do as i can and not as i say: Grounding language in robotic affordances</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, and et al. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691, 2022.</note>
</biblStruct>

<biblStruct coords="11,108.00,442.55,396.00,8.64;11,117.96,453.51,386.03,8.64;11,117.96,464.29,233.15,8.82" xml:id="b2">
	<monogr>
		<title level="m" type="main" coords="11,380.03,453.51,123.97,8.64;11,117.96,464.47,65.71,8.64">Constitutional ai: Harmlessness from ai feedback</title>
		<author>
			<persName coords=""><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sandipan</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cameron</forename><surname>Mckinnon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08073</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.</note>
</biblStruct>

<biblStruct coords="11,108.00,484.23,396.00,8.64;11,117.96,495.01,166.28,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coords="11,274.89,484.23,229.11,8.64;11,117.96,495.19,86.62,8.64">Rank analysis of incomplete block designs: I. the method of paired comparisons</title>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bradley</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Milton</forename><forename type="middle">E</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,212.58,495.01,42.51,8.59">Biometrika</title>
		<imprint>
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 1952.</note>
</biblStruct>

<biblStruct coords="11,108.00,514.78,396.00,8.82;11,117.36,525.74,260.25,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coords="11,158.08,514.96,209.27,8.64">A robust layered control system for a mobile robot</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brooks</surname></persName>
		</author>
		<idno type="DOI">10.1109/JRA.1986.1087032</idno>
	</analytic>
	<monogr>
		<title level="j" coords="11,377.05,514.78,126.95,8.59;11,117.36,525.74,44.35,8.59">IEEE Journal on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="23"/>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Brooks. A robust layered control system for a mobile robot. IEEE Journal on Robotics and Automation, 2(1):14-23, 1986. doi: 10.1109/JRA.1986.1087032.</note>
</biblStruct>

<biblStruct coords="11,108.00,545.68,396.00,8.64;11,117.96,556.64,387.78,8.64" xml:id="b5">
	<monogr>
		<title level="m" type="main" coords="11,139.84,556.64,151.21,8.64">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dhariwal</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.14165"/>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, and et al. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.</note>
</biblStruct>

<biblStruct coords="11,108.00,576.41,397.74,8.64;11,117.60,587.37,388.05,8.64;11,117.96,598.15,387.28,8.82;11,117.96,609.28,22.42,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coords="11,117.60,587.37,388.05,8.64;11,117.96,598.33,221.74,8.64">A contemporary review on chatbots, ai-powered virtual conversational agents, chatgpt: Applications, open challenges and future research directions</title>
		<author>
			<persName coords=""><forename type="first">Avyay</forename><surname>Casheekar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Archit</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kanishk</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaushik</forename><surname>Sanjay Prabhakar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathiravan</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,347.82,598.15,104.15,8.59">Computer Science Review</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">100632</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Avyay Casheekar, Archit Lahiri, Kanishk Rath, Kaushik Sanjay Prabhakar, and Kathiravan Srinivasan. A contemporary review on chatbots, ai-powered virtual conversational agents, chatgpt: Appli- cations, open challenges and future research directions. Computer Science Review, 52:100632, 2024.</note>
</biblStruct>

<biblStruct coords="11,108.00,629.05,396.00,8.64;11,117.96,640.01,386.53,8.64;11,117.96,651.19,161.89,8.00" xml:id="b7">
	<monogr>
		<title level="m" type="main" coords="11,205.63,640.01,246.93,8.64">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName coords=""><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1504.00325"/>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server, 2015. URL https://arxiv.org/abs/1504.00325.</note>
</biblStruct>

<biblStruct coords="11,108.00,670.73,396.00,8.64;11,117.96,681.51,386.03,8.82;11,117.96,692.47,57.83,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coords="11,482.56,670.73,21.44,8.64;11,117.96,681.69,196.72,8.64">Deep reinforcement learning from human preferences</title>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Paul F Christiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miljan</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shane</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dario</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,325.43,681.51,178.56,8.59;11,117.96,692.47,28.81,8.59">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 2017.</note>
</biblStruct>

<biblStruct coords="11,108.00,712.24,396.00,8.82;11,117.96,723.20,211.35,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coords="11,169.24,712.42,265.01,8.64">Efficient selectivity and backup operators in monte-carlo tree search</title>
		<author>
			<persName coords=""><forename type="first">Rémi</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,452.42,712.24,51.58,8.59;11,117.96,723.20,142.90,8.59">International conference on computers and games</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games. Springer, 2006.</note>
</biblStruct>

<biblStruct coords="12,108.00,85.34,397.74,8.64;12,117.96,96.12,386.04,8.82;12,117.71,107.08,58.94,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coords="12,117.96,96.30,201.63,8.64">Mind2web: Towards a generalist agent for the web</title>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boyuan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shijie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boshi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="12,327.87,96.12,176.13,8.59;12,117.71,107.08,29.78,8.59">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems, 2024.</note>
</biblStruct>

<biblStruct coords="12,108.00,126.59,396.00,8.64;12,117.96,137.55,242.10,8.64" xml:id="b11">
	<monogr>
		<title level="m" type="main" coords="12,398.27,126.59,105.72,8.64;12,117.96,137.55,212.98,8.64">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.</note>
</biblStruct>

<biblStruct coords="12,108.00,156.87,397.75,8.64;12,117.65,167.83,325.14,8.64" xml:id="b12">
	<monogr>
		<title level="m" type="main" coords="12,117.65,167.83,107.87,8.64">The llama 3 herd of models</title>
		<author>
			<persName coords=""><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.21783"/>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, and et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.</note>
</biblStruct>

<biblStruct coords="12,108.00,187.16,396.00,8.64;12,117.96,198.12,386.04,8.64;12,117.96,208.90,255.12,8.82" xml:id="b13">
	<monogr>
		<title level="m" type="main" coords="12,275.63,198.12,228.37,8.64;12,117.96,209.08,87.49,8.64">A comprehensive evaluation benchmark for multimodal large language models</title>
		<author>
			<persName coords=""><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yulei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mengdan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinrui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiawu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.13394</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.</note>
</biblStruct>

<biblStruct coords="12,108.00,228.41,396.36,8.64;12,117.96,239.37,386.04,8.64;12,117.63,250.15,288.55,8.82" xml:id="b14">
	<analytic>
		<title level="a" type="main" coords="12,446.65,228.41,57.71,8.64;12,117.96,239.37,367.49,8.64">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="12,117.63,250.15,258.79,8.59">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.</note>
</biblStruct>

<biblStruct coords="12,108.00,269.66,397.24,8.64;12,117.96,280.62,386.04,8.64;12,117.96,291.40,224.31,8.82" xml:id="b15">
	<monogr>
		<title level="m" type="main" coords="12,210.28,280.62,293.72,8.64;12,117.96,291.57,57.15,8.64">Is your llm secretly a world model of the internet? model-based planning for web agents</title>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boyuan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boyu</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanjari</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.06559</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yu Gu, Boyuan Zheng, Boyu Gou, Kai Zhang, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, and Yu Su. Is your llm secretly a world model of the internet? model-based planning for web agents. arXiv preprint arXiv:2411.06559, 2024.</note>
</biblStruct>

<biblStruct coords="12,108.00,310.90,397.75,8.64;12,117.96,321.68,386.03,8.82;12,117.96,332.64,100.17,8.82" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Xinyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><forename type="middle">Lyna</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ning</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Youran</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mao</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.04519</idno>
		<title level="m" coords="12,117.96,321.86,322.18,8.64">rstar-math: Small llms can master math reasoning with self-evolved deep thinking</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025.</note>
</biblStruct>

<biblStruct coords="12,108.00,352.15,397.74,8.64;12,117.96,362.93,303.46,8.82" xml:id="b17">
	<monogr>
		<title level="m" type="main" coords="12,117.96,363.11,245.77,8.64">Reasoning with language model is planning with world model</title>
		<author>
			<persName coords=""><forename type="first">Shibo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haodi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><forename type="middle">Jiahua</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daisy</forename><forename type="middle">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv, 2023.</note>
</biblStruct>

<biblStruct coords="12,108.00,382.26,380.79,8.82" xml:id="b18">
	<analytic>
		<title level="a" type="main" coords="12,169.39,382.44,172.77,8.64">Mechanical reasoning by mental simulation</title>
		<author>
			<persName coords=""><forename type="first">Mary</forename><surname>Hegarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="12,349.79,382.26,110.13,8.59">Trends in cognitive sciences</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mary Hegarty. Mechanical reasoning by mental simulation. Trends in cognitive sciences, 2004.</note>
</biblStruct>

<biblStruct coords="12,108.00,401.77,397.65,8.64;12,117.96,412.73,387.28,8.64;12,117.96,423.51,387.28,8.82;12,117.71,434.65,46.04,8.64" xml:id="b19">
	<analytic>
		<title level="a" type="main" coords="12,225.94,412.73,167.87,8.64">Teaching machines to read and comprehend</title>
		<author>
			<persName coords=""><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coords="12,295.63,423.51,205.40,8.59">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- leyman, and Phil Blunsom. Teaching machines to read and comprehend. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28.</note>
</biblStruct>

<biblStruct coords="12,166.82,434.65,337.68,8.64;12,117.96,445.82,321.79,8.00" xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Inc</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf"/>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper_ files/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf.</note>
</biblStruct>

<biblStruct coords="12,108.00,464.93,396.00,8.64;12,117.49,475.89,387.75,8.64;12,117.96,486.85,22.42,8.64" xml:id="b21">
	<monogr>
		<title level="m" type="main" coords="12,305.43,475.89,195.89,8.64">Cogagent: A visual language model for gui agents</title>
		<author>
			<persName coords=""><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qingsong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenmeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junhui</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2023.</note>
</biblStruct>

<biblStruct coords="12,108.00,506.18,396.00,8.64;12,117.96,516.96,386.04,8.82;12,117.30,527.92,58.94,8.82" xml:id="b22">
	<analytic>
		<title level="a" type="main" coords="12,296.61,506.18,207.39,8.64;12,117.96,517.14,151.07,8.64">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="12,277.00,516.96,227.00,8.59;12,117.30,527.92,29.17,8.59">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. Conference on Computer Vision and Pattern Recognition (CVPR), 2019.</note>
</biblStruct>

<biblStruct coords="12,108.00,547.43,397.24,8.64;12,117.96,558.39,387.28,8.64;12,117.96,569.35,386.04,8.64;12,117.49,580.31,387.50,8.64;12,117.96,591.48,72.23,8.00" xml:id="b23">
	<monogr>
		<title level="m" type="main" coords="12,318.92,580.31,40.34,8.64">Mistral 7b</title>
		<author>
			<persName coords=""><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florian</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gianna</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucile</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renard</forename><surname>Lélio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marie-Anne</forename><surname>Lavaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Teven</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thibaut</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothée</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">El</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sayed</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2310.06825"/>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/ abs/2310.06825.</note>
</biblStruct>

<biblStruct coords="12,108.00,610.41,384.18,8.82" xml:id="b24">
	<analytic>
		<title level="a" type="main" coords="12,179.37,610.59,239.80,8.64">On the computational complexity of combinatorial problems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Karp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="12,427.15,610.41,35.67,8.59">Networks</title>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Richard M Karp. On the computational complexity of combinatorial problems. Networks, 1975.</note>
</biblStruct>

<biblStruct coords="12,108.00,629.92,397.74,8.64;12,117.60,640.70,386.40,8.82;12,117.96,651.66,330.41,8.82" xml:id="b25">
	<analytic>
		<title level="a" type="main" coords="12,117.60,640.88,347.51,8.64">Agentsimulator: An agent-based approach for data-driven business process simulation</title>
		<author>
			<persName coords=""><forename type="first">Lukas</forename><surname>Kirchdorfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Blümel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timotheus</forename><surname>Kampik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Van Der Aa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="12,483.74,640.70,20.26,8.59;12,117.96,651.66,224.35,8.59">2024 6th International Conference on Process Mining (ICPM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="97" to="104"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Lukas Kirchdorfer, Robert Blümel, Timotheus Kampik, Han Van der Aa, and Heiner Stuckenschmidt. Agentsimulator: An agent-based approach for data-driven business process simulation. In 2024 6th International Conference on Process Mining (ICPM), pp. 97-104. IEEE, 2024.</note>
</biblStruct>

<biblStruct coords="12,108.00,671.17,396.00,8.64;12,117.96,682.13,386.04,8.64;12,117.96,692.91,358.25,8.82" xml:id="b26">
	<monogr>
		<title level="m" type="main" coords="12,388.13,682.13,115.88,8.64;12,117.96,693.09,186.93,8.64">Visualwebarena: Evaluating multimodal agents on realistic visual web tasks</title>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vikram</forename><surname>Duvvur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><forename type="middle">Chong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Po-Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.13649</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024a.</note>
</biblStruct>

<biblStruct coords="12,108.00,712.42,396.00,8.64;12,117.96,723.20,113.44,8.82" xml:id="b27">
	<monogr>
		<title level="m" type="main" coords="12,404.68,712.42,99.31,8.64;12,117.96,723.38,51.35,8.64">Tree search for language model agents</title>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Mcaleer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents. arXiv, 2024b.</note>
</biblStruct>

<biblStruct coords="13,108.00,85.34,396.00,8.64;13,117.96,96.30,248.50,8.64" xml:id="b28">
	<monogr>
		<title level="m" type="main" coords="13,386.88,85.34,117.12,8.64;13,117.96,96.30,26.81,8.64">Reward design with language models</title>
		<author>
			<persName coords=""><forename type="first">Minae</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalesha</forename><surname>Bullard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dorsa</forename><surname>Sadigh</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2303.00001"/>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models, 2023a. URL https://arxiv.org/abs/2303.00001.</note>
</biblStruct>

<biblStruct coords="13,108.00,115.63,397.75,8.64;13,117.96,126.59,386.04,8.64;13,117.96,137.37,386.03,8.82;13,117.71,148.32,107.37,8.82" xml:id="b29">
	<analytic>
		<title level="a" type="main" coords="13,275.70,126.59,228.31,8.64;13,117.96,137.55,109.38,8.64">Efficient memory management for large language model serving with pagedattention</title>
		<author>
			<persName coords=""><forename type="first">Woosuk</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,245.81,137.37,258.19,8.59;13,117.71,148.32,73.53,8.59">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</title>
		<meeting>the ACM SIGOPS 29th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023b.</note>
</biblStruct>

<biblStruct coords="13,108.00,167.83,396.00,8.64;13,117.96,178.61,318.35,8.82" xml:id="b30">
	<analytic>
		<title level="a" type="main" coords="13,468.42,167.83,35.58,8.64;13,117.96,178.79,162.05,8.64">Building machines that learn and think like people</title>
		<author>
			<persName coords=""><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tomer D Ullman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="13,287.74,178.61,119.70,8.59">Behavioral and brain sciences</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 2017.</note>
</biblStruct>

<biblStruct coords="13,108.00,198.12,396.17,8.64;13,117.96,209.08,386.03,8.64;13,117.96,219.86,101.25,8.82" xml:id="b31">
	<monogr>
		<title level="m" type="main" coords="13,245.91,209.08,258.09,8.64;13,117.96,220.04,43.86,8.64">Rlaif: Scaling reinforcement learning from human feedback with ai feedback</title>
		<author>
			<persName coords=""><forename type="first">Harrison</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samrat</forename><surname>Phatale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hassan</forename><surname>Mansoor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kellie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Colton</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Carbune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv, 2023.</note>
</biblStruct>

<biblStruct coords="13,108.00,239.37,397.75,8.64;13,117.96,250.15,387.69,8.82;13,117.96,261.11,278.08,8.82" xml:id="b32">
	<analytic>
		<title level="a" type="main" coords="13,117.96,250.33,342.08,8.64">Camel: Communicative agents for "mind" exploration of large language model society</title>
		<author>
			<persName coords=""><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hasan</forename><surname>Abed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Al</forename><surname>Kader Hammoud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hani</forename><surname>Itani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dmitrii</forename><surname>Khizbullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,478.12,250.15,27.54,8.59;13,117.96,261.11,248.92,8.59">Thirtyseventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for "mind" exploration of large language model society. In Thirty- seventh Conference on Neural Information Processing Systems, 2023.</note>
</biblStruct>

<biblStruct coords="13,108.00,280.62,397.25,8.64;13,117.96,291.57,372.29,8.64" xml:id="b33">
	<monogr>
		<title level="m" type="main" coords="13,268.62,291.57,192.26,8.64">Vila: On pre-training for visual language models</title>
		<author>
			<persName coords=""><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongxu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023.</note>
</biblStruct>

<biblStruct coords="13,108.00,310.90,397.39,8.64;13,117.96,321.68,387.28,8.82;13,117.96,332.82,22.42,8.64" xml:id="b34">
	<monogr>
		<title level="m" type="main" coords="13,480.53,310.90,24.86,8.64;13,117.96,321.86,248.25,8.64">Qlass: Boosting language agent inference via q-guided stepwise search</title>
		<author>
			<persName coords=""><forename type="first">Zongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2502.02584</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Zongyu Lin, Yao Tang, Xingcheng Yao, Da Yin, Ziniu Hu, Yizhou Sun, and Kai-Wei Chang. Qlass: Boosting language agent inference via q-guided stepwise search. arXiv preprint arXiv:2502.02584, 2025.</note>
</biblStruct>

<biblStruct coords="13,108.00,352.15,397.25,8.64;13,117.96,363.11,386.04,8.64;13,117.96,374.07,387.78,8.64;13,117.60,384.85,324.05,8.82" xml:id="b35">
	<monogr>
		<title level="m" type="main" coords="13,117.60,385.03,153.80,8.64">Agentbench: Evaluating llms as agents</title>
		<author>
			<persName coords=""><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanyu</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hangliang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaiwen</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kejuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shudan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.03688</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. arXiv preprint arXiv: 2308.03688, 2023.</note>
</biblStruct>

<biblStruct coords="13,108.00,404.18,396.00,8.82;13,117.36,415.14,237.69,8.82" xml:id="b36">
	<analytic>
		<title level="a" type="main" coords="13,158.04,404.36,164.51,8.64">Evaluation may be easier than generation</title>
		<author>
			<persName coords=""><forename type="first">Moni</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,341.15,404.18,162.86,8.59;13,117.36,415.14,163.49,8.59">Proceedings of the twenty-eighth annual ACM symposium on Theory of computing</title>
		<meeting>the twenty-eighth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="74" to="83"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Moni Naor. Evaluation may be easier than generation. In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing, pp. 74-83, 1996.</note>
</biblStruct>

<biblStruct coords="13,108.00,434.65,396.00,8.64;13,117.96,445.60,51.19,8.64" xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Josh</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sandhini</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lama</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ahmad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Gpt-4 technical report</note>
	<note type="raw_reference">OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, and et al. Gpt-4 technical report, 2024.</note>
</biblStruct>

<biblStruct coords="13,108.00,464.93,397.25,8.64;13,117.96,475.89,388.53,8.64;13,117.47,487.07,132.00,8.00" xml:id="b38">
	<monogr>
		<title level="m" type="main" coords="13,218.21,475.89,198.02,8.64">Towards a unified agent with foundation models</title>
		<author>
			<persName coords=""><forename type="first">Norman</forename><surname>Di Palo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arunkumar</forename><surname>Byravan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leonard</forename><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Markus</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2307.09668"/>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Norman Di Palo, Arunkumar Byravan, Leonard Hasenclever, Markus Wulfmeier, Nicolas Heess, and Martin Riedmiller. Towards a unified agent with foundation models, 2023. URL https: //arxiv.org/abs/2307.09668.</note>
</biblStruct>

<biblStruct coords="13,108.00,506.18,396.00,8.64;13,117.96,517.14,197.07,8.64" xml:id="b39">
	<monogr>
		<title level="m" type="main" coords="13,452.62,506.18,51.37,8.64;13,117.96,517.14,168.17,8.64">Autonomous evaluation and refinement of digital agents</title>
		<author>
			<persName coords=""><forename type="first">Jiayi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><surname>Tomlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yifei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents, 2024.</note>
</biblStruct>

<biblStruct coords="13,108.00,536.47,396.00,8.64;13,117.96,547.25,386.04,8.82;13,117.96,558.21,134.95,8.82" xml:id="b40">
	<monogr>
		<title level="m" type="main" coords="13,189.32,547.43,282.76,8.64">Agent q: Advanced reasoning and learning for autonomous ai agents</title>
		<author>
			<persName coords=""><forename type="first">Pranav</forename><surname>Putta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edmund</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sumeet</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.07199</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024.</note>
</biblStruct>

<biblStruct coords="13,108.00,577.72,397.25,8.64;13,117.96,588.68,387.78,8.64;13,117.96,599.63,184.31,8.64" xml:id="b41">
	<monogr>
		<title level="m" type="main" coords="13,296.12,588.68,179.97,8.64">Agent planning with world knowledge model</title>
		<author>
			<persName coords=""><forename type="first">Shuofei</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Runnan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2405.14205"/>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Agent planning with world knowledge model, 2024. URL https://arxiv.org/abs/2405.14205.</note>
</biblStruct>

<biblStruct coords="13,108.00,618.96,396.00,8.64;13,117.96,629.74,386.04,8.82;13,117.96,640.70,195.60,8.82" xml:id="b42">
	<analytic>
		<title level="a" type="main" coords="13,142.14,629.92,315.86,8.64">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName coords=""><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,466.09,629.74,37.91,8.59;13,117.96,640.70,166.44,8.59">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 2024.</note>
</biblStruct>

<biblStruct coords="13,108.00,660.21,396.17,8.64;13,117.96,671.17,340.48,8.64" xml:id="b43">
	<monogr>
		<title level="m" type="main" coords="13,382.73,660.21,121.43,8.64;13,117.96,671.17,124.22,8.64">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName coords=""><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1606.05250"/>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text, 2016. URL https://arxiv.org/abs/1606.05250.</note>
</biblStruct>

<biblStruct coords="13,108.00,690.50,397.74,8.64;13,117.96,701.46,386.04,8.64;13,117.47,712.24,386.53,8.82;13,117.71,723.60,57.28,8.00" xml:id="b44">
	<analytic>
		<title level="a" type="main" coords="13,117.96,701.46,369.87,8.64">Sayplan: Grounding large language models using 3d scene graphs for scalable task planning</title>
		<author>
			<persName coords=""><forename type="first">Krishan</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesse</forename><surname>Haviland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sourav</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jad</forename><surname>Abou-Chakra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niko</forename><surname>Suenderhauf</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=wMpOMO0Ss7a"/>
	</analytic>
	<monogr>
		<title level="m" coords="13,117.47,712.24,170.97,8.59">7th Annual Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. In 7th Annual Conference on Robot Learning, 2023. URL https://openreview.net/forum?id= wMpOMO0Ss7a.</note>
</biblStruct>

<biblStruct coords="14,108.00,85.34,396.00,8.64;14,117.96,96.30,310.73,8.64" xml:id="b45">
	<monogr>
		<title level="m" type="main" coords="14,140.92,96.30,70.97,8.64">A generalist agent</title>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.06175"/>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, and et al. A generalist agent, 2022. URL https://arxiv.org/abs/2205.06175.</note>
</biblStruct>

<biblStruct coords="14,108.00,116.19,396.00,8.64;14,117.60,127.14,387.78,8.64;14,117.96,137.92,349.25,8.82" xml:id="b46">
	<monogr>
		<title level="m" type="main" coords="14,457.01,127.14,48.37,8.64;14,117.96,138.10,292.17,8.64">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<author>
			<persName coords=""><forename type="first">Machel</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv, 2024.</note>
</biblStruct>

<biblStruct coords="14,108.00,157.99,397.74,8.64;14,117.60,168.95,387.64,8.64;14,117.96,179.91,22.42,8.64" xml:id="b47">
	<monogr>
		<title level="m" type="main" coords="14,117.60,168.95,383.22,8.64">Agentclinic: a multimodal agent benchmark to evaluate ai in simulated clinical environments</title>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Schmidgall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rojin</forename><surname>Ziaei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carl</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduardo</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Jopling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Moor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor. Agentclinic: a multimodal agent benchmark to evaluate ai in simulated clinical environments, 2024.</note>
</biblStruct>

<biblStruct coords="14,108.00,199.80,396.35,8.64;14,117.96,210.58,326.90,8.82" xml:id="b48">
	<monogr>
		<title level="m" type="main" coords="14,497.02,199.80,7.34,8.64;14,117.96,210.75,269.31,8.64">A critical evaluation of ai feedback for aligning large language models</title>
		<author>
			<persName coords=""><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sedrick</forename><surname>Keh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kushal</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">Archit Sharma, Sedrick Keh, Eric Mitchell, Chelsea Finn, Kushal Arora, and Thomas Kollar. A critical evaluation of ai feedback for aligning large language models. arXiv, 2024.</note>
</biblStruct>

<biblStruct coords="14,108.00,230.64,397.38,8.64;14,117.96,241.42,386.04,8.82;14,117.71,252.38,73.88,8.82" xml:id="b49">
	<analytic>
		<title level="a" type="main" coords="14,464.56,230.64,40.83,8.64;14,117.96,241.60,205.14,8.64">Reflexion: Language agents with verbal reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">Noah</forename><surname>Shinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Federico</forename><surname>Cassano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashwin</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="14,330.53,241.42,173.46,8.59;14,117.71,252.38,29.78,8.59">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.</note>
</biblStruct>

<biblStruct coords="14,108.00,272.45,397.24,8.64;14,117.96,283.41,386.03,8.64;14,117.96,294.19,386.03,8.82;14,117.96,305.14,147.58,8.82" xml:id="b50">
	<analytic>
		<title level="a" type="main" coords="14,262.01,283.41,241.99,8.64;14,117.96,294.37,72.00,8.64">Alfred: A benchmark for interpreting grounded instructions for everyday tasks</title>
		<author>
			<persName coords=""><forename type="first">Mohit</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Winson</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="14,208.20,294.19,295.80,8.59;14,117.96,305.14,43.96,8.59">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10740" to="10749"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10740-10749, 2020.</note>
</biblStruct>

<biblStruct coords="14,108.00,325.21,396.36,8.64;14,117.96,336.17,386.03,8.64;14,117.65,346.95,386.83,8.82;14,117.96,358.31,161.89,8.00" xml:id="b51">
	<analytic>
		<title level="a" type="main" coords="14,170.85,336.17,317.03,8.64">ALFWorld: Aligning Text and Embodied Environments for Interactive Learning</title>
		<author>
			<persName coords=""><forename type="first">Mohit</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc-Alexandre</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.03768"/>
	</analytic>
	<monogr>
		<title level="m" coords="14,117.65,346.95,332.06,8.59">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. URL https://arxiv.org/abs/2010.03768.</note>
</biblStruct>

<biblStruct coords="14,108.00,377.98,397.25,8.64;14,117.96,388.93,386.03,8.64;14,117.96,399.71,296.59,8.82" xml:id="b52">
	<monogr>
		<title level="m" type="main" coords="14,399.79,388.93,104.21,8.64;14,117.96,399.89,239.35,8.64">Mastering chess and shogi by self-play with a general reinforcement learning algorithm</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv, 2017.</note>
</biblStruct>

<biblStruct coords="14,108.00,419.78,396.35,8.64;14,117.96,430.56,374.99,8.82" xml:id="b53">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Charlie</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.03314</idno>
		<title level="m" coords="14,343.87,419.78,160.48,8.64;14,117.96,430.74,207.62,8.64">Scaling llm test-time compute optimally can be more effective than scaling model parameters</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.</note>
</biblStruct>

<biblStruct coords="14,108.00,450.63,397.38,8.64;14,117.96,461.59,386.04,8.64;14,117.60,472.37,388.05,8.82;14,117.96,483.32,271.50,8.82" xml:id="b54">
	<analytic>
		<title level="a" type="main" coords="14,441.08,450.63,64.30,8.64;14,117.96,461.59,231.96,8.64">Trial and error: Exploration-based trajectory optimization of LLM agents</title>
		<author>
			<persName coords=""><forename type="first">Yifan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lin</forename><surname>Yuchen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="14,210.85,472.37,294.80,8.59;14,117.96,483.32,68.12,8.59">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</editor>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error: Exploration-based trajectory optimization of LLM agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computa- tional Linguistics. Association for Computational Linguistics, 2024.</note>
</biblStruct>

<biblStruct coords="14,108.00,503.39,397.25,8.64;14,117.96,514.35,386.04,8.64;14,117.96,525.13,157.88,8.82" xml:id="b55">
	<monogr>
		<title level="m" type="main" coords="14,341.79,514.35,162.21,8.64;14,117.96,525.31,96.96,8.64">Aligning large multimodal models with factually augmented rlhf</title>
		<author>
			<persName coords=""><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shengcao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang-Yan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv, 2023a.</note>
</biblStruct>

<biblStruct coords="14,108.00,545.20,396.00,8.64;14,117.39,555.98,387.86,8.82;14,117.96,567.11,27.40,8.64" xml:id="b56">
	<monogr>
		<title level="m" type="main" coords="14,216.47,556.16,255.95,8.64">Salmon: Self-alignment with principle-following reward models</title>
		<author>
			<persName coords=""><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qinhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenfang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Salmon: Self-alignment with principle-following reward models. arXiv, 2023b.</note>
</biblStruct>

<biblStruct coords="14,108.00,587.00,397.24,8.64;14,117.65,597.96,386.34,8.64;14,117.96,608.74,100.15,8.82" xml:id="b57">
	<monogr>
		<title level="m" type="main" coords="14,314.19,597.96,189.80,8.64;14,117.96,608.92,42.92,8.64">Androidenv: A reinforcement learning platform for android</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philippe</forename><surname>Hamel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anita</forename><surname>Gergely</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gheorghe</forename><surname>Comanici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zafarali</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tyler</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shibl</forename><surname>Mourad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv: A reinforcement learning platform for android. arXiv, 2021.</note>
</biblStruct>

<biblStruct coords="14,108.00,628.81,396.00,8.64;14,117.71,639.77,368.56,8.64" xml:id="b58">
	<monogr>
		<title level="m" type="main" coords="14,436.95,628.81,67.04,8.64;14,117.71,639.77,147.29,8.64">Scienceworld: Is your agent smarter than a 5th grader?</title>
		<author>
			<persName coords=""><forename type="first">Ruoyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc-Alexandre</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prithviraj</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2203.07540"/>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader?, 2022a. URL https://arxiv.org/abs/2203.07540.</note>
</biblStruct>

<biblStruct coords="14,108.00,659.65,397.65,8.64;14,117.96,670.61,387.77,8.64;14,117.96,681.39,164.56,8.82" xml:id="b59">
	<monogr>
		<title level="m" type="main" coords="14,208.21,670.61,293.05,8.64">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName coords=""><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh- ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.</note>
</biblStruct>

<biblStruct coords="14,108.00,701.46,396.35,8.64;14,117.96,712.24,386.03,8.82;14,117.96,723.20,180.94,8.82" xml:id="b60">
	<analytic>
		<title level="a" type="main" coords="14,166.90,712.42,280.67,8.64">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="14,455.63,712.24,48.37,8.59;14,117.96,723.20,151.92,8.59">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 2022.</note>
</biblStruct>

<biblStruct coords="15,108.00,85.34,397.24,8.64;15,117.96,96.30,387.78,8.64;15,117.96,107.26,184.31,8.64" xml:id="b61">
	<monogr>
		<title level="m" type="main" coords="15,191.69,96.30,284.33,8.64">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.11903"/>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903.</note>
</biblStruct>

<biblStruct coords="15,108.00,126.19,397.25,8.64;15,117.96,137.14,387.78,8.64;15,117.96,147.92,49.53,8.82" xml:id="b62">
	<monogr>
		<title level="m" type="main" coords="15,179.98,137.14,321.58,8.64">Watch every step! llm agent learning via iterative step-level process refinement</title>
		<author>
			<persName coords=""><forename type="first">Weimin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yifan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiutian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, and Sujian Li. Watch every step! llm agent learning via iterative step-level process refinement. arXiv, 2024.</note>
</biblStruct>

<biblStruct coords="15,108.00,167.03,396.00,8.64;15,117.96,177.99,386.20,8.64;15,117.96,188.77,201.18,8.82" xml:id="b63">
	<monogr>
		<title level="m" type="main" coords="15,295.90,177.99,208.27,8.64;15,117.96,188.95,144.04,8.64">Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation</title>
		<author>
			<persName coords=""><forename type="first">An</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wanrong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiwu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv, 2023.</note>
</biblStruct>

<biblStruct coords="15,108.00,207.88,397.74,8.64;15,117.96,218.66,297.03,8.82" xml:id="b64">
	<monogr>
		<title level="m" type="main" coords="15,117.96,218.84,239.45,8.64">React: Synergizing reasoning and acting in language models</title>
		<author>
			<persName coords=""><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv, 2022.</note>
</biblStruct>

<biblStruct coords="15,108.00,237.77,396.00,8.64;15,117.96,248.73,387.03,8.64;15,117.96,259.90,72.23,8.00" xml:id="b65">
	<monogr>
		<title level="m" type="main" coords="15,388.16,237.77,115.83,8.64;15,117.96,248.73,236.12,8.64">Webshop: Towards scalable real-world web interaction with grounded language agents</title>
		<author>
			<persName coords=""><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Howard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2207.01206"/>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents, 2023a. URL https://arxiv.org/ abs/2207.01206.</note>
</biblStruct>

<biblStruct coords="15,108.00,278.61,396.25,8.64;15,117.96,289.57,387.78,8.64;15,117.96,300.53,184.31,8.64" xml:id="b66">
	<monogr>
		<title level="m" type="main" coords="15,173.55,289.57,297.06,8.64">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName coords=""><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.10601"/>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023b. URL https://arxiv.org/abs/2305.10601.</note>
</biblStruct>

<biblStruct coords="15,108.00,319.46,396.00,8.64;15,117.96,330.42,309.21,8.64" xml:id="b67">
	<monogr>
		<title level="m" type="main" coords="15,434.41,319.46,69.59,8.64;15,117.96,330.42,92.34,8.64">Modeling context in referring expressions</title>
		<author>
			<persName coords=""><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1608.00272"/>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg. Modeling context in referring expressions, 2016. URL https://arxiv.org/abs/1608.00272.</note>
</biblStruct>

<biblStruct coords="15,108.00,349.35,397.74,8.64;15,117.96,360.13,386.04,8.82;15,117.96,371.09,100.17,8.82" xml:id="b68">
	<monogr>
		<title level="m" type="main" coords="15,117.96,360.31,322.25,8.64">Exact: Teaching ai agents to explore with reflective-mcts and exploratory learning</title>
		<author>
			<persName coords=""><forename type="first">Xiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vineeth</forename><surname>Vajipey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.02052</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, and Zhou Yu. Exact: Teaching ai agents to explore with reflective-mcts and exploratory learning. arXiv preprint arXiv:2410.02052, 2024.</note>
</biblStruct>

<biblStruct coords="15,108.00,390.20,396.00,8.64;15,117.49,400.98,220.46,8.82" xml:id="b69">
	<monogr>
		<title level="m" type="main" coords="15,153.21,401.15,127.16,8.64">Self-rewarding language models</title>
		<author>
			<persName coords=""><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">Yuanzhe</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv, 2024.</note>
</biblStruct>

<biblStruct coords="15,108.00,420.08,397.39,8.64;15,117.96,430.86,231.87,8.82" xml:id="b70">
	<monogr>
		<title level="m" type="main" coords="15,453.86,420.08,51.53,8.64;15,117.96,431.04,174.82,8.64">Agenttuning: Enabling generalized agent abilities for llms</title>
		<author>
			<persName coords=""><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingdao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rui</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. arXiv, 2023.</note>
</biblStruct>

<biblStruct coords="15,108.00,449.97,396.00,8.64;15,117.96,460.75,386.03,8.82;15,117.96,471.71,104.60,8.82" xml:id="b71">
	<monogr>
		<title level="m" type="main" coords="15,418.58,449.97,85.42,8.64;15,117.96,460.93,321.37,8.64">Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b</title>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoshui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongzhan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.07394</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint arXiv:2406.07394, 2024a.</note>
</biblStruct>

<biblStruct coords="15,108.00,490.82,396.00,8.64;15,117.96,501.78,386.04,8.64;15,117.96,512.56,84.39,8.82" xml:id="b72">
	<monogr>
		<title level="m" type="main" coords="15,217.04,501.78,286.96,8.64;15,117.96,512.74,26.81,8.64">Building cooperative embodied agents modularly with large language models</title>
		<author>
			<persName coords=""><forename type="first">Hongxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weihua</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiaming</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qinhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianmin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language models. arXiv, 2023.</note>
</biblStruct>

<biblStruct coords="15,108.00,531.67,396.00,8.64;15,117.96,542.45,387.71,8.82" xml:id="b73">
	<monogr>
		<title level="m" type="main" coords="15,462.00,531.67,42.00,8.64;15,117.96,542.62,326.11,8.64">Improving reinforcement learning from human feedback with efficient reward model ensemble</title>
		<author>
			<persName coords=""><forename type="first">Shun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenfang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sunli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note type="raw_reference">Shun Zhang, Zhenfang Chen, Sunli Chen, Yikang Shen, Zhiqing Sun, and Chuang Gan. Improving reinforcement learning from human feedback with efficient reward model ensemble. arXiv, 2024b.</note>
</biblStruct>

<biblStruct coords="15,108.00,561.55,396.16,8.64;15,117.96,572.33,387.28,8.82;15,117.96,583.47,22.42,8.64" xml:id="b74">
	<analytic>
		<title level="a" type="main" coords="15,280.32,561.55,223.84,8.64;15,117.96,572.51,95.02,8.64">Large language models as commonsense knowledge for large-scale task planning</title>
		<author>
			<persName coords=""><forename type="first">Zirui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wee</forename><surname>Sun Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="15,230.74,572.33,270.34,8.59">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.</note>
</biblStruct>

<biblStruct coords="15,108.00,602.40,396.00,8.64;15,117.96,613.18,348.64,8.82" xml:id="b75">
	<analytic>
		<title level="a" type="main" coords="15,369.84,602.40,134.16,8.64;15,117.96,613.36,70.80,8.64">Gpt-4v(ision) is a generalist web agent, if grounded</title>
		<author>
			<persName coords=""><forename type="first">Boyuan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boyu</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jihyung</forename><surname>Kil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="15,207.60,613.18,229.76,8.59">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is a generalist web agent, if grounded. In Forty-first International Conference on Machine Learning, 2024.</note>
</biblStruct>

<biblStruct coords="15,108.00,632.29,397.25,8.64;15,117.39,643.25,386.62,8.64;15,117.96,654.03,370.43,8.82;18,147.30,175.34,52.57,8.64;18,222.47,175.34,17.43,8.64;18,275.65,175.34,17.43,8.64;18,334.24,175.34,17.43,8.64;18,388.00,175.34,17.43,8.64;18,436.48,175.34,17.43,8.64;18,149.79,186.30,47.59,8.64;18,222.47,186.30,17.43,8.64;18,275.65,186.30,17.43,8.64;18,334.24,186.30,17.43,8.64;18,388.00,186.30,17.43,8.64;18,436.48,186.30,17.43,8.64;18,151.72,197.26,43.73,8.64;18,222.47,197.26,17.43,8.64;18,275.65,197.26,17.43,8.64;18,334.24,197.26,17.43,8.64;18,388.00,197.26,17.43,8.64;18,436.48,197.26,17.43,8.64;18,155.73,208.22,35.71,8.64;18,222.48,208.22,17.43,8.64;18,275.66,208.22,17.43,8.64;18,334.25,208.22,17.43,8.64;18,388.00,208.22,17.43,8.64" xml:id="b76">
	<monogr>
		<title level="m" type="main" coords="15,295.24,643.25,208.77,8.64;15,117.96,654.21,75.15,8.64">Webarena: A realistic web environment for building autonomous agents</title>
		<author>
			<persName coords=""><forename type="first">Shuyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abishek</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xianyi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.13854</idno>
		<idno>Phi-3.8B 20.0 19.5 16.7 17.9</idno>
		<ptr target="https://webarena.dev.LLaMA-70B57.361.244.3"/>
		<imprint>
			<date type="published" when="0150">2023. 52.1 50.6 LLaMA-8B 35.7 34.3 26.0 31.4 29.3 Mistral-7B 24.5 26.0 19.5 22.6 21</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. URL https://webarena.dev. LLaMA-70B 57.3 61.2 44.3 52.1 50.6 LLaMA-8B 35.7 34.3 26.0 31.4 29.3 Mistral-7B 24.5 26.0 19.5 22.6 21.7 Phi-3.8B 20.0 19.5 16.7 17.9</note>
</biblStruct>

<biblStruct coords="18,445.19,208.22,8.72,8.64;18,119.20,231.81,370.52,8.64;18,153.66,266.33,39.84,8.64;18,211.82,266.33,252.88,8.64;18,147.30,282.49,52.57,8.64;18,222.47,282.49,17.43,8.64;18,275.65,282.49,17.43,8.64;18,334.24,282.49,17.43,8.64;18,388.00,282.49,17.43,8.64;18,436.48,282.49,17.43,8.64;18,149.79,293.45,47.59,8.64;18,222.47,293.45,17.43,8.64;18,275.65,293.45,17.43,8.64;18,334.24,293.45,17.43,8.64;18,388.00,293.45,17.43,8.64;18,436.48,293.45,17.43,8.64;18,151.72,304.41,43.73,8.64;18,222.47,304.41,17.43,8.64;18,275.65,304.41,17.43,8.64;18,334.24,304.41,17.43,8.64;18,388.00,304.41,17.43,8.64;18,436.48,304.41,17.43,8.64;18,155.73,315.37,35.71,8.64;18,222.48,315.37,17.43,8.64;18,275.66,315.37,17.43,8.64;18,334.25,315.37,17.43,8.64;18,388.00,315.37,17.43,8.64" xml:id="b77">
	<analytic>
		<title level="a" type="main" coords="18,449.55,208.22,4.36,8.64;18,119.20,231.81,370.52,8.64;18,153.66,266.33,39.84,8.64;18,211.82,266.33,231.58,8.64">9 Table 8: Comparison of reward model selection and data demands on ScienceWorld seen set. Backbone VILA-3B VILA-13B LLaVA-13B 1/5 Data 1/25</title>
		<idno>-8B 28.1 27.5 22.2 26.8 24.2 Mistral-7B 21.1 22.9 19.2 21.6 19.7 Phi-3.8B 17.0 15.3 13.7 14.2</idno>
	</analytic>
	<monogr>
		<title level="j" coords="18,445.89,266.33,18.81,8.64;18,147.30,282.49,52.57,8.64;18,149.79,293.45,29.74,8.64">Data LLaMA-70B</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">0 60</biblScope>
		</imprint>
	</monogr>
	<note>LLaMA</note>
	<note type="raw_reference">9 Table 8: Comparison of reward model selection and data demands on ScienceWorld seen set. Backbone VILA-3B VILA-13B LLaVA-13B 1/5 Data 1/25 Data LLaMA-70B 57.0 60.7 48.2 50.0 47.7 LLaMA-8B 28.1 27.5 22.2 26.8 24.2 Mistral-7B 21.1 22.9 19.2 21.6 19.7 Phi-3.8B 17.0 15.3 13.7 14.2</note>
</biblStruct>

<biblStruct coords="18,445.19,315.37,8.72,8.64;18,115.76,338.96,380.48,8.64;31,225.75,204.17,27.22,6.25;31,255.51,179.10,12.12,17.26;31,111.22,177.74,95.51,9.41;31,111.22,188.84,113.00,9.40;31,111.22,199.92,17.44,9.40;31,110.78,215.20,49.75,9.41;31,192.58,215.20,3.21,9.41" xml:id="b78">
	<analytic>
		<title level="a" type="main" coords="18,449.55,315.37,4.36,8.64;18,115.76,338.96,377.09,8.64;31,225.75,204.17,27.22,6.25;31,255.51,179.10,12.12,17.26;31,111.22,177.74,95.51,9.41;31,111.22,188.84,41.29,9.40">7 Table 9: Comparison of reward model selection and data demands on ScienceWorld unseen set</title>
	</analytic>
	<monogr>
		<title level="j" coords="31,225.75,204.17,27.22,6.25">B</title>
		<imprint/>
	</monogr>
	<note>07L2L64XL ... Action1: search[fluoride free toothpaste 3. .5 oz pack of 4 price &lt; 50.00Action2: click</note>
	<note type="raw_reference">7 Table 9: Comparison of reward model selection and data demands on ScienceWorld unseen set. B07L2L64XL ... Action1: search[fluoride free toothpaste 3.5 oz pack of 4 price &lt; 50.00Action2: click[]</note>
</biblStruct>

<biblStruct coords="31,121.39,143.45,368.70,10.96;31,229.54,154.24,152.33,10.95;31,269.88,203.96,28.55,6.25;31,112.39,251.37,80.74,9.40;31,112.77,287.90,71.10,12.55;31,425.23,203.59,29.31,6.25;31,452.93,178.53,12.12,17.25;31,310.70,177.17,110.03,9.40;31,310.70,188.26,107.75,9.40;31,310.70,199.34,41.53,9.40;31,310.26,214.63,94.90,9.40;31,465.29,203.57,28.17,6.25;31,311.41,267.46,79.87,9.40;31,311.16,252.16,80.74,9.40;31,311.32,287.36,76.61,12.55;31,256.73,213.19,11.17,5.50;31,256.73,218.58,13.22,5.50;31,256.73,223.96,9.16,5.50;31,260.42,229.35,40.37,5.50;31,260.42,234.74,40.37,5.50;31,256.92,250.14,11.17,5.50;31,256.92,255.53,13.22,5.50;31,256.92,260.92,9.16,5.50;31,260.61,266.30,40.37,5.50;31,260.61,271.69,40.37,5.50;32,110.57,313.17,69.19,9.31;32,110.57,350.22,90.51,9.31;32,122.02,269.14,368.70,10.84;32,118.37,279.81,375.98,10.83;32,218.32,290.48,176.11,10.83;32,110.57,373.85,69.21,9.30;32,110.57,384.81,190.05,9.30;32,110.57,395.78,106.44,9.30;32,110.57,448.45,173.81,9.30;32,110.57,459.15,184.51,6.97;32,110.57,467.37,42.44,6.97;32,110.57,490.35,70.35,12.41;32,308.46,489.66,75.77,12.41;32,411.70,498.87,72.82,12.41;32,217.75,499.01,69.05,12.41;32,110.57,327.48,187.54,9.30;32,110.57,338.45,181.87,9.30;32,110.57,361.97,138.40,9.30;32,110.57,408.93,188.57,9.30;32,110.57,419.63,182.63,6.97;32,110.57,427.85,26.13,6.97;32,163.89,427.85,65.73,6.97;32,110.57,435.52,141.06,9.30;32,110.57,474.72,155.49,9.30;32,311.67,313.37,69.21,9.30;32,311.67,350.43,90.48,9.30;32,311.67,374.04,69.21,9.30;32,311.67,385.00,190.05,9.30;32,311.67,395.97,106.44,9.30;32,311.67,448.64,182.57,9.31;32,311.67,459.35,164.67,6.97;32,311.67,327.68,187.54,9.30;32,311.67,338.64,181.87,9.30;32,311.67,362.16,138.40,9.30;32,311.67,409.12,188.57,9.30;32,311.67,419.82,182.63,6.97;32,311.67,428.04,119.05,6.97" xml:id="b79">
	<analytic>
		<title level="a" type="main" coords="31,121.39,143.45,368.70,10.96;31,229.54,154.24,148.68,10.95;31,310.70,177.17,110.03,9.40;31,310.70,188.26,91.74,9.40;31,401.37,214.63,3.79,9.40;31,465.29,203.57,28.17,6.25;31,311.41,267.46,47.39,9.40">Instruction: I need a fluoride free toothpaste for fresh breath. I will need a pack of 4 in 3.5 ounce size, and price lower than 50.00 dollars</title>
	</analytic>
	<monogr>
		<title level="j" coords="31,269.88,203.96,28.55,6.25;31,112.39,251.37,25.40,9.40">B08DR4SDLJ Action3</title>
		<imprint/>
	</monogr>
	<note>] B07L2LN685 Action4: click. Buy Now] Action3: click[pack of 4] GT Reward = 0.75 Price: $12.69 Size: 3.5 ounce (pack of 2) 3.5 ounce (pack of 4) Price: $12.69 Size: 3.5 ounce (pack of 2) 3.5 ounce (pack of 4) Action1: look around Action2: teleport to outside Instruction: Your task is to find the animal with the longest life span, then the shortest life span. First, focus on the animal with the longest life span. Then, focus on the animal with the shortest life span. The animals are in the 'outside' location. Action3: look around Observation3: Here you see: … an axe, a baby chipmunk, a baby dragonfly … a parrot egg Action5: focus on baby dragonfly (Thought: Dragonflies are known to have a relatively short life span, typically ranging from a few weeks to a few months.) GT Reward = .0 GT Reward = 0.83 ARMAP Selected ARMAP Aborted Observation1: This room is called the art studio. In it, you see: the agent, a substance called air, a large cupboard. Observation2: You teleport to the outside. Action4: focus on parrot egg (Thought: A parrot is a type of bird, and birds are known to live for many years. The life span of a parrot can be quite long, often 30 years in some species. ) Observation4: You focus on the parrot egg. Observation5: You focus on the baby dragonfly. Action1: look around Action2: teleport to outside Action3: look around Observation3: Here you see: … an axe, a baby chipmunk, a baby dragonfly … a parrot egg Action5: focus on baby chipmunk (Thought: I need to focus on the animal with the shortest life span, which is the baby chipmunk.) Observation1: This room is called the art</note>
	<note type="raw_reference">Instruction: I need a fluoride free toothpaste for fresh breath. I will need a pack of 4 in 3.5 ounce size, and price lower than 50.00 dollars. B08DR4SDLJ Action3: click[pack of 4] GT Reward = .0 B08DQS3T87 ... Action1: search[toothpaste fresh breath fluoride free 4 pack 3.5 oz price &lt; 50.00Action2: click[B08DQS3T87] B07L2LN685 Action4: click[Buy Now] Action3: click[pack of 4] GT Reward = 0.75 Price: $12.69 Size: 3.5 ounce (pack of 2) 3.5 ounce (pack of 4) Price: $12.69 Size: 3.5 ounce (pack of 2) 3.5 ounce (pack of 4) Action1: look around Action2: teleport to outside Instruction: Your task is to find the animal with the longest life span, then the shortest life span. First, focus on the animal with the longest life span. Then, focus on the animal with the shortest life span. The animals are in the 'outside' location. Action3: look around Observation3: Here you see: … an axe, a baby chipmunk, a baby dragonfly … a parrot egg Action5: focus on baby dragonfly (Thought: Dragonflies are known to have a relatively short life span, typically ranging from a few weeks to a few months.) GT Reward = .0 GT Reward = 0.83 ARMAP Selected ARMAP Aborted Observation1: This room is called the art studio. In it, you see: the agent, a substance called air, a large cupboard. Observation2: You teleport to the outside. Action4: focus on parrot egg (Thought: A parrot is a type of bird, and birds are known to live for many years. The life span of a parrot can be quite long, often 30 years in some species. ) Observation4: You focus on the parrot egg. Observation5: You focus on the baby dragonfly. Action1: look around Action2: teleport to outside Action3: look around Observation3: Here you see: … an axe, a baby chipmunk, a baby dragonfly … a parrot egg Action5: focus on baby chipmunk (Thought: I need to focus on the animal with the shortest life span, which is the baby chipmunk.) Observation1: This room is called the art studio. In it, you see: the agent, a substance called air, a large cupboard. Observation2: You teleport to the outside. Action4: focus on parrot egg (Thought: A parrot is a type of bird, and birds are known to live for many years. The life span of a parrot can be quite long, often exceeding 30 years in some species. )</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>