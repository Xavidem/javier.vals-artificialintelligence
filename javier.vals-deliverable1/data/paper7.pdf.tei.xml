<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coords="1,76.01,79.85,442.74,12.90;1,251.62,95.79,91.57,12.90">A Survey on Bridging EEG Signals and Generative AI: From Image and Text to Beyond</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-17">17 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,81.93,133.82,71.96,10.75"><forename type="first">Shreya</forename><surname>Shukla</surname></persName>
							<email>shreya.shukla@utexas.edu</email>
							<affiliation key="aff0" coords="1,174.53,148.23,252.19,10.37">
								<note type="raw_affiliation">School of Information , University of Texas at Austin</note>
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,162.44,133.82,55.75,10.75"><forename type="first">Jose</forename><surname>Torres</surname></persName>
							<affiliation key="aff0" coords="1,174.53,148.23,252.19,10.37">
								<note type="raw_affiliation">School of Information , University of Texas at Austin</note>
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.34,133.82,73.97,10.75"><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
							<email>abhijitmishra@utexas.edu</email>
							<affiliation key="aff0" coords="1,174.53,148.23,252.19,10.37">
								<note type="raw_affiliation">School of Information , University of Texas at Austin</note>
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.03,133.82,74.75,10.75"><forename type="first">Jacek</forename><surname>Gwizdka</surname></persName>
							<email>jacekg@utexas.edu</email>
							<affiliation key="aff0" coords="1,174.53,148.23,252.19,10.37">
								<note type="raw_affiliation">School of Information , University of Texas at Austin</note>
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,395.88,133.82,126.44,10.75"><forename type="first">Shounak</forename><surname>Roychowdhury</surname></persName>
							<email>shounak.roychowdhury@utexas.edu</email>
							<affiliation key="aff0" coords="1,174.53,148.23,252.19,10.37">
								<note type="raw_affiliation">School of Information , University of Texas at Austin</note>
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coords="1,76.01,79.85,442.74,12.90;1,251.62,95.79,91.57,12.90">A Survey on Bridging EEG Signals and Generative AI: From Image and Text to Beyond</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-17">17 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">C3781AF70781C8C3E47205259D6A9182</idno>
					<idno type="arXiv">arXiv:2502.12048v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-02-18T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,87.87,233.56,184.25,8.64;1,87.55,245.52,184.58,8.64;1,87.55,257.47,186.24,8.64;1,87.87,269.43,185.90,8.64;1,87.87,281.38,185.90,8.64;1,87.87,293.34,80.68,8.64">Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) has opened new frontiers in brain signal decoding, enabling assistive communication, neural representation learning, and multimodal integration.</s><s coords="1,173.80,293.34,98.33,8.64;1,87.87,305.29,185.90,8.64;1,87.63,317.25,186.15,8.64;1,87.87,329.20,149.74,8.64">BCIs, particularly those leveraging Electroencephalography (EEG), provide a non-invasive means of translating neural activity into meaningful outputs.</s><s coords="1,243.91,329.20,28.22,8.64;1,87.87,341.16,185.90,8.64;1,87.87,353.11,184.25,8.64;1,87.57,365.07,184.56,8.64;1,87.55,377.02,186.24,8.64;1,87.87,388.98,185.99,8.64">Recent advances in deep learning, including Generative Adversarial Networks (GANs) and Transformer-based Large Language Models (LLMs), have significantly improved EEGbased generation of images, text, and speech.</s><s coords="1,87.57,400.93,184.56,8.64;1,87.87,412.89,185.90,8.64;1,87.87,424.84,185.91,8.64;1,87.87,436.80,184.25,8.64;1,87.55,448.76,186.24,8.64;1,87.87,460.71,184.25,8.64;1,87.87,472.67,185.90,8.64;1,87.87,484.62,16.18,8.64">This paper provides a literature review of the state-of-the-art in EEG-based multimodal generation, focusing on (i) EEG-to-image generation through GANs, Variational Autoencoders (VAEs), and Diffusion Models, and (ii) EEG-totext generation leveraging Transformer based language models and contrastive learning methods.</s><s coords="1,107.18,484.62,166.61,8.64;1,87.87,496.40,184.25,8.82;1,87.87,508.53,79.15,8.64">Additionally, we discuss the emerging domain of EEG-to-speech synthesis, an evolving multimodal frontier.</s><s coords="1,170.11,508.53,103.26,8.64;1,87.87,520.49,185.90,8.64;1,87.87,532.44,185.90,8.64;1,87.87,544.40,38.50,8.64">We highlight key datasets, use cases, challenges, and EEG feature encoding methods that underpin generative approaches.</s><s coords="1,129.47,544.40,143.01,8.64;1,87.87,556.35,184.25,8.64;1,87.87,568.31,185.90,8.64;1,87.87,580.26,185.90,8.64;1,87.87,592.22,184.25,8.64;1,87.87,604.17,119.80,8.64">By providing a structured overview of EEG-based generative AI, this survey aims to equip researchers and practitioners with insights to advance neural decoding, enhance assistive technologies, and expand the frontiers of brain-computer interaction.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" coords="1,70.87,620.89,154.96,10.75">Introduction &amp; Motivation</head><p><s coords="1,70.53,641.98,218.61,9.46;1,70.51,655.53,218.63,9.46;1,70.51,669.08,220.44,9.46;1,70.87,682.63,220.08,9.46;1,70.87,696.18,29.98,9.46">The convergence of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) is transforming human-computer interaction by enabling direct brain-to-device communication.</s><s coords="1,106.48,696.18,184.46,9.46;1,70.87,709.73,218.27,9.46;1,70.47,723.27,218.66,9.46;1,70.87,736.82,218.27,9.46;1,70.87,750.37,170.94,9.46">These advancements have enabled applications in assistive communication for individuals with disabilities, cognitive neuroscience, mental health assessment, augmented reality (AR)/virtual reality (VR), and neural art generation.</s><s coords="1,245.19,750.37,45.75,9.46;1,70.87,763.92,218.27,9.46;1,306.14,216.03,218.27,9.88;1,306.14,229.58,219.00,9.81;1,306.14,243.13,218.27,9.81;1,306.14,256.68,218.45,9.88;1,306.14,270.65,182.58,9.46">Electroencephalography (EEG), a widely used non-invasive neural recording technique, enables both passive and active Brain-Computer Interfaces (BCIs) and holds potential for applications in real-time adaptive human-computer interaction <ref type="bibr" coords="1,489.41,257.10,35.18,9.46;1,306.14,270.65,53.38,9.46" target="#b73">(Zander et al., 2010;</ref><ref type="bibr" coords="1,362.71,270.65,121.21,9.46" target="#b69">Wolpaw and Boulay, 2010)</ref>.</s><s coords="1,493.51,270.65,30.90,9.46;1,306.14,284.20,220.08,9.46;1,306.14,297.75,218.27,9.46;1,306.14,311.30,220.08,9.46;1,306.14,324.85,159.97,9.46">Recent advancements in deep learning and generative models have significantly improved the decoding of EEG signals, enabling the translation of neural activity into text, images, and speech.</s><s coords="1,471.16,324.85,54.61,9.46;1,306.14,338.40,218.27,9.46;1,306.14,351.95,218.27,9.46;1,305.80,365.50,220.42,9.46;1,306.14,379.05,218.27,9.46;1,306.14,392.60,218.27,9.46;1,306.14,406.14,219.63,9.46;1,306.14,419.69,113.59,9.46">Specifically, Generative AI, including Generative Adversarial Networks (GANs) <ref type="bibr" coords="1,391.48,351.95,113.70,9.46" target="#b14">(Goodfellow et al., 2014)</ref> and Transformers <ref type="bibr" coords="1,369.12,365.50,98.17,9.46" target="#b64">(Vaswani et al., 2017)</ref>, has significantly advanced brain decoding, facilitating visual reconstruction, language generation, and speech synthesis <ref type="bibr" coords="1,349.38,406.14,73.82,9.46" target="#b3">(Bai et al., 2023;</ref><ref type="bibr" coords="1,425.92,406.14,99.85,9.46;1,306.14,419.69,25.35,9.46" target="#b59">Srivastava and Shinde, 2020;</ref><ref type="bibr" coords="1,335.40,419.69,79.43,9.46">Lee et al., 2023b)</ref>.</s><s coords="1,426.65,419.69,99.57,9.46;1,306.14,433.24,218.27,9.46;1,305.78,446.79,220.43,9.46;1,306.14,460.34,218.26,9.46;1,305.78,473.89,218.63,9.46;1,306.14,487.44,218.27,9.46;1,306.14,500.99,218.27,9.46;1,306.14,514.54,61.51,9.46">GANs improve crosssubject classification and EEG data augmentation <ref type="bibr" coords="1,305.78,446.79,77.77,9.46" target="#b57">(Song et al., 2021)</ref>, while Transformer-based architectures and multimodal deep learning frameworks <ref type="bibr" coords="1,305.78,473.89,83.52,9.46">(Liu et al., 2024a;</ref><ref type="bibr" coords="1,393.44,473.89,90.36,9.46" target="#b67">Wang and Ji, 2022)</ref> enhance EEG-to-text translation and semantic decoding <ref type="bibr" coords="1,507.18,487.44,17.22,9.46;1,306.14,500.99,48.36,9.46" target="#b1">(Ali et al., 2024)</ref>, pushing the boundaries of brain-signal interpretation.</s></p><p><s coords="1,317.05,533.39,207.36,9.66;1,305.48,546.94,218.93,9.66;1,306.14,560.68,218.27,9.46;1,306.14,574.23,139.85,9.46">In light of recent breakthroughs in Generative AI, this survey provides a scope review of recent advancements in EEG-based generative AI, with a focus on two primary directions.</s><s coords="1,449.37,574.23,75.03,9.46;1,306.14,587.59,220.07,9.40;1,306.14,601.14,218.26,9.66;1,306.14,614.88,218.26,9.46;1,306.14,628.43,68.84,9.46">The first explores how brain signals can be used to generate or reconstruct visual stimuli, utilizing models such as GANs and Diffusion Models to decode perceptual representations.</s><s coords="1,378.36,628.43,147.87,9.46;1,306.14,641.78,219.63,9.66;1,305.75,655.53,218.66,9.46;1,305.78,669.08,218.63,9.46;1,306.14,682.63,218.26,9.46;1,306.14,696.18,162.84,9.46">The second investigates the application of deep learning for EEG-to-text translation, where recurrent neural network and Transformers <ref type="bibr" coords="1,305.78,669.08,94.53,9.46" target="#b64">(Vaswani et al., 2017)</ref> based language models, and contrastive learning techniques play a crucial role in learning linguistic representation.</s><s coords="1,474.65,696.18,50.14,9.46;1,306.14,709.53,220.08,9.66;1,306.14,723.08,218.26,9.40;1,306.14,736.63,218.26,9.66;1,305.75,750.37,152.27,9.46">The survey also examines emerging trends in speech decoding from EEG signals and multimodal integration considerations surrounding the use of generative AI for brain signal interpretation.</s><s coords="1,464.75,750.37,61.02,9.46;1,305.75,763.92,218.66,9.46;2,70.87,202.00,220.08,9.46;2,70.87,215.55,218.27,9.46;2,70.87,229.10,220.07,9.46;2,70.87,242.65,93.32,9.46">Through this, we hope to provide a structured understanding of EEG-based generative AI to researchers and practitioners, offering insights to drive innovation in neural decoding, assistive technology, and braincomputer interaction.</s></p><p><s coords="2,81.78,256.54,207.36,9.46;2,70.87,270.09,218.27,9.46;2,70.87,283.64,218.27,9.46;2,70.87,297.19,218.27,9.46;2,70.87,310.74,158.59,9.46">Before proceeding, we would like to highlight that Figure <ref type="figure" coords="2,124.92,270.09,5.56,9.46" target="#fig_0">1</ref> provides a high-level overview of the EEG-to-stimuli generation pipeline, illustrating the key stages from neural data acquisition to the generation of text, images, or audio.</s><s coords="2,232.86,310.74,57.63,9.46;2,70.53,324.29,218.61,9.46;2,70.87,337.84,220.08,9.46;2,70.87,351.39,218.27,9.46;2,70.87,364.94,43.57,9.46">Additionally, Table <ref type="table" coords="2,97.83,324.29,5.56,9.46" target="#tab_2">1</ref> summarizes key datasets that incorporate EEG and other modalities, serving as a valuable resource for researchers exploring multimodal neural decoding.</s><s coords="2,118.45,364.94,170.69,9.46;2,70.87,378.48,218.27,9.46;2,70.87,392.03,220.08,9.46;2,70.87,405.58,200.44,9.46">Finally, Figure <ref type="figure" coords="2,187.02,364.94,5.56,9.46" target="#fig_1">2</ref> centralizes a detailed overview of the techniques and approaches covered in this survey, contextualizing their purpose and applications in EEG-based generative modeling.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" coords="2,70.87,429.38,89.09,10.75">Related Work</head><p><s coords="2,70.87,452.29,220.07,9.46;2,70.87,465.84,220.08,9.46;2,70.87,479.39,125.28,9.46">Several surveys have explored EEG-based braincomputer interfaces (BCIs) and deep learning techniques for neural decoding.</s><s coords="2,203.21,479.39,86.65,9.46;2,70.87,492.94,220.08,9.46;2,70.87,506.49,220.08,9.46;2,70.87,520.04,218.27,9.46;2,70.87,533.59,220.18,9.46"><ref type="bibr" coords="2,203.21,479.39,86.65,9.46" target="#b10">(Chen et al., 2022)</ref> provides a broad overview of EEG-based BCI applications, discussing signal processing methodologies and traditional machine learning techniques but lacks an in-depth analysis of generative models.</s><s coords="2,70.51,547.14,219.02,9.46;2,70.87,560.68,218.27,9.46;2,70.87,574.23,219.63,9.46;2,70.87,587.78,218.26,9.46;2,70.87,601.33,220.08,9.46;2,70.87,614.88,154.75,9.46"><ref type="bibr" coords="2,70.51,547.14,82.56,9.46" target="#b13">(Gong et al., 2021)</ref> and <ref type="bibr" coords="2,174.23,547.14,83.49,9.46" target="#b68">(Weng et al., 2024)</ref> review deep learning techniques, particularly CNNs and RNNs, for EEG classification, emotion recognition, and cognitive state monitoring, yet they focus more on supervised learning approaches rather than selfsupervised and generative modeling.</s><s coords="2,228.80,614.88,61.70,9.46;2,70.51,628.43,218.63,9.46;2,70.87,641.98,218.27,9.46;2,70.87,655.53,220.08,9.46;2,70.87,669.08,218.27,9.46;2,70.47,682.63,159.43,9.46">More recently, (Murad and Rahimi, 2024) examined EEG-to-text decoding but primarily covered classification-based approaches without exploring the role of large language models (LLMs) or multimodal integration with vision-based EEG applications.</s><s coords="2,233.32,682.63,57.17,9.46;2,70.51,696.18,218.63,9.46;2,70.87,709.73,218.27,9.46;2,70.87,723.27,219.63,9.46;2,70.87,736.82,220.17,9.46">Additionally, <ref type="bibr" coords="2,70.51,696.18,100.65,9.46" target="#b61">(Sun and Mou, 2023)</ref> discusses self-supervised learning (SSL) techniques for EEG but does not emphasize their applicability to generative tasks, such as EEG-to-image or EEG-to-text generation.</s><s coords="2,70.87,750.37,218.27,9.46;2,70.87,763.92,220.07,9.46;2,306.14,202.00,220.08,9.46;2,306.14,215.55,19.56,9.46">Our survey specifically examines the intersection of EEG and generative AI, focusing on recent ap-proches for EEG-to-text and EEG-to-image translation.</s><s coords="2,329.09,215.55,195.33,9.46;2,306.14,229.10,218.27,9.46;2,306.14,242.65,219.63,9.46;2,306.14,256.20,218.27,9.46;2,306.14,269.75,48.77,9.46">While previous studies focus on EEG feature extraction and classification, our work highlights how self-supervised learning, contrastive learning, and multimodal alignment improve EEG-based generation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" coords="2,306.14,293.25,153.74,10.75">EEG-to-Image Generation</head><p><s coords="2,305.80,315.94,220.42,9.46;2,306.14,329.49,160.71,9.46">This section explores regenerating images from visually evoked brain signals via EEG.</s><s coords="2,469.57,329.49,54.84,9.46;2,306.14,343.04,219.63,9.46;2,306.14,356.59,220.08,9.46;2,306.14,370.14,144.20,9.46">It covers use cases, concerns addressed, techniques employed, and EEG feature encoding methods for image generation used by surveyed studies.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" coords="2,306.14,392.98,189.63,9.81">Use Cases and Addressed Concerns</head><p><s coords="2,306.14,411.39,218.66,9.46;2,306.14,424.94,219.63,9.46;2,306.14,438.49,218.27,9.46;2,306.14,452.04,220.08,9.46;2,306.14,465.59,220.08,9.46;2,306.14,479.13,218.27,9.46;2,305.78,492.68,218.63,9.46;2,306.14,506.23,57.55,9.46">Surveyed studies address key challenges like low signal-to-noise ratio of EEG signals <ref type="bibr" coords="2,476.27,424.94,49.50,9.46;2,306.14,438.49,24.63,9.46" target="#b3">(Bai et al., 2023;</ref><ref type="bibr" coords="2,333.50,438.49,70.65,9.46" target="#b28">Lan et al., 2023;</ref><ref type="bibr" coords="2,406.89,438.49,79.48,9.46">Zeng et al., 2023a)</ref>, limited information and individual differences in EEG signals <ref type="bibr" coords="2,326.40,465.59,71.50,9.46" target="#b3">(Bai et al., 2023)</ref>, lower performance on natural object images compared to digits and characters <ref type="bibr" coords="2,305.78,492.68,87.62,9.46" target="#b39">(Mishra et al., 2023)</ref> and small dataset sizes <ref type="bibr" coords="2,495.90,492.68,28.51,9.46;2,306.14,506.23,52.76,9.46" target="#b55">(Singh et al., 2023)</ref>.</s><s coords="2,369.65,506.23,154.77,9.46;2,306.14,519.78,219.17,9.46;2,306.14,533.33,218.27,9.46;2,306.14,546.88,33.43,9.46">Additionally, some efforts explore alternatives to supervised learning <ref type="bibr" coords="2,457.77,519.78,67.55,9.46" target="#b32">(Li et al., 2020;</ref><ref type="bibr" coords="2,306.14,533.33,76.35,9.46" target="#b56">Song et al., 2023)</ref>, since it demands large amount of data.</s><s coords="2,343.27,546.88,182.94,9.46;2,306.14,560.43,218.27,9.46;2,306.14,573.98,218.27,9.46;2,306.14,587.53,218.27,9.46;2,306.14,601.08,152.52,9.46"><ref type="bibr" coords="2,343.27,546.88,79.91,9.46" target="#b56">Song et al. (2023)</ref> addresses concerns regarding convolution layers applied separately along temporal and spatial dimensions, which disrupts the correlation between channels and hinders the spatial properties of brain activity.</s><s coords="2,462.59,601.08,61.82,9.46;2,306.14,614.63,220.07,9.46;2,306.14,628.18,219.63,9.46;2,306.14,641.72,28.18,9.46">Overall, these approaches aim to enhance the training, performance, and interpretation of brain data <ref type="bibr" coords="2,484.10,628.18,41.67,9.46;2,306.14,641.72,23.48,9.46" target="#b33">(Li et al., 2024)</ref>.</s><s coords="2,317.05,655.53,207.36,9.46;2,306.14,669.08,220.08,9.46;2,306.14,682.63,218.27,9.46;2,306.14,696.18,220.08,9.46;2,306.14,709.73,218.27,9.46;2,306.14,723.27,218.27,9.46;2,306.14,736.82,218.27,9.46;2,306.14,750.37,54.55,9.46;3,97.53,94.70,59.45,5.17;3,200.79,94.70,10.42,5.17;3,278.90,94.70,8.94,5.17;3,317.44,94.70,122.88,5.17;3,506.70,94.70,5.96,5.17;3,74.84,102.65,70.21,5.17;3,200.79,102.65,10.42,5.17;3,266.53,102.65,33.67,5.17;3,317.44,102.54,126.07,5.28;3,506.70,102.65,5.96,5.17;3,74.84,110.60,95.22,5.17;3,200.79,110.60,41.23,5.17;3,280.39,110.60,5.96,5.17;3,317.44,110.60,107.55,5.17;3,506.70,110.60,5.96,5.17;3,74.84,118.55,63.26,5.17;3,200.79,118.55,14.90,5.17;3,280.39,118.55,5.96,5.17;3,317.44,118.55,145.63,5.17;3,508.19,118.55,2.98,5.17;3,74.84,126.50,95.75,5.17;3,104.79,166.25,55.93,5.17;3,200.79,166.25,42.23,5.17;3,280.39,166.25,5.96,5.17;3,317.44,166.25,173.53,5.17;3,506.70,166.25,5.96,5.17;3,74.84,174.20,74.18,5.17;3,200.79,174.20,14.90,5.17;3,278.90,174.20,8.94,5.17;3,317.44,174.20,79.55,5.17;3,506.70,174.20,5.96,5.17;3,74.84,182.15,78.16,5.17;3,200.79,182.15,14.91,5.17;3,278.90,182.15,8.94,5.17;3,317.44,182.15,126.04,5.17;3,506.70,182.15,5.96,5.17;3,74.84,190.10,81.80,5.17;3,200.79,190.10,14.91,5.17;3,278.90,190.10,8.94,5.17;3,317.44,190.10,89.35,5.17;3,506.70,190.10,5.96,5.17;3,74.84,198.05,94.39,5.17;3,200.79,198.05,48.51,5.17;3,280.39,198.05,5.96,5.17;3,317.44,198.05,124.37,5.17;3,506.70,198.05,5.96,5.17;3,74.84,206.00,104.98,5.17;3,200.79,206.00,14.91,5.17;3,280.39,206.00,5.96,5.17;3,317.44,206.00,106.17,5.17;3,508.19,206.00,2.98,5.17;3,74.84,213.95,106.73,5.17;3,200.79,213.95,14.91,5.17;3,280.39,213.95,5.96,5.17;3,317.44,213.95,140.54,5.17;3,506.70,213.95,5.96,5.17;3,70.51,294.85,220.44,9.46;3,70.87,308.39,218.27,9.46;3,70.51,321.94,220.44,9.46;3,70.87,335.49,218.27,9.46;3,70.87,349.04,175.13,9.46"><ref type="bibr" coords="2,317.05,655.53,99.47,9.46" target="#b23">Kavasidis et al. (2017)</ref>, <ref type="bibr" coords="2,424.40,655.53,80.81,9.46" target="#b56">Song et al. (2023)</ref> and <ref type="bibr" coords="2,306.14,669.08,86.53,9.46" target="#b39">Mishra et al. (2023)</ref> extract class-specific EEG encodings that contain discriminative information to improve image generation quality, while <ref type="bibr" coords="2,497.18,696.18,23.23,9.46;2,306.14,709.73,76.81,9.46" target="#b42">(Nemrodov et al., 2018)</ref> focus on utilizing spatiotemporal EEG information to determine the neural correlates of facial identity representations and <ref type="bibr" coords="2,480.07,736.82,44.34,9.46;2,306.14,750.37,54.55,9.46" target="#b24">(Khaleghi et al., 2022)</ref>   <ref type="bibr" coords="3,97.53,94.70,59.45,5.17" target="#b17">(Hollenstein et al., 2019)</ref> Text 128 Expanded subjects with similar content as Zuco 1.0 18 Alice <ref type="bibr" coords="3,89.25,102.65,55.81,5.17" target="#b5">(Bhattasali et al., 2020)</ref> Text 61 + 1 ground 2,129 words, 84 sentences from Alice in Wonderland 52 Envisioned Speech <ref type="bibr" coords="3,121.63,110.60,48.43,5.17" target="#b27">(Kumar et al., 2018)</ref> Imagined Speech 14 20 text stimuli (digits, characters), 10 objects 23 Alljoined <ref type="bibr" coords="3,98.86,118.55,39.24,5.17" target="#b70">(Xu et al., 2024)</ref> Image 64 10,000 images per participant from 80 MS-COCO categories 8 ImageNet EEG <ref type="bibr" coords="3,112.92,126.50,57.67,5.17" target="#b58">(Spampinato et al., 2017</ref>  <ref type="bibr" coords="3,104.79,166.25,55.93,5.17" target="#b63">(Tirupattur et al., 2018)</ref> Imagined Objects 14 EEG recorded while participants imagined digits, characters, and objects 23 OCED <ref type="bibr" coords="3,92.56,174.20,56.46,5.17" target="#b22">(Kaneshiro et al., 2015)</ref> Image 128 12 images per 6 object categories 10 NMED-T <ref type="bibr" coords="3,99.51,182.15,53.49,5.17" target="#b37">(Losorelli et al., 2017)</ref> Music 128 10 songs (4:30-5:00 mins) with tempos 56-150 BPM 20 NMED-H <ref type="bibr" coords="3,100.18,190.10,56.47,5.17" target="#b21">(Kaneshiro et al., 2016)</ref> Music 125 4 versions of 4 songs, total 16 stimuli 48 KARA ONE <ref type="bibr" coords="3,106.97,198.05,62.26,5.17" target="#b78">(Zhao and Rudzicz, 2015)</ref> Text, Audio, Speech 64 Rest state, stimulus, imagined speech, speaking task 12 Japanese Speech EEG <ref type="bibr" coords="3,129.31,206.00,50.51,5.17" target="#b40">(Mizuno et al., 2024)</ref> Audio 64 503 spoken sentences (male/female speaker) 1 Phrase/Word Speech EEG <ref type="bibr" coords="3,138.78,213.95,42.80,5.17" target="#b46">(Park et al., 2024)</ref> Audio 64 Audio of 13 words/phrases, followed by speech replication 10  <ref type="bibr" coords="3,70.51,294.85,138.01,9.46" target="#b53">(Shimizu and Srinivasan, 2022)</ref>, generating classspecific EEG encodings as latent representations <ref type="bibr" coords="3,70.51,321.94,86.20,9.46" target="#b39">(Mishra et al., 2023)</ref>, and decoding multi-level perceptual information from EEG signals to produce multi-grained outputs <ref type="bibr" coords="3,167.84,349.04,73.47,9.46" target="#b28">(Lan et al., 2023)</ref>.</s></p><p><s coords="3,81.78,363.83,209.17,9.46;3,70.87,377.37,218.27,9.46;3,70.87,390.92,220.08,9.46;3,70.87,404.47,220.08,9.46;3,70.87,418.02,220.08,9.46;3,70.87,431.57,218.27,9.46;3,70.87,445.12,152.83,9.46">Additionally, research efforts focus on enhancing the generalizability of feature extraction pipelines across datasets <ref type="bibr" coords="3,180.26,390.92,81.99,9.46" target="#b54">(Singh et al., 2024)</ref>, evaluating the performance of different channels <ref type="bibr" coords="3,264.17,404.47,22.32,9.46;3,70.87,418.02,72.90,9.46" target="#b60">(Sugimoto et al., 2024)</ref>, and incorporating attention modules to highlight the significance of each channel or frequency band <ref type="bibr" coords="3,152.79,445.12,66.20,9.46" target="#b33">(Li et al., 2024)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" coords="3,70.87,471.35,173.36,9.81">Techniques Used Across Studies</head><p><s coords="3,70.47,491.70,220.47,9.46;3,70.87,505.25,220.18,9.46">Various computer vision generative models are employed to reconstruct images from EEG signals.</s><s coords="3,70.53,518.38,220.42,9.88;3,70.87,531.93,220.08,9.88;3,70.87,545.48,218.27,9.88;3,70.87,559.45,219.63,9.46;3,70.87,573.00,220.08,9.46;3,70.87,586.55,219.63,9.46;3,70.87,600.10,27.61,9.46">These include Variational Autoencoders <ref type="bibr" coords="3,260.55,518.80,25.33,9.46;3,70.87,532.35,79.31,9.46" target="#b23">(Kavasidis et al., 2017;</ref><ref type="bibr" coords="3,154.15,532.35,88.78,9.46" target="#b65">Wakita et al., 2021)</ref>, Generative Adversarial Networks (GANs) <ref type="bibr" coords="3,242.03,545.90,47.10,9.46;3,70.87,559.45,54.07,9.46" target="#b23">(Kavasidis et al., 2017;</ref><ref type="bibr" coords="3,128.47,559.45,98.24,9.46" target="#b24">Khaleghi et al., 2022;</ref><ref type="bibr" coords="3,230.25,559.45,60.24,9.46;3,70.87,573.00,25.15,9.46" target="#b39">Mishra et al., 2023;</ref><ref type="bibr" coords="3,98.73,573.00,80.54,9.46" target="#b54">Singh et al., 2024;</ref><ref type="bibr" coords="3,181.99,573.00,63.20,9.46" target="#b33">Li et al., 2024)</ref>, and conditional GANs <ref type="bibr" coords="3,136.02,586.55,81.41,9.46" target="#b55">(Singh et al., 2023;</ref><ref type="bibr" coords="3,220.00,586.55,70.50,9.46;3,70.87,600.10,23.01,9.46" target="#b0">Ahmadieh et al., 2024)</ref>.</s><s coords="3,101.86,599.67,187.28,9.88;3,70.87,613.65,218.27,9.46;3,70.87,627.20,218.27,9.46;3,70.87,640.75,220.08,9.46;3,70.87,654.30,220.18,9.46">Diffusion models, including prior diffusion models that refine EEG embeddings into image priors <ref type="bibr" coords="3,99.65,627.20,138.50,9.46" target="#b53">(Shimizu and Srinivasan, 2022)</ref>, as well as pre-trained diffusion models such as Stable Diffusion <ref type="bibr" coords="3,93.11,654.30,75.83,9.46" target="#b3">(Bai et al., 2023)</ref>, are also commonly used.</s><s coords="3,70.47,667.84,220.47,9.46;3,70.87,681.39,219.17,9.46;3,70.87,694.94,218.27,9.46;3,70.87,708.49,65.14,9.46">Additionally, diffusion modules based on U-net architecture have been used in <ref type="bibr" coords="3,201.76,681.39,88.28,9.46">(Zeng et al., 2023a;</ref><ref type="bibr" coords="3,70.87,694.94,71.18,9.46" target="#b28">Lan et al., 2023)</ref> to further enhance EEG-to-image reconstruction.</s></p><p><s coords="3,81.78,722.85,209.17,9.88;3,70.87,736.82,218.26,9.46;3,70.87,750.37,218.27,9.46;3,70.87,763.92,220.07,9.46;3,306.14,267.75,218.27,9.46;3,306.14,281.30,220.08,9.46;3,306.14,294.85,112.75,9.46">Contrastive learning is another popular approach to align multimodal embeddings, employed in studies <ref type="bibr" coords="3,113.46,750.37,80.50,9.46" target="#b55">(Singh et al., 2023;</ref><ref type="bibr" coords="3,196.25,750.37,68.61,9.46" target="#b28">Lan et al., 2023;</ref><ref type="bibr" coords="3,267.15,750.37,21.98,9.46;3,70.87,763.92,53.43,9.46" target="#b56">Song et al., 2023;</ref><ref type="bibr" coords="3,127.53,763.92,100.57,9.46" target="#b60">Sugimoto et al., 2024)</ref> to obtain dis-criminative features from EEG signals and align the two modalities by constraining their cosine similarity <ref type="bibr" coords="3,335.02,294.85,79.17,9.46" target="#b56">(Song et al., 2023)</ref>.</s><s coords="3,422.30,294.42,102.11,9.88;3,306.14,307.97,218.27,9.88;3,305.78,321.94,219.99,9.46;3,306.14,335.49,220.08,9.46;3,306.14,349.04,218.27,9.46;3,306.14,362.59,218.27,9.46;3,306.14,376.14,123.35,9.46">Furthermore, attention mechanisms are integrated into various models <ref type="bibr" coords="3,305.78,321.94,93.12,9.46" target="#b39">(Mishra et al., 2023;</ref><ref type="bibr" coords="3,402.56,321.94,80.77,9.46" target="#b56">Song et al., 2023;</ref><ref type="bibr" coords="3,486.97,321.94,38.80,9.46;3,306.14,335.49,24.94,9.46" target="#b33">Li et al., 2024)</ref> to enhance image quality, capture spatial correlations that reflect brain activity inferred from EEG data, and determine the relative importance of individual EEG channels.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" coords="3,306.14,403.04,188.25,9.81">EEG Feature Encoding Techniques</head><p><s coords="3,306.14,423.77,220.08,9.46;3,306.14,437.32,220.08,9.46;3,306.14,450.87,218.27,9.46;3,306.14,464.42,220.07,9.46;3,306.14,477.96,18.23,9.46">In EEG-to-image reconstruction, the process typically begins with an encoder identifying the latent feature space of EEG signals, followed by a decoder that converts these features into an image.</s><s coords="3,332.44,477.96,191.98,9.46;3,306.14,491.51,220.07,9.46;3,306.14,504.64,218.27,9.88;3,306.14,518.61,58.41,9.46">Long Short-Term Memory (LSTM)-based architectures are widely used due to their effectiveness in capturing temporal dependencies in EEG signals.</s><s coords="3,368.99,518.61,155.43,9.46;3,306.14,532.16,220.07,9.46;3,306.14,545.71,218.27,9.46;3,306.14,559.26,102.53,9.46"><ref type="bibr" coords="3,368.99,518.61,101.16,9.46" target="#b23">Kavasidis et al. (2017)</ref> employs an LSTM network to generate a compact and classdiscriminative feature vector, which is also used for object recognition.</s><s coords="3,417.20,559.26,108.57,9.46;3,306.14,572.81,218.27,9.46;3,306.14,585.93,218.26,9.88;3,306.14,599.48,72.03,9.81">Similarly, <ref type="bibr" coords="3,465.57,559.26,60.20,9.46;3,306.14,572.81,25.96,9.46" target="#b55">(Singh et al., 2023)</ref> integrates LSTM with a triplet-loss-based contrastive learning approach to enhance feature discrimination.</s><s coords="3,383.36,599.91,141.05,9.46;3,306.14,613.46,218.27,9.46;3,306.14,627.01,218.26,9.46;3,305.75,640.55,218.66,9.46;3,306.14,654.10,73.57,9.46"><ref type="bibr" coords="3,383.36,599.91,84.48,9.46" target="#b54">Singh et al. (2024)</ref> extends this approach by incorporating both CNN and LSTM architectures trained under EEG label supervision with triplet loss, further improving discriminative feature learning.</s><s coords="3,385.25,654.10,140.53,9.46;3,306.14,667.65,218.27,9.46;3,306.14,681.20,220.07,9.46;3,306.14,694.33,218.27,9.88;3,305.87,708.30,218.54,9.46;3,306.14,721.85,219.08,9.46;3,306.14,735.40,123.44,9.46">Additionally, <ref type="bibr" coords="3,447.20,654.10,78.57,9.46;3,306.14,667.65,25.96,9.46" target="#b0">(Ahmadieh et al., 2024)</ref> uses LSTM to extract EEG features across two dimensions (EEG channels and signal duration) and enhances feature generation through various regression methods, including polynomial regression, neural network regression, and type-1 and type-2 fuzzy regression.</s></p><p><s coords="3,317.05,750.37,209.17,9.46;3,306.14,763.50,220.17,9.88">Several studies also leverage convolutional architectures to capture spatial dependencies in EEG.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,81.57,75.10,45.34,4.22">Techniques (EEG-Text)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,150.37,75.10,53.29,4.19">Studies and main use cases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,81.57,83.53,11.64,4.04">CNNs</head><p><s coords="4,150.37,80.87,363.34,4.04;4,150.37,86.19,242.16,4.04">• Generate medical reports <ref type="bibr" coords="4,202.81,80.87,39.10,4.04" target="#b6">(Biswal et al., 2019)</ref> • Translate user's active intent to text represented by morse code <ref type="bibr" coords="4,368.32,80.87,57.30,4.04" target="#b59">(Srivastava and Shinde, 2020)</ref> • Folded ensemble technique to minimize the computational complexity and solve all the class imbalance issues to enhance the accuracy of text generation (Rathod et al., 2024)</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,81.57,94.44,14.23,4.04">LSTMs</head><p><s coords="4,150.37,91.78,363.34,4.04;4,150.37,97.10,150.62,4.04">• Translate user's active intent to text represented by morse code <ref type="bibr" coords="4,274.79,91.78,57.12,4.04" target="#b59">(Srivastava and Shinde, 2020)</ref> • Folded ensemble technique to minimize the computational complexity and solve all the class imbalance issues to enhance the accuracy of text generation (Rathod et al., 2024)</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,81.57,110.67,11.64,4.04">LLMs</head><p><s coords="4,150.37,102.69,363.34,4.04;4,150.37,108.01,363.93,4.04;4,150.37,113.33,363.34,4.04;4,150.25,118.65,191.68,4.04;4,81.57,134.88,23.11,4.04">• EEG-to-text seq-to-seq decoding and zero-shot sentence sentiment classification on natural reading tasks <ref type="bibr" coords="4,347.72,102.69,37.33,4.04" target="#b67">(Wang and Ji, 2022)</ref> • Improving accuracy of open-vocabulary EEG-to-text decoding <ref type="bibr" coords="4,505.77,102.69,7.94,4.04;4,150.37,108.01,23.89,4.04">(Liu et al., 2024a)</ref> • Bridge the semantic gap between EEG and Text <ref type="bibr" coords="4,266.97,108.01,35.08,4.04" target="#b66">(Wang et al., 2024)</ref> • Open vocabulary EEG decoding incorporating a subject-dependent representation learning module <ref type="bibr" coords="4,487.62,108.01,26.68,4.04;4,150.37,113.33,10.64,4.04" target="#b2">(Amrani et al., 2024)</ref> • Ensuring cross-modal semantic consistency between EEG and Text <ref type="bibr" coords="4,289.12,113.33,31.26,4.04" target="#b62">(Tao et al., 2024)</ref> • Capture global and local contextual information and long-term dependencies • <ref type="bibr" coords="4,468.52,113.33,34.17,4.04" target="#b9">(Chen et al., 2025)</ref> • Use visual stimuli rather than text circumvent the complexities of language processing <ref type="bibr" coords="4,303.79,118.65,38.14,4.04" target="#b38">(Mishra et al., 2024)</ref> Transformer</s></p><p><s coords="4,150.37,124.24,363.34,4.04;4,150.37,129.56,363.34,4.04;4,150.37,134.88,363.34,4.04;4,150.37,140.20,253.90,4.04;4,236.29,287.12,277.41,4.04;4,150.37,292.44,138.36,4.04;4,81.57,300.68,40.48,4.04;4,150.37,298.02,363.34,4.04;4,150.22,303.34,221.84,4.04;4,81.57,314.25,39.25,4.04">• EEG-to-text seq-to-seq decoding and zero-shot sentence sentiment classification on natural reading tasks <ref type="bibr" coords="4,348.30,124.24,37.44,4.04" target="#b67">(Wang and Ji, 2022)</ref> • Open vocabulary EEG decoding incorporating a subject-dependent representation learning module <ref type="bibr" coords="4,209.53,129.56,39.23,4.04" target="#b2">(Amrani et al., 2024)</ref> • Ensuring cross-modal semantic consistency between EEG and Text <ref type="bibr" coords="4,379.39,129.56,31.86,4.04" target="#b62">(Tao et al., 2024)</ref> • Improving accuracy of open-vocabulary EEG-to-text decoding <ref type="bibr" coords="4,168.48,134.88,33.26,4.04">(Liu et al., 2024a)</ref> • Bridge the semantic gap between EEG and Text using LLMs <ref type="bibr" coords="4,319.32,134.88,35.46,4.04" target="#b66">(Wang et al., 2024)</ref> • Recalibrates subject-dependent EEG representation to the semantic-dependent EEG representation <ref type="bibr" coords="4,177.41,140.20,33.69,4.04" target="#b12">(Feng et al., 2023)</ref> • Open vocabulary EEG-to-Text translation tasks with or without word -level markers <ref type="bibr" coords="4,369.82,140.20,34.45,4.04" target="#b11">(Duan et al., 2023)</ref>   <ref type="bibr" coords="4,236.29,287.12,30.88,4.04" target="#b3">(Bai et al., 2023)</ref> • Reconstructing images using the same semantics as the corresponding EEG <ref type="bibr" coords="4,410.57,287.12,35.95,4.04">(Zeng et al., 2023a)</ref> • Multi-level perceptual information decoding to draw multi grained outputs from given EEG <ref type="bibr" coords="4,256.54,292.44,32.19,4.04" target="#b28">(Lan et al., 2023)</ref> Attention Mechanism • Generates images along with producing class-specific EEG encoding as a latent representation <ref type="bibr" coords="4,331.08,298.02,38.36,4.04" target="#b39">(Mishra et al., 2023)</ref> • Self-supervised framework to decode natural images for object recognition <ref type="bibr" coords="4,150.22,303.34,34.53,4.04" target="#b56">(Song et al., 2023)</ref> • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion <ref type="bibr" coords="4,342.96,303.34,29.09,4.04" target="#b33">(Li et al., 2024)</ref> Contrastive Learning</s></p><p><s coords="4,150.37,308.93,363.92,4.04;4,150.37,314.25,358.48,4.04;4,317.05,558.50,209.17,9.46;4,306.14,572.05,220.08,9.46;4,306.14,585.60,220.07,9.46;4,305.75,599.15,220.03,9.46;4,306.14,612.70,220.07,9.46;4,306.14,626.25,218.27,9.46;4,306.14,639.80,62.70,9.46">• Framework for synthesizing the images using small-size EEG datasets <ref type="bibr" coords="4,284.27,308.93,35.56,4.04" target="#b55">(Singh et al., 2023)</ref> • Multi-level perceptual information decoding to draw multi grained outputs from given EEG <ref type="bibr" coords="4,494.27,308.93,20.02,4.04;4,150.37,314.25,11.08,4.04" target="#b28">(Lan et al., 2023)</ref> • Self-supervised framework to decode natural images for object recognition <ref type="bibr" coords="4,309.33,314.25,35.33,4.04" target="#b56">(Song et al., 2023)</ref> • Generating perceptual and cognitive contents using EEG data <ref type="bibr" coords="4,467.03,314.25,41.82,4.04" target="#b60">(Sugimoto et al., 2024</ref>  To integrate temporal and spatial feature extraction mechanisms to improve EEG-based image reconstruction, <ref type="bibr" coords="4,364.75,585.60,82.00,9.46">Zeng et al. (2023a)</ref> develops a framework inspired by EEGChannelNet <ref type="bibr" coords="4,460.73,599.15,65.05,9.46;4,306.14,612.70,25.96,9.46" target="#b44">(Palazzo et al., 2020)</ref> and ResNet-18, combining spatial, temporal, and temporal-spatial blocks with a multi-kernel residual block.</s><s coords="4,372.13,639.80,152.28,9.46;4,306.14,653.35,220.08,9.46;4,305.75,666.90,220.47,9.46;4,305.87,680.02,220.35,9.88;4,306.14,693.57,25.16,9.81">Shimizu and Srinivasan (2022) uses a time-series-inspired architecture with a channelwise transformed encoder and temporal-spatial convolution to extract rich latent EEG representations.</s></p><p><s coords="4,317.05,709.73,209.17,9.46;4,306.14,723.27,218.27,9.46;4,306.14,736.82,47.11,9.46">Self-supervised learning and contrastive learning have been applied to enhance EEG feature extraction.</s><s coords="4,359.24,736.82,165.17,9.46;4,306.14,750.37,219.63,9.46;4,306.14,763.92,218.27,9.46;5,70.87,74.72,75.92,9.46"><ref type="bibr" coords="4,359.24,736.82,74.75,9.46" target="#b3">Bai et al. (2023)</ref> uses masked signal modeling, where EEG tokens are partially masked, and a 1D convolutional layer transforms all tokens into embeddings.</s><s coords="5,150.17,74.72,139.69,9.46;5,70.87,88.27,218.27,9.46;5,70.87,101.82,70.15,9.46">A Masked Autoencoder (MAE) predicts the missing tokens, refining the learned representations.</s><s coords="5,148.04,101.82,142.90,9.46;5,70.87,115.37,218.27,9.46;5,70.87,128.49,218.26,9.88;5,70.87,142.04,182.64,9.88"><ref type="bibr" coords="5,148.04,101.82,77.65,9.46" target="#b28">Lan et al. (2023)</ref> employs contrastive learning to extract pixel-level semantics from EEG signals while generating a saliency map of silhouette information using GANs.</s><s coords="5,260.48,142.47,28.65,9.46;5,70.87,156.02,218.27,9.46;5,70.87,169.57,218.27,9.46;5,70.87,183.12,58.79,9.46">It also aligns CLIP embeddings for image captions with an EEG sample-level encoder through a specialized loss function.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4" coords="5,70.87,206.99,113.83,9.81">Evaluation Metrics</head><p><s coords="5,70.87,225.98,220.07,9.46;5,70.87,239.53,218.26,9.46;5,70.87,253.08,220.17,9.46">EEG-to-image generation often begins with object classification to ensure extracted EEG features contain useful class-discriminative information.</s><s coords="5,70.87,266.43,218.27,9.66;5,70.51,280.18,219.53,9.46;5,70.87,293.73,218.27,9.46;5,70.87,307.28,158.70,9.46">Metrics like top-k accuracy are commonly used <ref type="bibr" coords="5,70.51,280.18,141.64,9.46" target="#b53">(Shimizu and Srinivasan, 2022;</ref><ref type="bibr" coords="5,215.55,280.18,74.49,9.46" target="#b28">Lan et al., 2023;</ref><ref type="bibr" coords="5,70.87,293.73,80.08,9.46" target="#b56">Song et al., 2023)</ref>, along with qualitative visual analysis and quantitative evaluations.</s><s coords="5,232.92,307.28,58.03,9.46;5,70.87,320.63,218.27,9.66;5,70.87,334.38,218.27,9.46;5,70.87,347.92,219.00,9.46;5,70.47,361.28,220.47,9.66;5,70.87,374.83,218.26,9.66;5,70.87,388.57,219.63,9.46;5,70.87,401.92,220.08,9.66;5,70.87,415.47,218.65,9.66;5,70.51,429.22,219.99,9.46;5,70.87,442.77,220.08,9.46;5,70.87,456.32,77.23,9.46">Key quantitative metrics include Inception Score (IS) <ref type="bibr" coords="5,246.33,320.83,42.81,9.46;5,70.87,334.38,52.74,9.46" target="#b51">(Salimans et al., 2016)</ref>, used by <ref type="bibr" coords="5,170.86,334.38,104.81,9.46" target="#b23">(Kavasidis et al., 2017;</ref><ref type="bibr" coords="5,279.25,334.38,9.89,9.46;5,70.87,347.92,54.18,9.46" target="#b32">Li et al., 2020;</ref><ref type="bibr" coords="5,128.65,347.92,73.24,9.46" target="#b3">Bai et al., 2023;</ref><ref type="bibr" coords="5,205.50,347.92,84.37,9.46" target="#b55">Singh et al., 2023)</ref> which measures the quality of images, Frechet Inception Distance (FID) for measuring realism <ref type="bibr" coords="5,270.66,375.02,18.47,9.46;5,70.87,388.57,54.02,9.46" target="#b3">(Bai et al., 2023;</ref><ref type="bibr" coords="5,128.42,388.57,83.52,9.46" target="#b54">Singh et al., 2024;</ref><ref type="bibr" coords="5,215.47,388.57,75.03,9.46;5,70.87,402.12,23.01,9.46" target="#b0">Ahmadieh et al., 2024)</ref>, and saliency metrics such as Structural Similarity Index (SSIM) for assessing perceptual fidelity <ref type="bibr" coords="5,70.51,429.22,104.46,9.46" target="#b24">(Khaleghi et al., 2022;</ref><ref type="bibr" coords="5,179.35,429.22,111.14,9.46;5,70.87,442.77,24.35,9.46" target="#b53">Shimizu and Srinivasan, 2022;</ref><ref type="bibr" coords="5,97.76,442.77,67.62,9.46" target="#b3">Bai et al., 2023;</ref><ref type="bibr" coords="5,167.93,442.77,97.31,9.46" target="#b0">Ahmadieh et al., 2024;</ref><ref type="bibr" coords="5,267.78,442.77,18.53,9.46;5,70.87,456.32,72.63,9.46" target="#b60">Sugimoto et al., 2024)</ref>.</s><s coords="5,151.33,456.32,138.00,9.46;5,70.51,469.87,219.99,9.46;5,70.87,483.22,218.27,9.66;5,70.87,496.77,218.26,9.66;5,70.53,510.32,218.60,9.66;5,70.59,523.87,119.27,9.66">Other useful metrics are PixCorr (Pixel-wise Correlation) <ref type="bibr" coords="5,179.93,469.87,110.57,9.46;5,70.87,483.42,23.95,9.46" target="#b53">(Shimizu and Srinivasan, 2022)</ref>, Kernel Inception Distance (KID) <ref type="bibr" coords="5,259.46,483.42,29.68,9.46;5,70.87,496.97,53.13,9.46" target="#b54">(Singh et al., 2024)</ref>, LPIPS (Learned Perceptual Image Patch Similarity) <ref type="bibr" coords="5,149.50,510.51,76.43,9.46" target="#b3">(Bai et al., 2023)</ref> and Diversity Score <ref type="bibr" coords="5,97.76,524.06,87.41,9.46" target="#b39">(Mishra et al., 2023)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" coords="5,70.87,548.59,144.01,10.75">EEG-to-Text Generation</head><p><s coords="5,70.53,572.02,220.42,9.46;5,70.87,585.57,218.27,9.46;5,70.87,599.12,220.07,9.46;5,70.87,612.66,77.20,9.46">This section discusses how AI learns brain signal representations from EEG data and maps them to linguistic representations, with an overview depicted in Figure <ref type="figure" coords="5,140.05,612.66,4.01,9.46" target="#fig_2">3</ref>.</s><s coords="5,151.29,612.66,139.21,9.46;5,70.87,626.21,218.46,9.46;5,70.87,639.76,67.70,9.46">We survey use cases, techniques, concerns, and EEG feature encoding methods for text generation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" coords="5,70.87,663.63,189.63,9.81">Use Cases and Addressed Concerns</head><p><s coords="5,70.53,682.63,220.42,9.46;5,70.87,696.18,220.18,9.46">The studies referenced in this section share a common use case: generating text from EEG signals.</s></p><p><s coords="5,70.87,709.73,218.27,9.46;5,70.87,723.27,219.63,9.46;5,70.87,736.82,218.26,9.46;5,70.87,750.37,218.27,9.46;5,70.87,763.92,43.57,9.46">Several studies <ref type="bibr" coords="5,137.84,709.73,86.72,9.46" target="#b6">(Biswal et al., 2019;</ref><ref type="bibr" coords="5,227.28,709.73,61.85,9.46;5,70.87,723.27,63.65,9.46" target="#b59">Srivastava and Shinde, 2020;</ref><ref type="bibr" coords="5,139.11,723.27,83.80,9.46" target="#b71">Yang et al., 2023;</ref><ref type="bibr" coords="5,227.52,723.27,62.98,9.46;5,70.87,736.82,25.32,9.46" target="#b50">Rathod et al., 2024)</ref> use the closed vocabulary approach, relying on a fixed set of pre-defined words for EEG-based decoding.</s><s coords="5,121.57,763.92,167.57,9.46;5,396.11,230.18,130.12,9.46;5,306.14,243.73,220.07,9.46;5,306.14,257.28,218.27,9.46;5,306.14,270.83,217.98,9.46">Among these, Srivastava and Shinde  <ref type="formula" coords="5,396.11,230.18,19.74,9.46">2023</ref>) investigate text generation using morse code representation of EEG signals, where users' active intent is captured, mapped to morse codes, and then translated to text format.</s></p><p><s coords="5,317.05,285.20,208.73,9.46;5,306.14,298.74,218.27,9.46;5,306.14,312.29,219.17,9.46;5,306.14,325.84,218.27,9.46;5,306.14,339.39,220.07,9.46;5,306.14,352.94,218.27,9.46;5,306.14,366.49,157.48,9.46">Recent studies <ref type="bibr" coords="5,384.36,285.20,89.27,9.46" target="#b67">(Wang and Ji, 2022;</ref><ref type="bibr" coords="5,476.38,285.20,49.39,9.46;5,306.14,298.74,25.35,9.46" target="#b12">Feng et al., 2023;</ref><ref type="bibr" coords="5,334.68,298.74,80.67,9.46" target="#b11">Duan et al., 2023;</ref><ref type="bibr" coords="5,418.55,298.74,76.97,9.46">Liu et al., 2024a;</ref><ref type="bibr" coords="5,498.71,298.74,25.70,9.46;5,306.14,312.29,52.54,9.46" target="#b66">Wang et al., 2024;</ref><ref type="bibr" coords="5,361.47,312.29,89.32,9.46" target="#b2">Amrani et al., 2024;</ref><ref type="bibr" coords="5,453.57,312.29,71.75,9.46" target="#b62">Tao et al., 2024;</ref><ref type="bibr" coords="5,306.14,325.84,90.58,9.46" target="#b38">Mishra et al., 2024;</ref><ref type="bibr" coords="5,400.75,325.84,96.15,9.46" target="#b19">Ikegawa et al., 2024;</ref><ref type="bibr" coords="5,500.93,325.84,23.49,9.46;5,306.14,339.39,54.85,9.46" target="#b9">Chen et al., 2025)</ref> overcome closed-vocabulary limitations by exploring open-vocabulary text generation to emulate naturalistic conversations.</s><s coords="5,466.87,366.49,57.54,9.46;5,306.14,380.04,220.08,9.46;5,306.14,393.59,219.17,9.46;5,305.75,407.14,220.48,9.46;5,306.14,420.69,218.27,9.46;5,306.14,434.24,218.27,9.46;5,306.14,447.79,218.27,9.46;5,306.14,461.33,218.27,9.46;5,306.14,474.88,54.84,9.46">These studies also address the impact of subjectivity in subjectdependent EEG representation <ref type="bibr" coords="5,444.37,393.59,80.95,9.46" target="#b12">(Feng et al., 2023;</ref><ref type="bibr" coords="5,305.75,407.14,88.58,9.46" target="#b2">Amrani et al., 2024)</ref>, learn cross-modal representation <ref type="bibr" coords="5,335.21,420.69,87.52,9.46" target="#b66">(Wang et al., 2024;</ref><ref type="bibr" coords="5,426.44,420.69,73.16,9.46" target="#b62">Tao et al., 2024)</ref>, and capture long-term dependencies in text and also global contextual information from EEG data that transformers might miss <ref type="bibr" coords="5,412.24,461.33,87.10,9.46" target="#b50">(Rathod et al., 2024;</ref><ref type="bibr" coords="5,501.84,461.33,22.57,9.46;5,306.14,474.88,50.14,9.46" target="#b9">Chen et al., 2025)</ref>.</s></p><p><s coords="5,317.05,489.25,209.17,9.46;5,306.14,502.80,218.27,9.46;5,306.14,516.35,218.27,9.46;5,306.14,529.90,130.81,9.46">A significant challenge is the reliance on eyetracking fixation data as a marker for word-level EEG, which studies like <ref type="bibr" coords="5,419.71,516.35,85.63,9.46" target="#b11">(Duan et al., 2023;</ref><ref type="bibr" coords="5,508.96,516.35,15.46,9.46;5,306.14,529.90,59.24,9.46">Liu et al., 2024a)</ref> aim to address.</s><s coords="5,442.23,529.90,83.99,9.46;5,306.14,543.45,218.26,9.46;5,306.14,557.00,218.27,9.46;5,306.14,570.55,218.27,9.46;5,305.78,584.10,188.36,9.46">To overcome challenges like defining word-level boundaries in EEG signals and other language processing tasks, some studies have proposed language-agnostic solutions <ref type="bibr" coords="5,305.78,584.10,91.41,9.46" target="#b38">(Mishra et al., 2024;</ref><ref type="bibr" coords="5,400.26,584.10,93.88,9.46" target="#b19">Ikegawa et al., 2024)</ref></s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" coords="5,306.14,690.19,173.36,9.81">Techniques Used Across Studies</head><p><s coords="5,305.75,709.73,220.47,9.46;5,306.14,722.85,220.08,9.88;5,306.14,736.82,59.65,9.46">A noteworthy aspect of these studies is the utilization of Large Language Models (LLMs), particularly BART.</s><s coords="5,368.88,736.82,156.44,9.46;5,306.14,750.37,219.63,9.46;5,306.14,763.92,218.27,9.46;6,70.87,74.72,118.15,9.46">Several works <ref type="bibr" coords="5,435.05,736.82,90.26,9.46" target="#b67">(Wang and Ji, 2022;</ref><ref type="bibr" coords="5,306.14,750.37,74.43,9.46">Liu et al., 2024a;</ref><ref type="bibr" coords="5,383.28,750.37,79.62,9.46" target="#b66">Wang et al., 2024;</ref><ref type="bibr" coords="5,465.63,750.37,60.15,9.46;5,306.14,763.92,24.35,9.46" target="#b2">Amrani et al., 2024;</ref><ref type="bibr" coords="5,333.03,763.92,68.49,9.46" target="#b62">Tao et al., 2024;</ref><ref type="bibr" coords="5,404.05,763.92,75.89,9.46" target="#b9">Chen et al., 2025)</ref> have used BART for text generation.</s><s coords="6,194.48,74.72,94.66,9.46;6,70.87,88.27,220.07,9.46;6,70.87,101.82,218.27,9.46;6,70.87,115.37,220.08,9.46;6,70.87,128.92,22.71,9.46">In a study by <ref type="bibr" coords="6,257.62,74.72,31.52,9.46;6,70.87,88.27,52.68,9.46" target="#b38">Mishra et al. (2024)</ref>, LLMs were fine-tuned on EEG embeddings, image and text data in the training stage to generate text from just EEG signals during inference.</s></p><p><s coords="6,81.78,143.73,207.35,9.88;6,70.87,157.70,219.63,9.46;6,70.87,171.25,218.27,9.46;6,70.87,184.80,218.27,9.46;6,70.87,198.35,220.08,9.46;6,70.87,211.90,218.65,9.46;6,70.87,225.45,220.08,9.46;6,70.87,239.00,45.44,9.46">Contrastive learning is widely used in studies like <ref type="bibr" coords="6,89.28,157.70,77.52,9.46" target="#b12">(Feng et al., 2023;</ref><ref type="bibr" coords="6,169.28,157.70,68.34,9.46" target="#b62">Tao et al., 2024;</ref><ref type="bibr" coords="6,240.09,157.70,50.41,9.46;6,70.87,171.25,24.94,9.46" target="#b66">Wang et al., 2024)</ref> to identify positive EEG-text pairs (e.g., EEG data from the same sentence across subjects) and negative pairs (e.g., EEG data from different sentences or subjects), improving the model's ability to align EEG signals with corresponding text representations.</s><s coords="6,119.61,239.00,169.53,9.46;6,70.87,252.54,218.27,9.46;6,70.87,266.09,218.27,9.46;6,70.87,279.64,220.07,9.46;6,70.87,293.19,218.27,9.46;6,70.87,306.74,201.32,9.46">Another key technique is masked signal modeling, employed by <ref type="bibr" coords="6,179.02,252.54,75.43,9.46">Liu et al. (2024a)</ref>, where a transformer model is pre-trained to reconstruct randomly masked EEG signals from raw data, enabling the model to learn context, relationships, and semantics within sentence-level EEG signals.</s><s coords="6,275.56,306.74,13.57,9.46;6,70.87,320.29,218.27,9.46;6,70.87,333.41,220.08,9.88;6,70.87,346.96,218.27,9.88;6,70.87,360.94,218.27,9.46;6,70.87,374.49,218.65,9.46;6,70.87,388.04,126.81,9.46">An integrated approach by <ref type="bibr" coords="6,173.10,320.29,71.62,9.46" target="#b62">Tao et al. (2024)</ref> combines contrastive learning with masked signal modeling, where word-level EEG feature sequences are randomly masked and sentence-level sequences deliberately masked, guided by an intra-modality self-reconstruction objective.</s></p><p><s coords="6,81.78,403.27,207.36,9.46;6,70.87,416.82,220.08,9.46;6,70.87,430.37,218.66,9.46;6,70.87,443.91,157.77,9.46">In addition to these techniques, bi-directional Gated Recurrent Units (GRUs) are used to dynamically handle the varying lengths of word-level raw EEG signals <ref type="bibr" coords="6,130.27,443.91,93.58,9.46" target="#b2">(Amrani et al., 2024)</ref>.</s><s coords="6,234.15,443.91,54.99,9.46;6,70.87,457.46,218.65,9.46;6,70.87,471.01,218.26,9.46;6,70.87,484.56,218.27,9.46;6,70.87,498.11,207.41,9.46">Hierarchical GRUs further improve EEG data processing by capturing both long-range dependencies and local contextual information through the organization of hidden layers hierarchically <ref type="bibr" coords="6,194.33,498.11,79.27,9.46" target="#b9">(Chen et al., 2025)</ref>.</s><s coords="6,281.68,498.11,7.84,9.46;6,70.87,511.66,218.27,9.46;6,70.87,525.21,218.27,9.46;6,70.87,538.76,218.27,9.46;6,70.87,552.31,218.27,9.46;6,70.87,565.86,218.27,9.46;6,70.87,579.41,166.80,9.46">A unique approach by <ref type="bibr" coords="6,159.60,511.66,89.96,9.46" target="#b50">(Rathod et al., 2024)</ref> employs a folded ensemble deep CNN for text suggestion and a folded ensemble Bidirectional LSTM for text generation, effectively addressing class imbalance and significantly enhancing the accuracy of text generation in closed-vocabulary tasks.</s><s coords="6,317.05,186.60,207.36,9.88;6,306.14,200.15,220.08,9.88;6,306.14,214.13,52.91,9.46">Other studies explore the extraction of spectral and statistical features alongside temporal or spatial patterns.</s><s coords="6,362.44,214.13,161.97,9.46;6,306.14,227.68,220.07,9.46;6,306.14,241.23,218.27,9.46;6,306.14,254.78,218.27,9.46;6,305.75,268.33,218.85,9.46;6,306.14,281.87,218.45,9.46;6,306.14,295.42,220.17,9.46"><ref type="bibr" coords="6,362.44,214.13,75.09,9.46" target="#b71">Yang et al. (2023)</ref>, aiming to translate active intention into text using Morse code, employed Short-Term Fourier Transform (STFT) to extract spectral features and concatenated these with statistical features (e.g., min, max etc. for each channel), in addition to using 1D CNN for spatial features and RNN for temporal features.</s><s coords="6,306.14,308.97,220.08,9.46;6,306.14,322.52,218.27,9.46;6,305.78,335.65,220.44,9.88;6,306.14,349.19,218.45,9.88;6,306.14,363.17,59.69,9.46">Rathod et al. ( <ref type="formula" coords="6,367.92,308.97,17.85,9.46">2024</ref>), another closed-vocabulary solution, used features such as Wavelet Transform (WT), Common Spatial Patterns (CSP), and statistical features to generate EEG feature vectors for classification.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" coords="6,70.87,607.19,188.25,9.81">EEG Feature Encoding Techniques</head><p><s coords="6,317.05,380.63,209.17,9.46;6,306.14,394.18,220.17,9.46">Various studies have used state-of-the-art transformer architecture for encoding EEG features.</s><s coords="6,305.78,407.73,218.81,9.46;6,306.14,420.85,220.08,9.88;6,306.14,434.40,103.18,9.81"><ref type="bibr" coords="6,305.78,407.73,81.89,9.46" target="#b67">(Wang and Ji, 2022</ref>) uses a multi-layer transformer encoder to obtain EEG mapping from wordlevel EEG sequences.</s><s coords="6,417.36,434.83,107.05,9.46;6,306.14,448.38,220.08,9.46;6,306.14,461.93,218.27,9.46;6,306.14,475.48,26.42,9.46"><ref type="bibr" coords="6,417.36,434.83,83.60,9.46" target="#b12">Feng et al. (2023)</ref> uses a transformer-based pre-encoder to convert wordlevel EEG features into the Seq2Seq embedding space.</s><s coords="6,335.93,475.48,188.48,9.46;6,306.14,489.03,218.27,9.46;6,306.14,502.57,218.27,9.46;6,306.14,516.12,218.27,9.46;6,306.14,529.67,60.79,9.46">Another study by <ref type="bibr" coords="6,412.92,475.48,70.45,9.46" target="#b62">Tao et al. (2024)</ref> also uses an encoder to extract EEG embeddings and store them in a cross-modal codebook alongside word embeddings obtained from a transformer-based BART model.</s></p><p><s coords="6,317.05,547.14,209.17,9.46;6,306.14,560.68,218.27,9.46;6,306.14,574.23,188.74,9.46">Obtaining word-level EEG signals typically requires markers, often from eye-fixation data like in the Zuco dataset, limiting generalizability.</s><s coords="6,499.07,574.23,25.35,9.46;6,306.14,587.36,218.27,9.88;6,306.14,600.91,130.55,9.81">Some studies address this by using marker-free and sentence-level EEG signals.</s><s coords="6,442.21,601.33,82.93,9.46;6,306.14,614.88,220.07,9.46;6,306.14,628.43,41.74,9.46"><ref type="bibr" coords="6,442.21,601.33,82.93,9.46" target="#b11">Duan et al. (2023)</ref> extracts both word-level EEG and raw EEG embeddings.</s><s coords="6,351.26,628.43,174.96,9.46;6,306.14,641.98,220.07,9.46;6,306.14,655.53,148.78,9.46">For word-level EEG features with markers, a multi-head transformer layer projects embeddings into feature sequences.</s><s coords="6,462.64,655.53,61.78,9.46;6,305.75,669.08,218.67,9.46;6,306.14,682.63,220.08,9.46;6,306.14,696.18,218.27,9.46;6,306.14,709.73,56.56,9.46">For raw EEG waves, a multi-layer transformer encoder is trained for self-reconstruction of waveforms and the transformation of raw EEG signals into sequences of embeddings.</s><s coords="6,368.87,709.73,155.55,9.46;6,306.14,723.27,218.27,9.46;6,306.14,736.82,220.08,9.46;6,306.14,750.37,27.43,9.46">In a study by <ref type="bibr" coords="6,432.93,709.73,77.98,9.46">Liu et al. (2024a)</ref>, a convolutional transformer model is pretrained with sentence-level EEG signals using a masking technique.</s><s coords="6,336.97,750.37,187.43,9.46;6,306.14,763.92,218.27,9.46;7,70.87,74.72,59.02,9.46">It uses a multi-view transformer to encode different brain regions with separate convolutional transformers.</s><s coords="7,133.84,74.72,157.11,9.46;7,70.87,88.27,168.46,9.46"><ref type="bibr" coords="7,133.84,74.72,82.95,9.46" target="#b66">Wang et al. (2024)</ref> uses both wordlevel and sentence-level EEG features.</s><s coords="7,242.71,88.27,46.42,9.46;7,70.87,101.82,218.27,9.46;7,70.87,115.37,218.27,9.46;7,70.87,128.92,112.30,9.46">It employs a masking technique where word-level sequences are randomly masked and sentence-level features are compulsorily masked.</s></p><p><s coords="7,81.78,142.99,207.36,9.46;7,70.87,156.54,218.27,9.46;7,70.47,170.09,220.48,9.46;7,70.87,183.21,220.07,9.88;7,70.87,196.76,81.06,9.81">Chen et al. ( <ref type="formula" coords="7,138.92,142.99,19.78,9.46">2025</ref>) uses a stacked Hierarchical GRU-based decoder along with Masked Residual Attention Mechanism to obtain EEG representations that capture both local and global contextual information.</s><s coords="7,156.51,197.19,132.63,9.46;7,70.87,210.74,220.08,9.46;7,70.87,224.29,218.27,9.46;7,70.87,237.83,220.08,9.46;7,70.87,251.38,218.45,9.46;7,70.87,264.93,151.24,9.46"><ref type="bibr" coords="7,156.51,197.19,91.90,9.46" target="#b2">Amrani et al. (2024)</ref> employs a module consisting of bi-directional GRUs to dynamically address varying lengths of word-level raw EEG signals, a subject-specific 1D convolutional layer, and a multi-layer transformer encoder to encode word-level EEG signals.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4" coords="7,70.87,288.69,113.83,9.81">Evaluation Metrics</head><p><s coords="7,70.87,307.63,220.07,9.46;7,70.87,321.18,220.07,9.46;7,70.87,334.73,62.36,9.46">In the surveyed studies, generated text is evaluated against reference text using various established metrics.</s><s coords="7,136.59,334.73,152.54,9.46;7,70.87,348.08,218.27,9.66;7,70.87,361.83,219.17,9.46;7,70.87,375.18,219.64,9.66;7,70.87,388.92,219.63,9.46;7,70.87,402.28,219.63,9.66;7,70.87,416.02,219.17,9.46;7,70.87,429.57,219.63,9.46;7,70.87,442.92,218.27,9.66;7,70.87,456.67,190.80,9.46">The commonly used text evaluation metrics are as follows: METEOR (Banerjee and Lavie, 2005), employed by <ref type="bibr" coords="7,197.45,361.83,92.59,9.46" target="#b6">(Biswal et al., 2019;</ref><ref type="bibr" coords="7,70.87,375.37,80.25,9.46" target="#b9">Chen et al., 2025)</ref>; BLEU score <ref type="bibr" coords="7,219.10,375.37,71.40,9.46;7,70.87,388.92,23.01,9.46" target="#b45">(Papineni et al., 2002)</ref>, utilized by <ref type="bibr" coords="7,148.32,388.92,85.28,9.46" target="#b6">(Biswal et al., 2019;</ref><ref type="bibr" coords="7,235.93,388.92,54.57,9.46;7,70.87,402.47,24.35,9.46" target="#b67">Wang and Ji, 2022;</ref><ref type="bibr" coords="7,97.43,402.47,72.09,9.46" target="#b12">Feng et al., 2023)</ref>; ROUGE score <ref type="bibr" coords="7,239.58,402.47,41.72,9.46" target="#b34">(Lin, 2004</ref>), adopted by <ref type="bibr" coords="7,122.21,416.02,88.39,9.46" target="#b67">(Wang and Ji, 2022;</ref><ref type="bibr" coords="7,213.32,416.02,76.72,9.46" target="#b12">Feng et al., 2023;</ref><ref type="bibr" coords="7,70.87,429.57,81.23,9.46" target="#b11">Duan et al., 2023;</ref><ref type="bibr" coords="7,155.49,429.57,77.52,9.46">Liu et al., 2024a;</ref><ref type="bibr" coords="7,236.40,429.57,54.10,9.46;7,70.87,443.12,24.16,9.46" target="#b66">Wang et al., 2024)</ref>; and BERTScore <ref type="bibr" coords="7,175.51,443.12,85.76,9.46" target="#b77">(Zhang et al., 2019)</ref>, used by <ref type="bibr" coords="7,84.07,456.67,88.88,9.46" target="#b2">(Amrani et al., 2024;</ref><ref type="bibr" coords="7,175.46,456.67,81.61,9.46" target="#b38">Mishra et al., 2024)</ref>.</s><s coords="7,264.98,456.67,24.34,9.46;7,70.87,470.02,218.65,9.66;7,70.51,483.57,219.99,9.66;7,70.87,497.12,218.27,9.66;7,70.87,510.87,54.84,9.46">Other metrics include Word Error Rate (WER) used by <ref type="bibr" coords="7,70.51,483.77,79.07,9.46" target="#b12">(Feng et al., 2023)</ref>, Translation Error Rate (TER), and BLEURT <ref type="bibr" coords="7,132.62,497.32,86.40,9.46" target="#b52">(Sellam et al., 2020)</ref>, used by <ref type="bibr" coords="7,262.61,497.32,26.53,9.46;7,70.87,510.87,50.14,9.46" target="#b9">(Chen et al., 2025)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" coords="7,70.87,535.29,193.63,10.75">EEG-to-Sound/Speech Generation</head><p><s coords="7,70.35,558.64,220.59,9.46;7,70.87,572.19,218.45,9.46;7,70.87,585.73,218.27,9.46;7,70.87,599.28,218.26,9.46;7,70.87,612.83,71.80,9.46">We review studies focused on EEG-based generation of sound, speech, voice or music and cover use cases, concerns, techniques, and EEG feature encoding methods for generating Sound or Speech from from EEG.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" coords="7,70.87,636.59,189.63,9.81">Use Cases and Addressed Concerns</head><p><s coords="7,70.87,655.53,218.27,9.46;7,70.87,669.08,218.27,9.46;7,70.87,682.63,172.04,9.46">EEG-based generation has been explored in various fields beyond image reconstruction, particularly in audio and speech-related applications.</s><s coords="7,248.54,682.63,42.41,9.46;7,70.87,696.18,218.27,9.46;7,70.87,709.73,218.26,9.46;7,70.51,723.27,218.63,9.46;7,70.87,736.82,219.63,9.46;7,70.87,750.37,219.63,9.46;7,70.87,763.92,218.27,9.46;7,306.14,74.72,163.78,9.46">These include speech synthesis <ref type="bibr" coords="7,174.83,696.18,94.69,9.46" target="#b26">(Krishna et al., 2021;</ref><ref type="bibr" coords="7,272.46,696.18,16.68,9.46;7,70.87,709.73,55.83,9.46">Lee et al., 2023a)</ref>, music decoding and reconstruction <ref type="bibr" coords="7,70.51,723.27,168.39,9.46" target="#b49">(Ramirez-Aristizabal and Kello, 2022;</ref><ref type="bibr" coords="7,241.63,723.27,47.51,9.46;7,70.87,736.82,48.69,9.46" target="#b47">Postolache et al., 2024)</ref>, emotive music generation <ref type="bibr" coords="7,238.69,736.82,51.81,9.46;7,70.87,750.37,23.95,9.46" target="#b20">(Jiang et al., 2024)</ref>, voice reconstruction <ref type="bibr" coords="7,200.88,750.37,84.72,9.46">(Lee et al., 2023b)</ref>, talking-face generation <ref type="bibr" coords="7,180.01,763.92,83.23,9.46" target="#b46">(Park et al., 2024)</ref>, and speech recovery <ref type="bibr" coords="7,377.27,74.72,88.06,9.46" target="#b40">(Mizuno et al., 2024)</ref>.</s><s coords="7,473.23,74.72,51.18,9.46;7,306.14,88.27,220.08,9.46;7,306.14,101.82,218.27,9.46;7,306.14,115.37,219.17,9.46;7,306.14,128.92,218.26,9.46;7,306.14,142.47,220.07,9.46;7,306.14,156.02,218.26,9.46;7,306.14,169.57,141.18,9.46">While some studies focus on decoding audio signals for listening tasks in speech or music perception <ref type="bibr" coords="7,485.48,101.82,38.93,9.46;7,306.14,115.37,51.74,9.46" target="#b26">(Krishna et al., 2021;</ref><ref type="bibr" coords="7,360.60,115.37,164.72,9.46" target="#b49">Ramirez-Aristizabal and Kello, 2022;</ref><ref type="bibr" coords="7,306.14,128.92,75.20,9.46" target="#b46">Park et al., 2024;</ref><ref type="bibr" coords="7,384.07,128.92,89.54,9.46" target="#b40">Mizuno et al., 2024;</ref><ref type="bibr" coords="7,476.33,128.92,48.07,9.46;7,306.14,142.47,51.67,9.46" target="#b47">Postolache et al., 2024;</ref><ref type="bibr" coords="7,360.53,142.47,76.15,9.46" target="#b20">Jiang et al., 2024)</ref>, others also investigate speaking tasks and imagined speech <ref type="bibr" coords="7,486.43,156.02,37.98,9.46;7,306.14,169.57,51.51,9.46" target="#b26">(Krishna et al., 2021;</ref><ref type="bibr" coords="7,360.38,169.57,82.37,9.46">Lee et al., 2023b,a)</ref>.</s></p><p><s coords="7,317.05,184.80,209.27,9.46;7,305.78,198.35,218.63,9.46;7,306.14,211.90,218.27,9.46;7,306.14,225.45,149.47,9.46">For more naturalistic communication, <ref type="bibr" coords="7,485.74,184.80,40.59,9.46;7,305.78,198.35,35.23,9.46">Lee et al. (2023b)</ref> converts EEG signals recorded during imagined speech into the user's own voice, aiming for personalized speech synthesis.</s><s coords="7,459.03,225.45,65.66,9.46;7,306.14,239.00,218.27,9.46;7,305.75,252.54,197.36,9.46">Similarly, <ref type="bibr" coords="7,504.70,225.45,19.98,9.46;7,306.14,239.00,54.46,9.46" target="#b46">Park et al. (2024)</ref> synthesizes speech from EEG along with generating a talking face with lip-sync.</s><s coords="7,507.28,252.54,18.95,9.46;7,306.14,266.09,220.08,9.46;7,306.14,279.64,219.64,9.46;7,306.14,293.19,218.27,9.46;7,306.14,306.74,218.27,9.46;7,306.14,320.29,218.27,9.46;7,306.14,333.84,220.08,9.46;7,306.14,347.39,218.27,9.46;7,306.14,360.94,54.84,9.46">Furthermore, these studies tackle issues such as generating fragmented or abstract outputs <ref type="bibr" coords="7,475.48,279.64,50.30,9.46;7,306.14,293.19,23.15,9.46" target="#b46">(Park et al., 2024)</ref>, challenges of synthesizing complete speech from EEG <ref type="bibr" coords="7,355.16,306.74,92.54,9.46" target="#b40">(Mizuno et al., 2024)</ref>, being restricted to simpler music with limited timbres <ref type="bibr" coords="7,473.36,320.29,51.05,9.46;7,306.14,333.84,49.95,9.46" target="#b47">(Postolache et al., 2024)</ref>, and the absence of a standardized vocabulary for aligning EEG and audio data <ref type="bibr" coords="7,497.22,347.39,27.20,9.46;7,306.14,360.94,50.14,9.46" target="#b20">(Jiang et al., 2024)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" coords="7,306.14,388.72,173.36,9.81">Techniques Used Across Studies</head><p><s coords="7,306.14,409.54,218.27,9.81;7,306.14,423.51,220.08,9.46;7,306.14,437.06,219.63,9.46;7,306.14,450.61,220.08,9.46;7,306.14,464.16,16.92,9.46">Convolutional Neural Network (CNN)-based deep learning models have been used in studies <ref type="bibr" coords="7,508.04,423.51,13.64,9.46;7,306.14,437.06,78.41,9.46" target="#b26">(Krishna et al., 2021;</ref><ref type="bibr" coords="7,387.29,437.06,138.49,9.46;7,306.14,450.61,25.88,9.46" target="#b49">Ramirez-Aristizabal and Kello, 2022)</ref> to generate audio waveforms from EEG input.</s><s coords="7,326.45,464.16,199.77,9.46;7,306.14,477.71,218.27,9.46;7,306.14,491.26,220.07,9.46;7,306.14,504.81,218.45,9.46;7,306.14,518.36,170.28,9.46"><ref type="bibr" coords="7,326.45,464.16,91.52,9.46" target="#b26">Krishna et al. (2021)</ref> explores speech synthesis for both speaking and listening tasks, using a deep learning architecture with temporal convolution layers, 1D layer, and a time-distributed layer to generate audio waveforms directly.</s><s coords="7,482.28,518.36,43.50,9.46;7,306.14,531.90,218.27,9.46;7,306.14,545.45,207.71,9.46">Similarly, Ramirez-Aristizabal and Kello (2022) reconstructs music stimuli using sequential CNN regressors.</s><s coords="7,317.05,560.68,207.63,9.46;7,306.14,574.23,220.18,9.46"><ref type="bibr" coords="7,317.05,560.68,74.90,9.46">Lee et al. (2023b)</ref> propose NeuroTalk framework for voice reconstruction from imagined speech.</s><s coords="7,305.80,587.36,218.61,9.88;7,306.14,601.33,218.27,9.46;7,306.14,614.88,85.70,9.46">The framework uses a generator based on GRUs to capture sequential EEG information, which outputs a mel-spectrogram.</s><s coords="7,395.25,614.88,130.97,9.46;7,305.87,628.00,218.78,9.88;7,305.78,641.98,218.63,9.46;7,306.14,655.10,218.27,9.88;7,306.14,668.65,218.60,9.88;7,305.78,682.63,220.44,9.46;7,306.14,696.18,117.04,9.46">Mel-spectrogram is then converted into a waveform using a HiFi-GAN vocoder <ref type="bibr" coords="7,305.78,641.98,80.97,9.46" target="#b25">(Kong et al., 2020)</ref>, and the resulting waveform is transcribed into text using an Automatic Speech Recognition (ASR) system based on HuBERT <ref type="bibr" coords="7,305.78,682.63,75.48,9.46" target="#b18">(Hsu et al., 2021)</ref>, a self-supervised speech representation learning method.</s><s coords="7,426.56,696.18,97.84,9.46;7,306.14,709.73,218.27,9.46;7,306.14,723.27,218.27,9.46;7,306.14,736.40,218.27,9.88;7,305.75,750.37,218.66,9.46;7,306.14,763.92,113.31,9.46"><ref type="bibr" coords="7,426.56,696.18,76.17,9.46" target="#b46">Park et al. (2024)</ref> uses NeuroTalk framework to synthesize audible speech and integrates it with a personalized talking face using <ref type="bibr" coords="7,332.77,736.40,143.29,9.88">Wave2Lip (Prajwal et al., 2020)</ref> and Apple API-based avatar generator that accurately lip-sync to the synthesized speech.</s></p><p><s coords="8,81.78,74.30,207.36,9.81;8,70.87,88.27,219.63,9.46;8,70.87,101.82,218.27,9.46;8,70.87,115.37,56.06,9.46">Transformers and Latent Diffusion Models have been used to reconstruct speech <ref type="bibr" coords="8,228.51,88.27,61.99,9.46;8,70.87,101.82,25.96,9.46" target="#b40">(Mizuno et al., 2024)</ref> and music <ref type="bibr" coords="8,151.16,101.82,110.58,9.46" target="#b47">(Postolache et al., 2024;</ref><ref type="bibr" coords="8,265.65,101.82,23.49,9.46;8,70.87,115.37,51.27,9.46" target="#b20">Jiang et al., 2024)</ref>.</s><s coords="8,130.66,115.37,160.28,9.46;8,70.87,128.92,218.27,9.46;8,70.87,142.47,218.26,9.46;8,70.87,156.02,219.63,9.46;8,70.87,169.57,219.63,9.46;8,70.87,183.12,218.27,9.46;8,70.87,196.67,111.76,9.46"><ref type="bibr" coords="8,130.66,115.37,80.54,9.46" target="#b20">Jiang et al. (2024)</ref> employs a Transformer model for emotive music generation, while <ref type="bibr" coords="8,70.87,142.47,101.80,9.46" target="#b47">Postolache et al. (2024)</ref> decodes naturalistic music from EEG using a ControlNet adapter <ref type="bibr" coords="8,234.11,156.02,56.39,9.46;8,70.87,169.57,25.96,9.46" target="#b76">(Zhang et al., 2023)</ref> to guide AudioLDM2 <ref type="bibr" coords="8,203.79,169.57,81.80,9.46">(Liu et al., 2024b)</ref>, a pre-trained diffusion model, improving control over the generated music.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3" coords="8,70.87,218.51,188.25,9.81">EEG Feature Encoding Techniques</head><p><s coords="8,70.87,236.41,220.08,9.46;8,70.87,249.96,220.07,9.46;8,70.87,263.51,218.27,9.46;8,70.87,277.06,218.27,9.46;8,70.87,290.61,218.27,9.46;8,70.87,304.15,220.17,9.46">For speech, voice, and music decoding or generation from EEG, EEG signals are either transformed into intermediate representations, such as mel-spectrograms, or decoded into acoustic and articulatory features <ref type="bibr" coords="8,159.90,290.61,88.92,9.46" target="#b26">(Krishna et al., 2021)</ref>, or EEG temporal features <ref type="bibr" coords="8,151.61,304.15,83.72,9.46" target="#b20">(Jiang et al., 2024)</ref> are utilized.</s><s coords="8,70.87,317.70,220.08,9.46;8,70.87,331.25,218.27,9.46;8,70.87,344.80,220.08,9.46;8,70.87,358.35,142.00,9.46">Mel-spectrograms are especially useful, as they offer a shared representational state for both neural signals and audio, enabling more efficient translation between the two modalities.</s><s coords="8,81.78,371.90,207.36,9.46;8,70.87,385.02,218.46,9.88;8,70.87,399.00,220.07,9.46;8,70.87,412.12,171.51,9.88"><ref type="bibr" coords="8,81.78,371.90,93.32,9.46" target="#b26">Krishna et al. (2021)</ref> incorporates an attention model to predict articulatory features and another attention-regression model to convert these predicted features into acoustic features.</s><s coords="8,247.00,412.55,43.50,9.46;8,70.66,426.10,218.47,9.46;8,70.87,439.65,220.08,9.46;8,70.87,452.77,220.18,9.88">Similarly, <ref type="bibr" coords="8,70.66,426.10,80.22,9.46" target="#b20">Jiang et al. (2024)</ref> extracts EEG tokens through a multi-step process which includes DBSCAN clustering algorithm to derive EEG temporal features.</s><s coords="8,70.53,466.32,220.42,9.88;8,70.87,479.87,218.27,9.88;8,70.87,493.84,203.79,9.46">These features are eventually transformed EEG positional encoding EEG features using positional encoding, which are used to form EEG tokens.</s></p><p><s coords="8,81.78,506.97,209.16,9.88;8,70.87,520.94,218.26,9.46;8,70.51,534.49,218.81,9.46;8,70.87,548.04,218.27,9.46;8,70.87,561.59,34.29,9.46">In studies using mel-spectrograms as intermediate representations, Ramirez-Aristizabal and Kello (2022) employs a sequential CNN-based regressor to directly map EEG input to time-aligned music spectra.</s><s coords="8,109.47,561.59,179.67,9.46;8,70.87,575.14,220.08,9.46;8,70.87,588.69,220.08,9.46;8,70.87,602.24,218.27,9.46;8,70.87,615.79,78.00,9.46"><ref type="bibr" coords="8,109.47,561.59,79.25,9.46">Lee et al. (2023a)</ref> seeks to adapt spoken EEG to the subspace of imagined EEG using Common Spatial Pattern (CSP) filters trained on imagined EEG, aiming to generate a user's voice from imagined speech.</s><s coords="8,154.63,615.79,136.31,9.46;8,70.87,629.34,218.26,9.46;8,70.87,642.88,220.18,9.46">These CSP filters extract temporal oscillation patterns, minimizing distribution differences between spoken and imagined EEG.</s><s coords="8,70.87,656.43,220.08,9.46;8,70.87,669.98,218.26,9.46;8,70.87,683.53,218.27,9.46;8,70.87,697.08,218.27,9.46;8,70.87,710.63,108.78,9.46">Similarly, <ref type="bibr" coords="8,115.27,656.43,100.75,9.46" target="#b47">Postolache et al. (2024)</ref> applies this technique while temporally aligning users' voices with brain signals, using triggers to mark onset intervals and clearly distinguish actual utterance intervals in continuous brain signals.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4" coords="8,70.87,732.47,113.83,9.81">Evaluation Metrics</head><p><s coords="8,70.87,750.37,220.08,9.46;8,70.87,763.92,220.07,9.46;8,306.14,74.72,220.07,9.46;8,306.14,88.27,111.25,9.46">EEG-to-speech generation is evaluated using quantitative and qualitative metrics, based on its time-series structure, which also enables its representation as mel-spectrograms.</s><s coords="8,420.79,88.08,103.62,9.40;8,305.41,101.63,220.81,9.66;8,306.14,115.37,218.27,9.46;8,306.14,128.92,219.63,9.46;8,306.14,142.27,218.27,9.66;8,305.80,155.82,220.42,9.66;8,306.14,169.57,219.63,9.46;8,306.14,183.12,27.61,9.46">Mel Cepstral Distortion (MCD) and Root Mean Square Error (RMSE) measure similarity between reconstructed and original speech signals <ref type="bibr" coords="8,375.00,128.92,97.41,9.46" target="#b26">(Krishna et al., 2021;</ref><ref type="bibr" coords="8,476.24,128.92,49.53,9.46;8,306.14,142.47,23.58,9.46" target="#b46">Park et al., 2024)</ref>, while Structural Similarity Index (SSI) and Peak Signal-to-Noise Ratio (PSNR) assess spectrogram quality (Ramirez-Aristizabal and <ref type="bibr" coords="8,498.54,169.57,27.23,9.46;8,306.14,183.12,23.01,9.46" target="#b49">Kello, 2022)</ref>.</s><s coords="8,337.03,183.12,187.38,9.46;8,306.14,196.67,219.63,9.46;8,306.14,210.22,220.08,9.46;8,306.14,223.57,220.08,9.66;8,306.14,237.12,167.41,9.66">Linguistic accuracy is evaluated using Word Error Rate (WER), Character Error Rate (CER), and BERTScore <ref type="bibr" coords="8,378.67,210.22,89.53,9.46" target="#b40">(Mizuno et al., 2024)</ref>, and perceptual quality is quantified with Frechet Audio Distance (FAD) <ref type="bibr" coords="8,363.35,237.31,105.43,9.46" target="#b47">(Postolache et al., 2024)</ref>.</s><s coords="8,476.94,237.31,47.46,9.46;8,306.14,250.67,218.27,9.66;8,306.14,264.22,218.45,9.66;8,306.14,277.96,213.77,9.46">Additional metrics include Hits@k for search relevance <ref type="bibr" coords="8,498.28,250.86,26.13,9.46;8,306.14,264.41,53.79,9.46" target="#b20">(Jiang et al., 2024)</ref> and Mean Opinion Score (MOS) for subjective quality assessment <ref type="bibr" coords="8,436.91,277.96,78.20,9.46">(Lee et al., 2023b)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" coords="8,306.14,312.97,167.28,10.75">Conclusion and Future Work</head><p><s coords="8,305.63,343.90,218.78,9.46;8,306.14,357.45,220.07,9.46;8,306.14,371.00,220.07,9.46;8,306.14,384.54,218.27,9.46;8,305.78,398.09,85.35,9.46">With advancements in Generative AI, EEG-once primarily used for classification tasks-is now being harnessed for generation, which marks a significant step toward brain-computer interaction (BCI) applications.</s><s coords="8,394.81,398.09,131.41,9.46;8,306.14,411.64,220.08,9.46;8,306.14,425.19,220.08,9.46;8,306.14,438.74,220.08,9.46;8,306.14,452.29,220.08,9.46;8,306.14,465.84,192.11,9.46">Given its portability and noninvasive nature, EEG has strong potential for realtime, widespread applications, particularly in assistive communication by enabling direct thought-tospeech or thought-to-text systems that enhance accessibility and human-computer interaction.</s><s coords="8,501.65,465.84,24.57,9.46;8,306.14,479.39,220.08,9.46;8,306.14,492.94,220.18,9.46">However, comparing studies in this field remains challenging due to the lack of standardized benchmarks.</s><s coords="8,306.14,506.49,218.27,9.46;8,306.14,520.04,218.45,9.46;8,306.14,533.59,218.45,9.46;8,306.14,547.14,127.90,9.46">Even when studies utilized the same datasets, the subject-dependent nature of EEG data allowed for multiple ways of splitting and processing, either by subject or object category.</s><s coords="8,437.42,547.14,88.80,9.46;8,306.14,560.68,218.26,9.46;8,306.14,574.23,218.27,9.46;8,306.14,587.78,220.08,9.46;8,306.14,601.33,163.41,9.46">For a fair and meaningful comparison across the surveyed studies, it is crucial to establish standardized benchmarks that define consistent data partitioning, evaluation metrics, and model validation protocols.</s><s coords="8,473.87,601.33,50.54,9.46;8,306.14,614.88,218.27,9.46;8,306.14,628.43,218.27,9.46;8,305.87,641.98,218.54,9.46;8,306.14,655.53,38.28,9.46">This would ensure reproducibility, facilitate progress in the field, and enable a more accurate assessment of various approaches in EEG-based generative AI research.</s><s coords="8,347.77,655.53,176.64,9.46;8,306.14,669.08,218.27,9.46;8,306.14,682.63,190.17,9.46">Nevertheless, we remain optimistic about further advancements in EEG processing and its potential for generating different modalities.</s><s coords="8,499.69,682.63,26.53,9.46;8,306.14,696.18,218.46,9.46;8,306.14,709.73,220.07,9.46;8,306.14,723.27,220.07,9.46;8,306.14,736.82,218.27,9.46;8,306.14,750.37,218.27,9.46;8,306.14,763.92,61.96,9.46">As research progresses, improved methodologies, larger datasets, and standardized benchmarks will enhance the reliability and effectiveness of EEGbased generative solutions and bring us closer to real-time, practical implementations of EEG-driven generative AI.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="9,70.87,73.58,59.12,10.75">Limitations</head><p><s coords="9,70.35,98.94,218.78,9.46;9,70.87,112.49,219.63,9.46;9,70.87,126.04,218.27,9.46;9,70.87,139.59,58.91,9.46">While this survey provides a comprehensive overview of EEG-based generative AI applications, certain limitations exist due to the focused scope of this work.</s><s coords="9,137.01,139.59,153.94,9.46;9,70.87,153.14,219.63,9.46;9,70.87,166.69,220.07,9.46;9,70.87,180.24,218.65,9.46;9,70.51,193.79,220.54,9.46">Firstly, this survey primarily covers EEG-based Brain-Computer Interfaces (BCIs), deliberately excluding other neuroimaging techniques such as fMRI, Magnetoencephalography (MEG), and Near-Infrared Spectroscopy (NIRS).</s><s coords="9,70.47,207.33,218.66,9.46;9,70.87,220.88,220.08,9.46;9,70.87,234.43,218.26,9.46;9,70.87,247.98,218.27,9.46;9,70.87,261.53,82.91,9.46">Although these modalities play a significant role in BCI research and offer complementary advantages in terms of spatial resolution and multimodal integration, their detailed discussion is beyond the scope of this work.</s></p><p><s coords="9,81.78,276.41,209.17,9.46;9,70.87,289.96,218.27,9.46;9,70.87,303.51,218.27,9.46;9,70.87,317.06,220.07,9.46;9,70.87,330.61,108.58,9.46">Secondly, due to space constraints, in-depth discussions on the cognitive underpinnings of EEG signals -such as their biological origins, neural interpretations, and relationships with brain activity-have been omitted.</s><s coords="9,185.57,330.61,105.37,9.46;9,70.87,344.16,220.08,9.46;9,70.87,357.71,218.65,9.46;9,70.87,371.26,91.83,9.46">Similarly, technical details regarding EEG hardware, electrode configurations, and device specifications have been largely excluded for brevity.</s><s coords="9,166.19,371.26,124.75,9.46;9,70.87,384.81,220.08,9.46;9,70.87,398.36,218.27,9.46;9,70.87,411.91,184.21,9.46">While these aspects are crucial for practical EEG-based applications, our focus remains on the computational and generative modeling aspects of EEG data processing.</s></p><p><s coords="9,81.78,426.79,209.17,9.46;9,70.87,440.34,218.27,9.46;9,70.87,453.89,220.07,9.46;9,70.87,467.44,79.98,9.46">Finally, this survey assumes a general background in EEG signal processing, and generative modeling and expects familiarity with these foundational concepts.</s><s coords="9,154.22,467.44,136.73,9.46;9,70.87,480.98,220.08,9.46;9,70.87,494.53,218.27,9.46;9,70.87,508.08,105.09,9.46">While we provide essential explanations, a more in-depth introduction to the fundamentals of EEG and BCI technology is outside the scope of this review.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="9,70.87,535.32,86.66,10.75">Ethics Statement</head><p><s coords="9,70.87,560.68,220.08,9.46;9,70.87,574.23,220.08,9.46;9,70.87,587.78,220.17,9.46">EEG data is inherently sensitive, as it contains neural activity patterns that can potentially reveal cognitive states and sometimes personal information.</s><s coords="9,70.35,601.33,218.78,9.46;9,70.87,614.88,218.26,9.46;9,70.87,628.43,220.08,9.46;9,70.87,641.98,79.00,9.46">While the majority of the works covered in this survey adhere to established ethical guidelines and standards, some studies may require additional ethical justifications.</s><s coords="9,154.89,641.98,136.05,9.46;9,70.87,655.53,218.26,9.46;9,70.87,669.08,218.27,9.46;9,70.87,682.63,133.24,9.46">We have not conducted an exhaustive review of the ethical compliance of each cited work but emphasize the importance of ethical transparency in EEG research.</s><s coords="9,207.49,682.63,81.65,9.46;9,70.87,696.18,218.45,9.46;9,70.87,709.73,77.80,9.46">We do not endorse studies that raise ethical concerns or lack proper ethical oversight.</s><s coords="9,156.65,709.73,132.49,9.46;9,70.87,723.27,220.08,9.46;9,70.87,736.82,218.26,9.46;9,70.87,750.37,218.27,9.46;9,70.87,763.92,88.18,9.46">Any research involving EEG data collection and analysis should rigorously follow ethical protocols, including obtaining informed consent, ensuring data anonymity, and minimizing risks to participants.</s></p><p><s coords="9,317.05,74.72,209.17,9.46;9,306.14,88.27,220.08,9.46;9,306.14,101.82,218.27,9.46;9,306.14,115.37,85.63,9.46">Additionally, we acknowledge the use of Ope-nAI's ChatGPT-4 system solely for enhancing writing efficiency, generating LaTeX code, and aiding in error debugging.</s><s coords="9,395.71,115.37,130.51,9.46;9,305.87,128.92,220.36,9.46;9,306.14,142.47,220.07,9.46;9,306.14,156.02,168.17,9.46">No content related to the survey's research findings, citations, or factual discussions was autogenerated or retrieved using Generative AI-based search mechanisms.</s><s coords="9,480.84,156.02,43.84,9.46;9,306.14,169.57,218.27,9.46;9,306.14,183.12,119.66,9.46">Our work remains grounded in peer-reviewed literature and ethical academic standards.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,93.92,167.76,407.43,9.03;2,104.88,70.86,385.51,85.44"><head>Figure 1 :</head><label>1</label><figDesc><div><p><s coords="2,93.92,167.76,407.43,9.03">Figure 1: General Steps from EEG Data Gathering to Stimuli Reconstruction (Image, Text, or Sound)</s></p></div></figDesc><graphic coords="2,104.88,70.86,385.51,85.44" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,107.93,427.24,379.42,9.03"><head>Figure 2 :</head><label>2</label><figDesc><div><p><s coords="4,107.93,427.24,379.42,9.03">Figure 2: Techniques and References of Surveyed Studies for EEG to Text, Image and Beyond</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,306.14,182.34,219.92,9.03;5,306.14,194.68,211.72,8.64;5,313.23,70.87,204.10,100.02"><head>Figure 3 :</head><label>3</label><figDesc><div><p><s coords="5,306.14,182.34,219.92,9.03;5,306.14,194.68,211.72,8.64">Figure 3: Reconstructing text from EEG, with eyetracking data used to capture word-level EEG signals</s></p></div></figDesc><graphic coords="5,313.23,70.87,204.10,100.02" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,497.22,584.10,27.20,9.46;5,306.14,597.65,220.08,9.46;5,306.14,611.20,218.27,9.46;5,306.14,624.75,219.63,9.46;5,305.51,638.29,218.90,9.46;5,306.14,651.84,220.08,9.46;5,306.14,665.39,100.12,9.46"><head/><label/><figDesc><div><p><s coords="5,497.22,584.10,27.20,9.46;5,306.14,597.65,220.08,9.46;5,306.14,611.20,218.27,9.46;5,306.14,624.75,159.78,9.46">which capture signals through image modality and leverage advancements in image-text intermodality to generate text from the collected data.</s><s coords="5,469.29,624.75,56.48,9.46;5,305.51,638.29,218.90,9.46;5,306.14,651.84,220.08,9.46;5,306.14,665.39,100.12,9.46">Additionally, Yu-Hao Chen et al. (2025) introduce a VAE-based augmentation technique to address the issue of limited EEG-text datasets.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,70.87,628.43,218.26,9.46;6,70.87,641.55,218.26,9.88;6,70.87,655.10,218.26,9.88;6,70.87,669.08,218.27,9.46;6,70.87,682.63,218.27,9.46;6,70.87,695.75,220.08,9.88;6,70.87,709.30,218.27,9.88;6,70.87,723.27,218.27,9.46;6,70.87,736.82,220.07,9.46;6,70.87,750.37,218.27,9.46;6,70.87,763.92,220.08,9.46;6,306.14,74.72,218.27,9.46;6,306.14,88.27,220.07,9.46;6,306.14,101.82,220.18,9.46;6,305.78,115.37,218.62,9.46;6,306.14,128.92,219.63,9.46;6,306.14,142.47,218.26,9.46;6,306.14,155.59,218.27,9.88;6,306.14,169.14,172.89,9.88"><head>For</head><label/><figDesc><div><p><s coords="6,88.31,628.43,200.82,9.46;6,70.87,641.55,218.26,9.88;6,70.87,655.10,105.50,9.81">text generation tasks, EEG signals are encoded into features to capture temporal patterns and semantic information.</s><s coords="6,183.84,655.53,105.29,9.46;6,70.87,669.08,218.27,9.46;6,70.87,682.63,218.27,9.46;6,70.87,695.75,220.08,9.88;6,70.87,709.30,152.66,9.88">In the study by Biswal et al. (2019), which focuses on generating medical reports, EEG signals are encoded using stacked CNNs to capture shift-invariant features and RC-NNs to capture temporal patterns.</s><s coords="6,226.91,709.73,62.22,9.46;6,70.87,723.27,218.27,9.46;6,70.87,736.82,220.07,9.46;6,70.87,750.37,47.57,9.46">These features are then used to generate key phenotypes, which hierarchical LSTMs utilize to produce detailed explanations.</s><s coords="6,121.82,750.37,167.31,9.46;6,70.87,763.92,220.08,9.46;6,306.14,74.72,218.27,9.46;6,306.14,88.27,220.07,9.46;6,306.14,101.82,52.12,9.46">Srivastava and Shinde (2020) employs an ensemble model to extract EEG embeddings, us-ing CNNs to capture spatial variations and LSTMs to model temporal sequences and long-range dependencies.</s><s coords="6,361.64,101.82,164.68,9.46;6,305.78,115.37,218.62,9.46;6,306.14,128.92,219.63,9.46;6,306.14,142.47,218.26,9.46;6,306.14,155.59,218.27,9.88;6,306.14,169.14,172.89,9.88">Another study by Yu-Hao Chen et al. (2025), proposes two objectives: classification and sequence-to-sequence (seq2seq) text generation, employing residual blocks for feature extraction in both tasks to capture both spatial and temporal features of the EEG signals effectively.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,306.14,750.37,220.07,23.01"><head/><label/><figDesc><div><p><s coords="2,364.16,750.37,160.63,9.46;2,306.14,763.92,160.18,9.46">map EEG signals to visual saliency maps corresponding to each image.</s><s coords="2,472.03,763.92,54.19,9.46">Other Com-</s></p></div></figDesc><table coords="3,74.84,74.93,445.61,24.94"><row><cell>Dataset</cell><cell>Stimuli Type</cell><cell cols="2">Channels/Electrodes Stimuli Details</cell><cell>Subjects</cell></row><row><cell>Zuco 1.0 (Hollenstein et al., 2018)</cell><cell>Text</cell><cell>128</cell><cell>Sentences from Stanford Sentiment Treebank, Wikipedia corpus</cell><cell>12</cell></row><row><cell>Zuco 2.0</cell><cell/><cell/><cell/><cell/></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,70.87,233.50,436.32,57.25"><head>Table 1 :</head><label>1</label><figDesc><div><p><s coords="3,125.08,233.89,382.10,8.64">EEG-Based Datasets from Surveyed Studies with Text, Image and Audio/Speech/Music Stimuli</s></p></div></figDesc><table coords="3,70.87,267.75,218.27,23.01"><row><cell>mon strategies include projecting neural signals</cell></row><row><cell>into a shared subspace with image embeddings</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,81.41,140.20,433.11,150.95"><head/><label/><figDesc><div><p><s coords="4,405.43,140.20,30.56,4.04"><ref type="bibr" coords="4,330.41,276.21,43.11,4.04" target="#b23">(Kavasidis et al., 2017)</ref>rception with Transformers and VAE-based Data Augmentation (Yu-HaoChen et al., 2025)Gated Recurrent Units• Open vocabulary EEG decoding incorporating a subject-dependent representation learning module(Amrani et al., 2024)• Capture global and local contextual information and long-term dependencies.(Chenetal., 2025) Recurrent Neural Networks • Translate active intention into text format based on Morse code Yang et al. (2023) Attention Mechanism • Generate medical reports (Biswal et al., 2019) • Capture global and local contextual information and long-term dependencies (Chen et al., 2025) Li et al., 2020) • Visual Saliency and Image Reconstruction from EEG Signals (Khaleghi et al., 2022) • Map EEG signals to the visual saliency maps corresponding to each image (Song et al., 2023) • Demonstrate the generalizability of feature extraction pipeline across three different datasets (Singh et al., 2024) • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion (Li et al., 2024) • Reconstructing images using the same semantics as the corresponding EEG (Zeng et al., 2023a) • Zero-shot framework to project neural signals from different sources into the shared subspace (Shimizu and Srinivasan, 2022)LSTMs• Extracting visual class discriminative information from EEG data(Kavasidis et al., 2017)• Framework for synthesizing the images using small-size EEG datasets(Singh et al., 2023)• Image reconstruction using generative adversarial and deep fuzzy neural network(Ahmadieh et al., 2024)• Demonstrate the generalizability of feature extraction pipeline across three different datasets(Singh et al., 2024)Extracting visual class discriminative information from EEG data(Kavasidis et al., 2017)• Map EEG signals to the visual saliency maps corresponding to each image(Khaleghi et al., 2022)(Mishra et al., 2023)• Generates images along with producing class-specific EEG encoding as a latent representation(Singh et al., 2024)• Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion(Li et al., 2024)• Framework for synthesizing the images using small-size EEG datasets(Singh et al., 2023)• Image reconstruction using generative adversarial and deep fuzzy neural network(Ahmadieh et al., 2024)Variational Autoencoder • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text(Bai et al., 2023)• Photorealistic Reconstruction of Visual Texture From EEG Signals(Wakita et al., 2021)• Extracting visual class discriminative information from EEG data(Kavasidis et al., 2017)</s></p></div></figDesc><table coords="4,81.41,173.19,432.88,117.96"><row><cell>Contrastive Learning</cell><cell>• Recalibrates subject-dependent EEG representation to the semantic-dependent EEG representation Feng et al. (2023) • Ensuring cross-modal semantic consistency between EEG and Text (Tao et al., 2024) • Bridge the semantic gap between EEG and Text using LLMs (Wang et al., 2024)</cell></row><row><cell>Masked Signal Modeling</cell><cell>• Improving accuracy of open-vocabulary EEG-to-text decoding (Liu et al., 2024a) • Ensuring cross-modal semantic consistency between EEG and Text (Tao et al., 2024) • Bridge the semantic gap between EEG and Text using LLMs (Wang et al., 2024)</cell></row><row><cell>Techniques (EEG-Image)</cell><cell>Studies and main use cases</cell></row><row><cell cols="2">CNNs • Semi-supervised cross-modal image generation (Generative Adversarial Networks (GANs) • Diffusion Models</cell></row></table><note coords="4,150.37,281.80,363.92,4.04;4,150.20,287.12,84.93,4.04"><p><s coords="4,150.37,281.80,1.64,4.04">(Shimizu and Srinivasan, 2022)ct neural signals from different sources into the shared subspace(Shimizu and Srinivasan, 2022)• Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text</s></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,81.40,314.25,432.56,36.82"><head/><label/><figDesc><div><p><s coords="4,508.85,314.25,5.10,4.04"><ref type="bibr" coords="4,362.71,325.16,31.42,4.04" target="#b3">(Bai et al., 2023)</ref>) framework to decode natural images for object recognition(Song et al., 2023)Masked Signal Modeling• Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text(Bai et al., 2023)</s></p></div></figDesc><table coords="4,81.40,341.53,122.26,9.54"><row><cell>Techniques Audio/Speech/Music)</cell><cell>(EEG-</cell><cell>Studies and main use cases</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,306.14,237.34,219.92,8.64;9,317.05,248.30,209.01,8.64;9,317.05,259.26,209.01,8.64;9,317.05,270.04,207.36,8.82;9,316.80,281.00,172.35,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coords="9,425.46,248.30,100.60,8.64;9,317.05,259.26,209.01,8.64;9,317.05,270.22,150.89,8.64">Visual image reconstruction based on eeg signals using a generative adversarial and deep fuzzy neural network</title>
		<author>
			<persName coords=""><forename type="first">Hajar</forename><surname>Ahmadieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Farnaz</forename><surname>Gassemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Moradi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,478.12,270.04,46.29,8.59;9,316.80,281.00,120.62,8.59">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">105497</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hajar Ahmadieh, Farnaz Gassemi, and Moham- mad Hasan Moradi. 2024. Visual image reconstruc- tion based on eeg signals using a generative adver- sarial and deep fuzzy neural network. Biomedical Signal Processing and Control, 87:105497.</note>
</biblStruct>

<biblStruct coords="9,306.14,300.50,219.51,8.64;9,316.74,311.46,209.32,8.64;9,317.05,322.42,207.53,8.64;9,317.05,333.20,207.36,8.82;9,317.05,344.16,110.05,8.59" xml:id="b1">
	<monogr>
		<title level="m" type="main" coords="9,388.26,322.42,136.33,8.64;9,317.05,333.38,176.31,8.64">Get: A generative eeg transformer for continuous context-based neural signals</title>
		<author>
			<persName coords=""><forename type="first">Omair</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Muhammad</forename><surname>Saif-Ur Rehman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marita</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tobias</forename><surname>Glasmachers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Iossifidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Klaes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.03115</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Omair Ali, Muhammad Saif-ur Rehman, Marita Metzler, Tobias Glasmachers, Ioannis Iossifidis, and Chris- tian Klaes. 2024. Get: A generative eeg transformer for continuous context-based neural signals. arXiv preprint arXiv:2406.03115.</note>
</biblStruct>

<biblStruct coords="9,306.14,363.66,219.92,8.64;9,317.05,374.62,207.36,8.64;9,316.80,385.58,209.35,8.64;9,316.88,396.36,209.27,8.59" xml:id="b2">
	<analytic>
		<title level="a" type="main" coords="9,368.34,374.62,156.08,8.64;9,316.80,385.58,205.08,8.64">Deep representation learning for open vocabulary electroencephalography-to-text decoding</title>
		<author>
			<persName coords=""><forename type="first">Hamza</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniela</forename><surname>Micucci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Napoletano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="9,316.88,396.36,205.27,8.59">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hamza Amrani, Daniela Micucci, and Paolo Napole- tano. 2024. Deep representation learning for open vocabulary electroencephalography-to-text decoding. IEEE Journal of Biomedical and Health Informatics.</note>
</biblStruct>

<biblStruct coords="9,306.14,415.86,219.52,8.64;9,317.05,426.82,208.74,8.64;9,317.05,437.78,209.01,8.64;9,317.05,448.56,156.80,8.82" xml:id="b3">
	<monogr>
		<title level="m" type="main" coords="9,458.88,426.82,66.92,8.64;9,317.05,437.78,209.01,8.64;9,317.05,448.74,14.83,8.64">Dreamdiffusion: Generating high-quality images from brain eeg signals</title>
		<author>
			<persName coords=""><forename type="first">Yunpeng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yan-Pei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.16934</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yunpeng Bai, Xintao Wang, Yan-pei Cao, Yixiao Ge, Chun Yuan, and Ying Shan. 2023. Dreamdiffusion: Generating high-quality images from brain eeg sig- nals. arXiv preprint arXiv:2306.16934.</note>
</biblStruct>

<biblStruct coords="9,306.14,468.06,218.27,8.64;9,317.05,479.02,209.01,8.64;9,317.05,489.80,207.36,8.82;9,317.05,500.76,207.35,8.59;9,317.05,511.72,209.01,8.59;9,317.05,522.68,73.06,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coords="9,477.65,468.06,46.77,8.64;9,317.05,479.02,209.01,8.64;9,317.05,489.98,125.71,8.64">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName coords=""><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,463.30,489.80,61.10,8.59;9,317.05,500.76,207.35,8.59;9,317.05,511.72,209.01,8.59;9,317.05,522.68,14.39,8.59">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza- tion, pages 65-72.</note>
</biblStruct>

<biblStruct coords="9,306.14,542.18,219.51,8.64;9,317.05,553.14,207.36,8.64;9,317.05,564.10,207.35,8.64;9,317.05,574.88,209.01,8.82;9,317.05,585.84,207.36,8.82;9,316.30,596.98,37.36,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coords="9,488.13,553.14,36.28,8.64;9,317.05,564.10,207.35,8.64;9,317.05,575.06,60.01,8.64">The alice datasets: fmri &amp; eeg observations of natural language comprehension</title>
		<author>
			<persName coords=""><forename type="first">Shohini</forename><surname>Bhattasali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen-Ming</forename><surname>Luh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Berta</forename><surname>Franzluebbers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,396.35,574.88,129.72,8.59;9,317.05,585.84,177.71,8.59">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<meeting>the Twelfth Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="120" to="125"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Shohini Bhattasali, Jonathan Brennan, Wen-Ming Luh, Berta Franzluebbers, and John Hale. 2020. The alice datasets: fmri &amp; eeg observations of natural language comprehension. In Proceedings of the Twelfth Lan- guage Resources and Evaluation Conference, pages 120-125.</note>
</biblStruct>

<biblStruct coords="9,306.14,616.30,218.27,8.64;9,316.86,627.26,207.55,8.64;9,317.05,638.04,207.36,8.82;9,316.77,649.00,209.38,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coords="9,405.48,627.26,118.92,8.64;9,317.05,638.22,149.22,8.64">Eegtotext: learning to write medical reports from eeg recordings</title>
		<author>
			<persName coords=""><forename type="first">Siddharth</forename><surname>Biswal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brandon Westover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,489.00,638.04,35.40,8.59;9,316.77,649.00,141.19,8.59">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="513" to="531"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Siddharth Biswal, Cao Xiao, M Brandon Westover, and Jimeng Sun. 2019. Eegtotext: learning to write medical reports from eeg recordings. In Machine Learning for Healthcare Conference, pages 513-531.</note>
</biblStruct>

<biblStruct coords="9,317.05,660.14,29.62,8.64" xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="raw_reference">PMLR.</note>
</biblStruct>

<biblStruct coords="9,306.14,679.46,219.92,8.64;9,317.05,690.42,209.01,8.64;9,316.69,701.38,209.46,8.64;9,317.05,712.16,208.60,8.82;9,317.05,723.30,101.28,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coords="9,376.25,690.42,149.81,8.64;9,316.69,701.38,205.90,8.64">Sinc-based convolutional neural networks for eeg-bci-based motor imagery classification</title>
		<author>
			<persName coords=""><forename type="first">Alessandro</forename><surname>Bria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Claudio</forename><surname>Marrocco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francesco</forename><surname>Tortorella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="9,327.83,712.16,193.61,8.59">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="526" to="535"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Alessandro Bria, Claudio Marrocco, and Francesco Tor- torella. 2021. Sinc-based convolutional neural net- works for eeg-bci-based motor imagery classification. In International Conference on Pattern Recognition, pages 526-535. Springer.</note>
</biblStruct>

<biblStruct coords="9,306.14,742.62,219.52,8.64;9,317.05,753.58,209.01,8.64;9,317.05,764.54,207.36,8.64;10,81.78,75.34,209.01,8.64;10,81.78,86.12,209.01,8.82;10,81.78,97.08,78.21,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coords="9,408.53,753.58,117.53,8.64;9,317.05,764.54,207.36,8.64;10,81.78,75.34,209.01,8.64;10,81.78,86.30,17.83,8.64">Decoding text from electroencephalography signals: A novel hierarchical gated recurrent unit with masked residual attention mechanism</title>
		<author>
			<persName coords=""><forename type="first">Qiupu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yimou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fenmei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duolin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiankun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,107.66,86.12,183.13,8.59;10,81.78,97.08,21.35,8.59">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">109615</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Qiupu Chen, Yimou Wang, Fenmei Wang, Duolin Sun, and Qiankun Li. 2025. Decoding text from electroen- cephalography signals: A novel hierarchical gated recurrent unit with masked residual attention mecha- nism. Engineering Applications of Artificial Intelli- gence, 139:109615.</note>
</biblStruct>

<biblStruct coords="10,70.87,117.96,219.51,8.64;10,81.78,128.92,207.36,8.64;10,81.78,139.88,207.36,8.64;10,81.78,150.66,207.35,8.82;10,81.47,161.61,152.20,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coords="10,258.67,128.92,30.47,8.64;10,81.78,139.88,207.36,8.64;10,81.78,150.83,143.08,8.64">Toward open-world electroencephalogram decoding via deep learning: A comprehensive survey</title>
		<author>
			<persName coords=""><forename type="first">Xun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aiping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruobing</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jane</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,237.20,150.66,51.93,8.59;10,81.47,161.61,83.37,8.59">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="134"/>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xun Chen, Chang Li, Aiping Liu, Martin J McKeown, Ruobing Qian, and Z Jane Wang. 2022. Toward open-world electroencephalogram decoding via deep learning: A comprehensive survey. IEEE Signal Processing Magazine, 39(2):117-134.</note>
</biblStruct>

<biblStruct coords="10,70.87,182.49,219.51,8.64;10,81.78,193.45,207.36,8.64;10,81.42,204.41,209.46,8.64;10,81.78,215.19,134.67,8.59" xml:id="b11">
	<monogr>
		<title level="m" type="main" coords="10,197.07,193.45,92.07,8.64;10,81.42,204.41,205.82,8.64">Dewave: Discrete eeg waves encoding for brain dynamics to text translation</title>
		<author>
			<persName coords=""><forename type="first">Yiqun</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu-Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chin-Teng</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.14030</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yiqun Duan, Jinzhao Zhou, Zhen Wang, Yu-Kai Wang, and Chin-Teng Lin. 2023. Dewave: Discrete eeg waves encoding for brain dynamics to text translation. arXiv preprint arXiv:2309.14030.</note>
</biblStruct>

<biblStruct coords="10,70.87,236.07,218.27,8.64;10,81.47,247.03,207.67,8.64;10,81.78,257.99,207.52,8.64;10,81.78,268.77,207.36,8.82;10,81.22,279.73,207.92,8.59;10,81.47,290.69,51.75,8.59" xml:id="b12">
	<analytic>
		<title level="a" type="main" coords="10,156.91,247.03,132.23,8.64;10,81.78,257.99,207.52,8.64;10,81.78,268.95,172.26,8.64">Aligning semantic in brain and language: A curriculum contrastive method for electroencephalography-to-text generation</title>
		<author>
			<persName coords=""><forename type="first">Xiachong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,267.12,268.77,22.01,8.59;10,81.22,279.73,207.92,8.59;10,81.47,290.69,47.43,8.59">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xiachong Feng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. Aligning semantic in brain and language: A curriculum contrastive method for electroencephalography-to-text generation. IEEE Transactions on Neural Systems and Rehabilitation Engineering.</note>
</biblStruct>

<biblStruct coords="10,70.87,311.57,218.26,8.64;10,81.78,322.53,207.36,8.64;10,81.78,333.31,207.36,8.82;10,81.45,344.26,209.18,8.82;10,81.78,355.40,17.43,8.64" xml:id="b13">
	<analytic>
		<title level="a" type="main" coords="10,127.96,322.53,161.18,8.64;10,81.78,333.48,108.49,8.64">Deep learning in eeg: Advance of the last ten-year critical period</title>
		<author>
			<persName coords=""><forename type="first">Shu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaibo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junhua</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,198.96,333.31,90.18,8.59;10,81.45,344.26,156.22,8.59">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="348" to="365"/>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shu Gong, Kaibo Xing, Andrzej Cichocki, and Junhua Li. 2021. Deep learning in eeg: Advance of the last ten-year critical period. IEEE Transactions on Cognitive and Developmental Systems, 14(2):348- 365.</note>
</biblStruct>

<biblStruct coords="10,70.87,376.10,219.52,8.64;10,81.78,387.06,207.36,8.64;10,81.78,398.02,207.36,8.64;10,81.78,408.80,209.01,8.82;10,81.78,419.76,110.51,8.59" xml:id="b14">
	<analytic>
		<title level="a" type="main" coords="10,244.95,398.02,44.18,8.64;10,81.78,408.98,82.44,8.64">Generative adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,184.17,408.80,106.62,8.59;10,81.78,419.76,106.26,8.59">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial networks. In Advances in Neural Infor- mation Processing Systems.</note>
</biblStruct>

<biblStruct coords="10,70.87,440.64,219.92,8.64;10,81.78,451.60,207.36,8.64;10,81.78,462.56,207.35,8.64;10,81.78,473.34,208.60,8.82;10,81.78,484.48,26.84,8.64" xml:id="b15">
	<analytic>
		<title level="a" type="main" coords="10,259.78,451.60,29.36,8.64;10,81.78,462.56,207.35,8.64;10,81.78,473.52,135.06,8.64">Human eeg recordings for 1,854 concepts presented in rapid serial visual presentation streams</title>
		<author>
			<persName coords=""><forename type="first">Tijl</forename><surname>Grootswagers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivy</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amanda</forename><forename type="middle">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><forename type="middle">N</forename><surname>Hebart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,227.02,473.34,58.79,8.59">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tijl Grootswagers, Ivy Zhou, Amanda K Robinson, Mar- tin N Hebart, and Thomas A Carlson. 2022. Human eeg recordings for 1,854 concepts presented in rapid serial visual presentation streams. Scientific Data, 9(1):3.</note>
</biblStruct>

<biblStruct coords="10,70.87,505.18,219.92,8.64;10,81.78,516.13,209.10,8.64;10,81.78,527.09,207.36,8.64;10,81.78,537.87,208.60,8.82;10,81.78,549.01,41.78,8.64" xml:id="b16">
	<analytic>
		<title level="a" type="main" coords="10,111.11,527.09,178.03,8.64;10,81.78,538.05,142.93,8.64">Zuco, a simultaneous eeg and eye-tracking resource for natural sentence reading</title>
		<author>
			<persName coords=""><forename type="first">Nora</forename><surname>Hollenstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Rotsztejn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Troendle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Pedroni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Langer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,232.29,537.87,54.13,8.59">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13"/>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nora Hollenstein, Jonathan Rotsztejn, Marius Troen- dle, Andreas Pedroni, Ce Zhang, and Nicolas Langer. 2018. Zuco, a simultaneous eeg and eye-tracking resource for natural sentence reading. Scientific data, 5(1):1-13.</note>
</biblStruct>

<biblStruct coords="10,70.87,569.71,218.27,8.64;10,81.78,580.67,209.01,8.64;10,81.78,591.63,209.01,8.64;10,81.78,602.41,156.25,8.82" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Nora</forename><surname>Hollenstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Troendle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Langer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00903</idno>
		<title level="m" coords="10,171.55,580.67,119.24,8.64;10,81.78,591.63,209.01,8.64;10,81.78,602.59,14.39,8.64">Zuco 2.0: A dataset of physiological recordings during natural reading and annotation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Nora Hollenstein, Marius Troendle, Ce Zhang, and Nicolas Langer. 2019. Zuco 2.0: A dataset of physio- logical recordings during natural reading and annota- tion. arXiv preprint arXiv:1912.00903.</note>
</biblStruct>

<biblStruct coords="10,70.87,623.29,219.51,8.64;10,81.78,634.25,209.01,8.64;10,81.78,645.21,207.36,8.64;10,81.78,656.17,207.36,8.64;10,81.78,666.95,209.10,8.82;10,81.78,677.91,196.50,8.82" xml:id="b18">
	<analytic>
		<title level="a" type="main" coords="10,190.58,645.21,98.56,8.64;10,81.78,656.17,207.36,8.64;10,81.78,667.13,60.35,8.64">Hubert: Self-supervised speech representation learning by masked prediction of hidden units</title>
		<author>
			<persName coords=""><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hubert</forename><surname>Yao-Hung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kushal</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abdelrahman</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,150.48,666.95,140.40,8.59;10,81.78,677.91,129.85,8.59">IEEE/ACM transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3451" to="3460"/>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel- rahman Mohamed. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing, 29:3451-3460.</note>
</biblStruct>

<biblStruct coords="10,70.87,698.78,219.52,8.64;10,81.78,709.74,207.35,8.64;10,81.78,720.70,209.01,8.64;10,81.78,731.66,207.36,8.64;10,81.78,742.62,209.01,8.64;10,81.78,753.40,207.36,8.82;10,81.47,764.36,110.96,8.82" xml:id="b19">
	<analytic>
		<title level="a" type="main" coords="10,155.10,731.66,134.04,8.64;10,81.78,742.62,209.01,8.64;10,81.78,753.58,124.20,8.64">Text and image generation from intracranial electroencephalography using an embedding space for text and images</title>
		<author>
			<persName coords=""><forename type="first">Yuya</forename><surname>Ikegawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryohei</forename><surname>Fukuma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hidenori</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Satoru</forename><surname>Oshino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naoki</forename><surname>Tani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kentaro</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yasushi</forename><surname>Iimura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiroharu</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shota</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuya</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,215.74,753.40,73.39,8.59;10,81.47,764.36,47.43,8.59">Journal of Neural Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36019</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yuya Ikegawa, Ryohei Fukuma, Hidenori Sugano, Satoru Oshino, Naoki Tani, Kentaro Tamura, Yasushi Iimura, Hiroharu Suzuki, Shota Yamamoto, Yuya Fu- jita, et al. 2024. Text and image generation from intracranial electroencephalography using an embed- ding space for text and images. Journal of Neural Engineering, 21(3):036019.</note>
</biblStruct>

<biblStruct coords="10,306.14,75.34,220.01,8.64;10,317.05,86.30,207.36,8.64;10,317.05,97.08,208.60,8.82;10,316.30,108.22,50.09,8.64" xml:id="b20">
	<analytic>
		<title level="a" type="main" coords="10,317.05,86.30,207.36,8.64;10,317.05,97.26,85.45,8.64">Eeg-driven automatic generation of emotive music based on transformer</title>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinlin</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,413.23,97.08,108.12,8.59">Frontiers in Neurorobotics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">1437737</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hui Jiang, Yu Chen, Di Wu, and Jinlin Yan. 2024. Eeg-driven automatic generation of emotive music based on transformer. Frontiers in Neurorobotics, 18:1437737.</note>
</biblStruct>

<biblStruct coords="10,306.14,128.92,219.51,8.64;10,316.69,139.88,209.38,8.64;10,317.05,150.66,209.01,8.82;10,317.05,161.61,205.44,8.82" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Blair</forename><surname>Kaneshiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacek</forename><forename type="middle">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anthony</forename><forename type="middle">M</forename><surname>Dmochowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Norcia</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Berger</surname></persName>
		</author>
		<title level="m" coords="10,508.47,139.88,17.60,8.64;10,317.05,150.83,169.82,8.64;10,505.41,150.66,20.65,8.59;10,317.05,161.61,90.96,8.59">Naturalistic music eeg dataset-hindi (nmed-h)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Stanford Digital Repository. Stanford Digit. Repository</note>
	<note type="raw_reference">Blair Kaneshiro, Duc T Nguyen, Jacek P Dmochowski, Anthony M Norcia, and Jonathan Berger. 2016. Nat- uralistic music eeg dataset-hindi (nmed-h). In Stan- ford Digital Repository. Stanford Digit. Repository.</note>
</biblStruct>

<biblStruct coords="10,306.14,182.49,219.92,8.64;10,317.05,193.45,209.10,8.64;10,317.05,204.41,207.36,8.64;10,317.05,215.37,207.35,8.64;10,317.05,226.15,163.80,8.82" xml:id="b22">
	<analytic>
		<title level="a" type="main" coords="10,344.91,204.41,179.50,8.64;10,317.05,215.37,207.35,8.64;10,317.05,226.33,50.88,8.64">A representational similarity analysis of the dynamics of object processing using single-trial eeg classification</title>
		<author>
			<persName coords=""><forename type="first">Blair</forename><surname>Kaneshiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcos</forename><forename type="middle">Perreau</forename><surname>Guimaraes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyung-Suk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anthony</forename><forename type="middle">M</forename><surname>Norcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Suppes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,375.15,226.15,32.86,8.59">Plos one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">135697</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Blair Kaneshiro, Marcos Perreau Guimaraes, Hyung- Suk Kim, Anthony M Norcia, and Patrick Suppes. 2015. A representational similarity analysis of the dynamics of object processing using single-trial eeg classification. Plos one, 10(8):e0135697.</note>
</biblStruct>

<biblStruct coords="10,306.14,247.03,219.92,8.64;10,317.05,257.99,209.10,8.64;10,317.05,268.95,209.10,8.64;10,317.05,279.73,209.01,8.82;10,317.05,290.69,167.31,8.82" xml:id="b23">
	<analytic>
		<title level="a" type="main" coords="10,317.05,268.95,204.64,8.64">Brain2image: Converting brain signals into images</title>
		<author>
			<persName coords=""><forename type="first">Isaak</forename><surname>Kavasidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniela</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,328.30,279.73,197.76,8.59;10,317.05,290.69,87.92,8.59">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1809" to="1817"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Isaak Kavasidis, Simone Palazzo, Concetto Spamp- inato, Daniela Giordano, and Mubarak Shah. 2017. Brain2image: Converting brain signals into images. In Proceedings of the 25th ACM international con- ference on Multimedia, pages 1809-1817.</note>
</biblStruct>

<biblStruct coords="10,306.14,311.57,219.92,8.64;10,317.05,322.53,207.36,8.64;10,317.05,333.48,207.36,8.64;10,317.05,344.44,209.01,8.64;10,317.05,355.40,207.36,8.64;10,317.05,366.18,140.28,8.82" xml:id="b24">
	<analytic>
		<title level="a" type="main" coords="10,421.25,333.48,103.16,8.64;10,317.05,344.44,209.01,8.64;10,317.05,355.40,207.36,8.64;10,317.05,366.36,30.66,8.64">Visual saliency and image reconstruction from eeg signals via an effective geometric deep network-based generative adversarial network</title>
		<author>
			<persName coords=""><forename type="first">Nastaran</forename><surname>Khaleghi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tohid</forename><surname>Yousefi Rezaii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soosan</forename><surname>Beheshti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saeed</forename><surname>Meshgini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sobhan</forename><surname>Sheykhivand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebelan</forename><surname>Danishvar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,355.68,366.18,43.47,8.59">Electronics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">3637</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nastaran Khaleghi, Tohid Yousefi Rezaii, Soosan Be- heshti, Saeed Meshgini, Sobhan Sheykhivand, and Sebelan Danishvar. 2022. Visual saliency and image reconstruction from eeg signals via an effective ge- ometric deep network-based generative adversarial network. Electronics, 11(21):3637.</note>
</biblStruct>

<biblStruct coords="10,306.14,387.06,220.01,8.64;10,317.05,398.02,209.01,8.64;10,317.05,408.80,207.36,8.82;10,317.05,419.76,208.85,8.82;10,316.30,430.90,27.40,8.64" xml:id="b25">
	<analytic>
		<title level="a" type="main" coords="10,317.05,398.02,209.01,8.64;10,317.05,408.98,159.43,8.64">Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis</title>
		<author>
			<persName coords=""><forename type="first">Jungil</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaekyoung</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,485.47,408.80,38.94,8.59;10,317.05,419.76,160.22,8.59">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17022" to="17033"/>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. 2020. Hifi-gan: Generative adversarial networks for effi- cient and high fidelity speech synthesis. Advances in neural information processing systems, 33:17022- 17033.</note>
</biblStruct>

<biblStruct coords="10,306.14,451.60,218.27,8.64;10,316.69,462.56,207.72,8.64;10,317.05,473.34,207.36,8.82;10,316.72,484.30,210.15,8.82;10,317.05,495.43,43.99,8.64" xml:id="b26">
	<analytic>
		<title level="a" type="main" coords="10,414.55,462.56,109.86,8.64;10,317.05,473.52,37.07,8.64">Advancing speech synthesis using eeg</title>
		<author>
			<persName coords=""><forename type="first">Gautam</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Co</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mason</forename><surname>Carnahan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><forename type="middle">H</forename><surname>Tewfik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,373.56,473.34,150.85,8.59;10,316.72,484.30,160.04,8.59">2021 10th International IEEE/EMBS Conference on Neural Engineering (NER)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="199" to="204"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Gautam Krishna, Co Tran, Mason Carnahan, and Ahmed H Tewfik. 2021. Advancing speech synthesis using eeg. In 2021 10th International IEEE/EMBS Conference on Neural Engineering (NER), pages 199- 204. IEEE.</note>
</biblStruct>

<biblStruct coords="10,306.14,516.13,219.52,8.64;10,317.05,527.09,209.10,8.64;10,317.05,538.05,209.10,8.64;10,316.74,548.83,201.13,8.82" xml:id="b27">
	<analytic>
		<title level="a" type="main" coords="10,317.05,538.05,205.04,8.64">Envisioned speech recognition using eeg sensors</title>
		<author>
			<persName coords=""><forename type="first">Pradeep</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajkumar</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pawan</forename><surname>Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Debi</forename><forename type="middle">Prosad</forename><surname>Dogra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,316.74,548.83,143.87,8.59">Personal and Ubiquitous Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="185" to="199"/>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pradeep Kumar, Rajkumar Saini, Partha Pratim Roy, Pawan Kumar Sahu, and Debi Prosad Dogra. 2018. Envisioned speech recognition using eeg sensors. Personal and Ubiquitous Computing, 22:185-199.</note>
</biblStruct>

<biblStruct coords="10,306.14,569.71,219.51,8.64;10,317.05,580.67,209.10,8.64;10,317.05,591.63,207.36,8.64;10,316.80,602.41,207.61,8.82;10,317.05,613.37,110.98,8.82" xml:id="b28">
	<monogr>
		<title level="m" type="main" coords="10,317.05,591.63,207.36,8.64;10,316.80,602.59,176.52,8.64">Seeing through the brain: Image reconstruction of visual perception from human brain signals</title>
		<author>
			<persName coords=""><forename type="first">Yu-Ting</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Long</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lili</forename><surname>Qiu</surname></persName>
		</author>
		<idno>arXiv-2308</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
	<note type="raw_reference">Yu-Ting Lan, Kan Ren, Yansen Wang, Wei-Long Zheng, Dongsheng Li, Bao-Liang Lu, and Lili Qiu. 2023. Seeing through the brain: Image reconstruction of visual perception from human brain signals. arXiv e-prints, pages arXiv-2308.</note>
</biblStruct>

<biblStruct coords="10,306.14,634.25,218.27,8.64;10,316.58,645.21,209.07,8.64;10,317.05,656.17,207.36,8.64;10,317.05,667.13,208.86,8.64;10,317.05,677.91,208.60,8.82;10,316.30,689.04,56.73,8.64" xml:id="b29">
	<analytic>
		<title level="a" type="main" coords="10,441.07,656.17,83.35,8.64;10,317.05,667.13,208.86,8.64;10,317.05,678.08,78.23,8.64">Eegnet: a compact convolutional neural network for eeg-based braincomputer interfaces</title>
		<author>
			<persName coords=""><forename type="first">Amelia</forename><forename type="middle">J</forename><surname>Vernon J Lawhern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><forename type="middle">R</forename><surname>Solon</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Waytowich</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Stephen M Gordon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brent</forename><forename type="middle">J</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="10,402.65,677.91,118.79,8.59">Journal of neural engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">56013</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vernon J Lawhern, Amelia J Solon, Nicholas R Waytowich, Stephen M Gordon, Chou P Hung, and Brent J Lance. 2018. Eegnet: a compact convolutional neural network for eeg-based brain- computer interfaces. Journal of neural engineering, 15(5):056013.</note>
</biblStruct>

<biblStruct coords="10,306.14,709.74,219.92,8.64;10,317.05,720.70,209.10,8.64;10,317.05,731.66,209.01,8.64;10,317.05,742.44,209.01,8.82;10,317.05,753.40,207.36,8.82;10,316.30,764.54,43.99,8.64" xml:id="b30">
	<analytic>
		<title level="a" type="main" coords="10,317.05,731.66,209.01,8.64;10,317.05,742.62,40.57,8.64">Speech synthesis from brain signals based on generative model</title>
		<author>
			<persName coords=""><forename type="first">Young-Eun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sang-Ho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seo-Hyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jung-Sun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soowon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="10,376.81,742.44,149.25,8.59;10,317.05,753.40,176.91,8.59">2023 11th International Winter Conference on Brain-Computer Interface (BCI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="4"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Young-Eun Lee, Sang-Ho Kim, Seo-Hyun Lee, Jung- Sun Lee, Soowon Kim, and Seong-Whan Lee. 2023a. Speech synthesis from brain signals based on genera- tive model. In 2023 11th International Winter Con- ference on Brain-Computer Interface (BCI), pages 1-4. IEEE.</note>
</biblStruct>

<biblStruct coords="11,70.87,75.34,218.27,8.64;11,81.78,86.30,209.01,8.64;11,81.78,97.08,209.01,8.82;11,81.78,108.04,208.60,8.59;11,81.53,119.18,119.62,8.64" xml:id="b31">
	<analytic>
		<title level="a" type="main" coords="11,186.56,86.30,104.23,8.64;11,81.78,97.26,152.84,8.64">Towards voice reconstruction from eeg during imagined speech</title>
		<author>
			<persName coords=""><forename type="first">Young-Eun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seo-Hyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sang-Ho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,253.99,97.08,36.80,8.59;11,81.78,108.04,204.88,8.59">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="6030" to="6038"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Young-Eun Lee, Seo-Hyun Lee, Sang-Ho Kim, and Seong-Whan Lee. 2023b. Towards voice reconstruc- tion from eeg during imagined speech. In Proceed- ings of the AAAI Conference on Artificial Intelligence, volume 37, pages 6030-6038.</note>
</biblStruct>

<biblStruct coords="11,70.87,138.99,219.92,8.64;11,81.78,149.95,209.01,8.64;11,81.78,160.73,208.60,8.82;11,81.03,171.87,50.09,8.64" xml:id="b32">
	<analytic>
		<title level="a" type="main" coords="11,266.51,138.99,24.28,8.64;11,81.78,149.95,209.01,8.64;11,81.78,160.91,113.54,8.64">Semisupervised cross-modal image generation with generative adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Changde</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huiguang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,205.85,160.73,80.23,8.59">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">107085</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dan Li, Changde Du, and Huiguang He. 2020. Semi- supervised cross-modal image generation with gen- erative adversarial networks. Pattern Recognition, 100:107085.</note>
</biblStruct>

<biblStruct coords="11,70.87,191.68,219.52,8.64;11,81.78,202.64,209.01,8.64;11,81.78,213.60,207.36,8.64;11,81.78,224.38,206.09,8.82" xml:id="b33">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Dongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiachen</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haoyang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quanying</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.07721</idno>
		<title level="m" coords="11,249.06,202.64,41.72,8.64;11,81.78,213.60,207.36,8.64;11,81.78,224.56,64.00,8.64">Visual decoding and reconstruction via eeg embeddings with guided diffusion</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Dongyang Li, Chen Wei, Shiying Li, Jiachen Zou, Haoyang Qin, and Quanying Liu. 2024. Visual de- coding and reconstruction via eeg embeddings with guided diffusion. arXiv preprint arXiv:2403.07721.</note>
</biblStruct>

<biblStruct coords="11,70.87,244.37,218.27,8.64;11,81.78,255.15,207.36,8.82;11,81.78,266.11,109.00,8.82" xml:id="b34">
	<analytic>
		<title level="a" type="main" coords="11,158.69,244.37,130.44,8.64;11,81.78,255.33,100.44,8.64">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,208.23,255.15,80.91,8.59;11,81.78,266.11,50.13,8.59">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</note>
</biblStruct>

<biblStruct coords="11,70.87,286.11,218.27,8.64;11,81.78,297.07,207.36,8.64;11,81.53,308.02,209.26,8.64;11,81.78,318.80,207.36,8.82;11,81.78,329.76,75.27,8.59" xml:id="b35">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Hanwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Hajialigol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benny</forename><surname>Antony</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aiguo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.02165</idno>
		<title level="m" coords="11,221.04,297.07,68.09,8.64;11,81.53,308.02,209.26,8.64;11,81.78,318.98,142.18,8.64">Eeg2text: Open vocabulary eeg-to-text decoding with eeg pretraining and multi-view transformer</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Hanwen Liu, Daniel Hajialigol, Benny Antony, Aiguo Han, and Xuan Wang. 2024a. Eeg2text: Open vocabulary eeg-to-text decoding with eeg pre- training and multi-view transformer. arXiv preprint arXiv:2405.02165.</note>
</biblStruct>

<biblStruct coords="11,70.87,349.76,218.27,8.64;11,81.78,360.72,209.01,8.64;11,81.78,371.68,209.01,8.64;11,81.78,382.63,209.01,8.64;11,81.78,393.41,207.35,8.82;11,81.17,404.37,167.52,8.59" xml:id="b36">
	<analytic>
		<title level="a" type="main" coords="11,267.08,371.68,23.71,8.64;11,81.78,382.63,209.01,8.64;11,81.78,393.59,87.94,8.64">Audioldm 2: Learning holistic audio generation with selfsupervised pretraining</title>
		<author>
			<persName coords=""><forename type="first">Haohe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xubo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinhao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,177.18,393.41,111.95,8.59;11,81.17,404.37,163.31,8.59">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yux- uan Wang, and Mark D Plumbley. 2024b. Audi- oldm 2: Learning holistic audio generation with self- supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing.</note>
</biblStruct>

<biblStruct coords="11,70.87,424.37,219.52,8.64;11,81.78,435.33,209.01,8.64;11,81.78,446.28,207.35,8.64;11,81.78,457.06,199.57,8.82" xml:id="b37">
	<analytic>
		<title level="a" type="main" coords="11,208.63,435.33,82.16,8.64;11,81.78,446.28,207.35,8.64;11,81.78,457.24,78.92,8.64">Nmed-t: A tempofocused dataset of cortical and behavioral responses to naturalistic music</title>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Losorelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacek</forename><forename type="middle">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Blair</forename><surname>Dmochowski</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kaneshiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,179.45,457.06,23.74,8.59">ISMIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Steven Losorelli, Duc T Nguyen, Jacek P Dmochowski, and Blair Kaneshiro. 2017. Nmed-t: A tempo- focused dataset of cortical and behavioral responses to naturalistic music. In ISMIR, volume 3, page 5.</note>
</biblStruct>

<biblStruct coords="11,70.87,477.06,218.52,8.64;11,81.78,488.02,209.10,8.64;11,81.47,498.98,209.32,8.64;11,81.78,509.76,207.36,8.82;11,81.78,520.72,75.27,8.59" xml:id="b38">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shreya</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jose</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacek</forename><surname>Gwizdka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shounak</forename><surname>Roychowdhury</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.07507</idno>
		<title level="m" coords="11,81.47,498.98,209.32,8.64;11,81.78,509.94,136.90,8.64">Thought2text: Text generation from eeg signal using large language models (llms)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Abhijit Mishra, Shreya Shukla, Jose Torres, Jacek Gwizdka, and Shounak Roychowdhury. 2024. Thought2text: Text generation from eeg signal us- ing large language models (llms). arXiv preprint arXiv:2410.07507.</note>
</biblStruct>

<biblStruct coords="11,70.87,540.71,219.52,8.64;11,81.78,551.67,209.01,8.64;11,81.78,562.63,209.10,8.64;11,81.45,573.41,209.18,8.82;11,81.78,584.55,22.42,8.64" xml:id="b39">
	<analytic>
		<title level="a" type="main" coords="11,191.28,551.67,99.50,8.64;11,81.78,562.63,204.96,8.64">Neurogan: image reconstruction from eeg signals via an attention-based gan</title>
		<author>
			<persName coords=""><forename type="first">Rahul</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krishan</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ranjeet</forename><surname>Ranjan Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnav</forename><surname>Bhavsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,81.45,573.41,146.78,8.59">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="9181" to="9192"/>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rahul Mishra, Krishan Sharma, Ranjeet Ranjan Jha, and Arnav Bhavsar. 2023. Neurogan: image recon- struction from eeg signals via an attention-based gan. Neural Computing and Applications, 35(12):9181- 9192.</note>
</biblStruct>

<biblStruct coords="11,70.87,604.36,219.51,8.64;11,81.78,615.32,207.36,8.64;11,81.78,626.28,209.10,8.64;11,81.78,637.06,209.01,8.82;11,81.78,648.02,207.36,8.59;11,81.11,658.98,132.74,8.82" xml:id="b40">
	<analytic>
		<title level="a" type="main" coords="11,194.75,615.32,94.38,8.64;11,81.78,626.28,205.03,8.64">An investigation on the speech recovery from eeg signals using transformer</title>
		<author>
			<persName coords=""><forename type="first">Tomoaki</forename><surname>Mizuno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Takuya</forename><surname>Kishida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Natsue</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Toru</forename><surname>Nakashika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,94.07,637.06,196.72,8.59;11,81.78,648.02,207.36,8.59;11,81.11,658.98,56.38,8.59">2024 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="6"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Tomoaki Mizuno, Takuya Kishida, Natsue Yoshimura, and Toru Nakashika. 2024. An investigation on the speech recovery from eeg signals using transformer. In 2024 Asia Pacific Signal and Information Pro- cessing Association Annual Summit and Conference (APSIPA ASC), pages 1-6. IEEE.</note>
</biblStruct>

<biblStruct coords="11,70.87,678.97,219.92,8.64;11,81.53,689.93,207.61,8.64;11,81.78,700.71,207.36,8.82;11,81.78,711.67,75.27,8.59" xml:id="b41">
	<monogr>
		<title level="m" type="main" coords="11,274.99,678.97,15.80,8.64;11,81.53,689.93,207.61,8.64;11,81.78,700.89,132.02,8.64">Unveiling thoughts: A review of advancements in eeg brain signal decoding into text</title>
		<author>
			<persName coords=""><forename type="first">Akbar</forename><surname>Saydul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Murad</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rahimi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.00726</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Saydul Akbar Murad and Nick Rahimi. 2024. Un- veiling thoughts: A review of advancements in eeg brain signal decoding into text. arXiv preprint arXiv:2405.00726.</note>
</biblStruct>

<biblStruct coords="11,70.87,731.66,220.01,8.64;11,81.47,742.62,209.05,8.64;11,81.78,753.58,207.36,8.64;11,81.78,764.54,59.49,8.64" xml:id="b42">
	<monogr>
		<title level="m" type="main" coords="11,81.47,742.62,209.05,8.64;11,81.78,753.58,207.36,8.64;11,81.78,764.54,55.52,8.64">The neural dynamics of facial identity processing: insights from eeg-based pattern analysis and image reconstruction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nemrodov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Niemeier</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Nestor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D Nemrodov, M Niemeier, A Patel, and A Nestor. 2018. The neural dynamics of facial identity processing: insights from eeg-based pattern analysis and image reconstruction.</note>
</biblStruct>

<biblStruct coords="11,306.14,75.34,218.27,8.64;11,317.05,86.30,209.01,8.64;11,317.05,97.08,208.60,8.82;11,316.30,108.22,45.11,8.64" xml:id="b43">
	<analytic>
		<title level="a" type="main" coords="11,488.84,75.34,35.57,8.64;11,317.05,86.30,209.01,8.64;11,317.05,97.26,89.84,8.64">Analysis and synthesis of natural texture perception from visual evoked potentials</title>
		<author>
			<persName coords=""><forename type="first">Taiki</forename><surname>Orima</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isamu</forename><surname>Motoyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,416.62,97.08,104.62,8.59">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">698940</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Taiki Orima and Isamu Motoyoshi. 2021. Analysis and synthesis of natural texture perception from vi- sual evoked potentials. Frontiers in Neuroscience, 15:698940.</note>
</biblStruct>

<biblStruct coords="11,306.14,129.03,219.51,8.64;11,317.05,139.99,207.61,8.64;11,317.05,150.95,209.01,8.64;11,317.05,161.90,209.10,8.64;11,316.88,172.68,207.52,8.59;11,316.88,183.64,128.57,8.82" xml:id="b44">
	<analytic>
		<title level="a" type="main" coords="11,367.93,150.95,158.14,8.64;11,317.05,161.90,205.40,8.64">Decoding brain representations by multimodal learning of neural activity and visual features</title>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isaak</forename><surname>Kavasidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniela</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,316.88,172.68,207.52,8.59;11,316.88,183.64,45.62,8.59">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3833" to="3849"/>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Simone Palazzo, Concetto Spampinato, Isaak Kavasidis, Daniela Giordano, Joseph Schmidt, and Mubarak Shah. 2020. Decoding brain representations by mul- timodal learning of neural activity and visual features. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):3833-3849.</note>
</biblStruct>

<biblStruct coords="11,306.14,204.63,219.92,8.64;11,316.86,215.59,209.20,8.64;11,317.05,226.37,207.36,8.82;11,316.80,237.33,209.27,8.59;11,317.05,248.29,136.99,8.82" xml:id="b45">
	<analytic>
		<title level="a" type="main" coords="11,382.64,215.59,143.42,8.64;11,317.05,226.55,112.68,8.64">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,448.53,226.37,75.88,8.59;11,316.80,237.33,209.27,8.59;11,317.05,248.29,68.12,8.59">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computa- tional Linguistics, pages 311-318.</note>
</biblStruct>

<biblStruct coords="11,306.14,269.28,220.01,8.64;11,316.74,280.24,207.67,8.64;11,317.05,291.02,207.36,8.82;11,316.44,301.98,209.62,8.59;11,317.05,312.94,208.60,8.59;11,317.05,324.08,69.17,8.64" xml:id="b46">
	<analytic>
		<title level="a" type="main" coords="11,316.74,280.24,207.67,8.64;11,317.05,291.20,149.01,8.64">Towards eeg-based talking-face generation for brain signal-driven dynamic communication</title>
		<author>
			<persName coords=""><forename type="first">Ji-Ha</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seo-Hyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,484.87,291.02,39.54,8.59;11,316.44,301.98,209.62,8.59;11,317.05,312.94,203.32,8.59">2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="5"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Ji-Ha Park, Seo-Hyun Lee, and Seong-Whan Lee. 2024. Towards eeg-based talking-face generation for brain signal-driven dynamic communication. In 2024 46th Annual International Conference of the IEEE Engi- neering in Medicine and Biology Society (EMBC), pages 1-5. IEEE.</note>
</biblStruct>

<biblStruct coords="11,306.14,344.89,219.51,8.64;11,316.69,355.85,208.96,8.64;11,317.05,366.80,209.01,8.64;11,317.05,377.58,207.36,8.82;11,317.05,388.54,110.05,8.59" xml:id="b47">
	<monogr>
		<title level="m" type="main" coords="11,423.31,366.80,102.75,8.64;11,317.05,377.76,177.08,8.64">Naturalistic music decoding from eeg data via latent diffusion models</title>
		<author>
			<persName coords=""><forename type="first">Emilian</forename><surname>Postolache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Natalia</forename><surname>Polouliakh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiroaki</forename><surname>Kitano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akima</forename><surname>Connelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Cosmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Taketo</forename><surname>Akama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.09062</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Emilian Postolache, Natalia Polouliakh, Hiroaki Kitano, Akima Connelly, Emanuele Rodolà, Luca Cosmo, and Taketo Akama. 2024. Naturalistic music decod- ing from eeg data via latent diffusion models. arXiv preprint arXiv:2405.09062.</note>
</biblStruct>

<biblStruct coords="11,306.14,409.53,219.92,8.64;11,317.05,420.49,207.36,8.64;11,317.05,431.45,207.36,8.64;11,316.69,442.23,207.71,8.82;11,317.05,453.19,170.63,8.82" xml:id="b48">
	<analytic>
		<title level="a" type="main" coords="11,453.33,420.49,71.08,8.64;11,317.05,431.45,207.36,8.64;11,316.69,442.41,16.24,8.64">A lip sync expert is all you need for speech to lip generation in the wild</title>
		<author>
			<persName coords=""><forename type="first">Rudrabha</forename><surname>Kr Prajwal</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,351.41,442.23,172.99,8.59;11,317.05,453.19,101.30,8.59">Proceedings of the 28th ACM international conference on multimedia</title>
		<meeting>the 28th ACM international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="484" to="492"/>
		</imprint>
	</monogr>
	<note type="raw_reference">KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Nam- boodiri, and CV Jawahar. 2020. A lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM international conference on multimedia, pages 484-492.</note>
</biblStruct>

<biblStruct coords="11,306.14,474.18,220.01,8.64;11,317.05,485.14,207.35,8.64;11,317.05,495.92,174.79,8.82" xml:id="b49">
	<monogr>
		<title level="m" type="main" coords="11,317.05,485.14,207.35,8.64;11,317.05,496.10,32.15,8.64">Eeg2mel: Reconstructing sound from brain responses to music</title>
		<author>
			<persName coords=""><forename type="first">Adolfo G Ramirez-Aristizabal</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Kello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.13845</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Adolfo G Ramirez-Aristizabal and Chris Kello. 2022. Eeg2mel: Reconstructing sound from brain responses to music. arXiv preprint arXiv:2207.13845.</note>
</biblStruct>

<biblStruct coords="11,306.14,516.91,218.27,8.64;11,317.05,527.87,207.36,8.64;11,317.05,538.65,207.36,8.82;11,317.05,549.61,120.38,8.82" xml:id="b50">
	<analytic>
		<title level="a" type="main" coords="11,374.34,527.87,150.08,8.64;11,317.05,538.83,131.82,8.64">Folded ensemble deep learning based text generation on the brain signal</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vasundhara S Rathod</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Omprakash G Kakde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,456.15,538.65,68.26,8.59;11,317.05,549.61,66.23,8.59">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="29"/>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vasundhara S Rathod, Ashish Tiwari, and Omprakash G Kakde. 2024. Folded ensemble deep learning based text generation on the brain signal. Multimedia Tools and Applications, pages 1-29.</note>
</biblStruct>

<biblStruct coords="11,306.14,570.60,219.52,8.64;11,316.69,581.56,209.46,8.64;11,317.05,592.34,207.36,8.82;11,317.05,603.30,170.98,8.82" xml:id="b51">
	<analytic>
		<title level="a" type="main" coords="11,317.05,592.52,150.92,8.64">Improved techniques for training gans</title>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,475.75,592.34,48.66,8.59;11,317.05,603.30,151.92,8.59">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. 2016. Improved techniques for training gans. Advances in neural information processing systems, 29.</note>
</biblStruct>

<biblStruct coords="11,306.14,624.29,220.01,8.64;11,317.05,635.24,209.02,8.64;11,317.05,646.02,160.68,8.82" xml:id="b52">
	<monogr>
		<title level="m" type="main" coords="11,342.84,635.24,183.22,8.64;11,317.05,646.20,18.68,8.64">Bleurt: Learning robust metrics for text generation</title>
		<author>
			<persName coords=""><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ankur P</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04696</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Thibault Sellam, Dipanjan Das, and Ankur P Parikh. 2020. Bleurt: Learning robust metrics for text gener- ation. arXiv preprint arXiv:2004.04696.</note>
</biblStruct>

<biblStruct coords="11,306.14,667.01,219.92,8.64;11,317.05,677.97,207.36,8.64;11,317.05,688.93,169.56,8.64" xml:id="b53">
	<analytic>
		<title level="a" type="main" coords="11,450.45,667.01,75.61,8.64;11,317.05,677.97,207.36,8.64;11,317.05,688.93,42.56,8.64">Improving classification and reconstruction of imagined images from eeg signals</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="11,365.88,688.93,26.58,8.64">biorxiv</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H Shimizu and R Srinivasan. 2022. Improving classifi- cation and reconstruction of imagined images from eeg signals. biorxiv. retrieved july 5, 2022.</note>
</biblStruct>

<biblStruct coords="11,306.14,709.74,219.92,8.64;11,317.05,720.70,209.10,8.64;11,317.05,731.66,207.36,8.64;11,317.05,742.44,207.36,8.82;11,316.88,753.40,207.52,8.59;11,316.72,764.36,144.55,8.82" xml:id="b54">
	<analytic>
		<title level="a" type="main" coords="11,345.31,731.66,179.10,8.64;11,317.05,742.62,106.52,8.64">Learning robust deep visual representations from eeg brain recordings</title>
		<author>
			<persName coords=""><forename type="first">Prajwal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dwip</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gautam</forename><surname>Vashishtha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krishna</forename><surname>Miyapuram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shanmuganathan</forename><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="11,446.65,742.44,77.76,8.59;11,316.88,753.40,207.52,8.59;11,316.72,764.36,65.67,8.59">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="7553" to="7562"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Prajwal Singh, Dwip Dalal, Gautam Vashishtha, Kr- ishna Miyapuram, and Shanmuganathan Raman. 2024. Learning robust deep visual representations from eeg brain recordings. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 7553-7562.</note>
</biblStruct>

<biblStruct coords="12,70.87,75.34,218.27,8.64;12,81.78,86.30,207.36,8.64;12,81.78,97.08,207.36,8.82;12,81.53,108.04,209.27,8.59;12,81.78,119.00,207.35,8.82;12,81.03,130.13,43.99,8.64" xml:id="b55">
	<analytic>
		<title level="a" type="main" coords="12,213.44,86.30,75.69,8.64;12,81.78,97.26,152.84,8.64">Eeg2image: image reconstruction from eeg brain signals</title>
		<author>
			<persName coords=""><forename type="first">Prajwal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pankaj</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krishna</forename><surname>Miyapuram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shanmuganathan</forename><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="12,256.39,97.08,32.74,8.59;12,81.53,108.04,209.27,8.59;12,81.78,119.00,177.62,8.59">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Prajwal Singh, Pankaj Pandey, Krishna Miyapuram, and Shanmuganathan Raman. 2023. Eeg2image: image reconstruction from eeg brain signals. In ICASSP 2023-2023 IEEE International Conference on Acous- tics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE.</note>
</biblStruct>

<biblStruct coords="12,70.87,150.95,219.52,8.64;12,81.20,161.90,207.93,8.64;12,81.78,172.68,207.36,8.82;12,81.78,183.64,110.05,8.59" xml:id="b56">
	<monogr>
		<title level="m" type="main" coords="12,249.62,161.90,39.51,8.64;12,81.78,172.86,178.11,8.64">Decoding natural images from eeg for object recognition</title>
		<author>
			<persName coords=""><forename type="first">Yonghao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bingchuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nanlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaorong</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.13234</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yonghao Song, Bingchuan Liu, Xiang Li, Nanlin Shi, Yijun Wang, and Xiaorong Gao. 2023. Decoding natural images from eeg for object recognition. arXiv preprint arXiv:2308.13234.</note>
</biblStruct>

<biblStruct coords="12,70.87,204.63,218.27,8.64;12,81.42,215.59,209.37,8.64;12,81.78,226.55,209.01,8.64;12,81.78,237.33,207.36,8.82;12,81.78,248.29,75.27,8.59" xml:id="b57">
	<monogr>
		<title level="m" type="main" coords="12,134.57,215.59,156.22,8.64;12,81.78,226.55,209.01,8.64;12,81.78,237.51,134.27,8.64">Common spatial generative adversarial networks based eeg data augmentation for crosssubject brain-computer interface</title>
		<author>
			<persName coords=""><forename type="first">Yonghao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueyu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Longhan</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04456</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yonghao Song, Lie Yang, Xueyu Jia, and Longhan Xie. 2021. Common spatial generative adversar- ial networks based eeg data augmentation for cross- subject brain-computer interface. arXiv preprint arXiv:2102.04456.</note>
</biblStruct>

<biblStruct coords="12,70.87,269.28,219.51,8.64;12,81.78,280.24,209.10,8.64;12,81.78,291.20,207.36,8.64;12,81.53,301.98,209.26,8.82;12,81.78,312.94,208.60,8.59;12,81.78,324.08,72.50,8.64" xml:id="b58">
	<analytic>
		<title level="a" type="main" coords="12,112.51,291.20,176.62,8.64;12,81.53,302.16,75.59,8.64">Deep learning human mind for automated visual classification</title>
		<author>
			<persName coords=""><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isaak</forename><surname>Kavasidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniela</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nasim</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="12,174.81,301.98,115.98,8.59;12,81.78,312.94,204.57,8.59">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6809" to="6817"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Concetto Spampinato, Simone Palazzo, Isaak Kavasidis, Daniela Giordano, Nasim Souly, and Mubarak Shah. 2017. Deep learning human mind for automated visual classification. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 6809-6817.</note>
</biblStruct>

<biblStruct coords="12,70.87,344.89,219.65,8.64;12,81.47,355.67,209.32,8.82;12,81.78,366.63,208.60,8.59;12,81.78,377.76,66.69,8.64" xml:id="b59">
	<analytic>
		<title level="a" type="main" coords="12,242.25,344.89,48.27,8.64;12,81.47,355.85,126.81,8.64">Think2type: Thoughts to text using eeg waves</title>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tanvi</forename><surname>Shinde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="12,216.30,355.67,74.48,8.59;12,81.78,366.63,204.29,8.59">International Journal of Engineering Research &amp; Technology (IJERT)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">06</biblScope>
			<biblScope unit="page" from="2278" to="2296"/>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Aditya Srivastava and Tanvi Shinde. 2020. Think2type: Thoughts to text using eeg waves. International Jour- nal of Engineering Research &amp; Technology (IJERT), 9(06):2278-018.</note>
</biblStruct>

<biblStruct coords="12,70.87,398.57,218.27,8.64;12,81.78,409.53,207.72,8.64;12,81.78,420.31,207.35,8.82;12,81.45,431.27,207.69,8.59;12,81.47,442.23,184.57,8.82" xml:id="b60">
	<analytic>
		<title level="a" type="main" coords="12,137.66,409.53,151.84,8.64;12,81.78,420.49,143.51,8.64">Image generation using eeg data: A contrastive learning based approach</title>
		<author>
			<persName coords=""><forename type="first">Yuma</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Goragod</forename><surname>Pongthanisorn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Genci</forename><surname>Capi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="12,267.12,420.31,22.01,8.59;12,81.45,431.27,207.69,8.59;12,81.47,442.23,87.83,8.59">IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024. 2024</date>
			<biblScope unit="page" from="794" to="798"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Yuma Sugimoto, Goragod Pongthanisorn, and Genci Capi. 2024. Image generation using eeg data: A contrastive learning based approach. In 2024 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE), pages 794-798. IEEE.</note>
</biblStruct>

<biblStruct coords="12,70.87,463.22,218.27,8.64;12,81.78,474.18,209.10,8.64;12,81.17,484.96,158.12,8.82" xml:id="b61">
	<analytic>
		<title level="a" type="main" coords="12,248.38,463.22,40.75,8.64;12,81.78,474.18,205.07,8.64">Survey on the research direction of eeg-based signal processing</title>
		<author>
			<persName coords=""><forename type="first">Congzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chaozhou</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="12,81.17,484.96,101.21,8.59">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">1203059</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Congzhong Sun and Chaozhou Mou. 2023. Survey on the research direction of eeg-based signal processing. Frontiers in Neuroscience, 17:1203059.</note>
</biblStruct>

<biblStruct coords="12,70.87,505.95,219.52,8.64;12,81.78,516.91,209.01,8.64;12,81.78,527.69,207.36,8.82;12,81.78,538.65,75.27,8.59" xml:id="b62">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yitian</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.16312</idno>
		<title level="m" coords="12,231.81,516.91,58.97,8.64;12,81.78,527.87,142.13,8.64">See: Semantically aligned eeg-to-text translation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yitian Tao, Yan Liang, Luoyu Wang, Yongqing Li, Qing Yang, and Han Zhang. 2024. See: Semanti- cally aligned eeg-to-text translation. arXiv preprint arXiv:2409.16312.</note>
</biblStruct>

<biblStruct coords="12,70.87,559.64,218.27,8.64;12,81.78,570.60,208.75,8.64;12,81.42,581.56,209.37,8.64;12,81.78,592.34,207.36,8.82;12,81.78,603.30,208.85,8.82;12,81.78,614.43,17.43,8.64" xml:id="b63">
	<analytic>
		<title level="a" type="main" coords="12,241.52,570.60,49.00,8.64;12,81.42,581.56,209.37,8.64;12,81.78,592.52,57.15,8.64">Thoughtviz: Visualizing human thoughts using generative adversarial network</title>
		<author>
			<persName coords=""><forename type="first">Praveen</forename><surname>Tirupattur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yogesh</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="12,163.75,592.34,125.39,8.59;12,81.78,603.30,156.85,8.59">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="950" to="958"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Praveen Tirupattur, Yogesh Singh Rawat, Concetto Spampinato, and Mubarak Shah. 2018. Thoughtviz: Visualizing human thoughts using generative adver- sarial network. In Proceedings of the 26th ACM international conference on Multimedia, pages 950- 958.</note>
</biblStruct>

<biblStruct coords="12,70.87,635.24,218.27,8.64;12,81.78,646.20,207.36,8.64;12,81.78,657.16,207.36,8.64;12,81.53,667.94,209.26,8.82;12,81.78,678.90,65.85,8.59" xml:id="b64">
	<analytic>
		<title level="a" type="main" coords="12,228.26,657.16,60.88,8.64;12,81.53,668.12,35.51,8.64">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="12,137.59,667.94,153.20,8.59;12,81.78,678.90,61.60,8.59">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems.</note>
</biblStruct>

<biblStruct coords="12,70.87,699.89,220.01,8.64;12,81.78,710.85,207.36,8.64;12,81.78,721.63,209.01,8.82;12,81.78,732.59,79.41,8.82" xml:id="b65">
	<analytic>
		<title level="a" type="main" coords="12,107.78,710.85,181.36,8.64;12,81.78,721.81,63.78,8.64">Photorealistic reconstruction of visual texture from eeg signals</title>
		<author>
			<persName coords=""><forename type="first">Suguru</forename><surname>Wakita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Taiki</forename><surname>Orima</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isamu</forename><surname>Motoyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="12,152.90,721.63,137.88,8.59;12,81.78,732.59,27.83,8.59">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">754587</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Suguru Wakita, Taiki Orima, and Isamu Motoyoshi. 2021. Photorealistic reconstruction of visual texture from eeg signals. Frontiers in Computational Neuro- science, 15:754587.</note>
</biblStruct>

<biblStruct coords="12,70.87,753.58,219.52,8.64;12,81.78,764.54,207.36,8.64;12,317.05,75.34,209.01,8.64;12,317.05,86.30,207.35,8.64;12,317.05,97.08,188.89,8.82" xml:id="b66">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenxi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiguo</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.17433</idno>
		<title level="m" coords="12,245.68,764.54,43.46,8.64;12,317.05,75.34,209.01,8.64;12,317.05,86.30,207.35,8.64;12,317.05,97.26,46.41,8.64">Enhancing eeg-to-text decoding through transferable representations from pre-trained contrastive eeg-text masked autoencoder</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jiaqi Wang, Zhenxi Song, Zhengyu Ma, Xipeng Qiu, Min Zhang, and Zhiguo Zhang. 2024. Enhancing eeg-to-text decoding through transferable represen- tations from pre-trained contrastive eeg-text masked autoencoder. arXiv preprint arXiv:2402.17433.</note>
</biblStruct>

<biblStruct coords="12,306.14,117.96,219.92,8.64;12,317.05,128.92,207.36,8.64;12,317.05,139.70,207.36,8.82;12,317.05,150.66,208.60,8.59;12,316.80,161.79,119.62,8.64" xml:id="b67">
	<analytic>
		<title level="a" type="main" coords="12,473.21,117.96,52.85,8.64;12,317.05,128.92,207.36,8.64;12,317.05,139.88,135.67,8.64">Open vocabulary electroencephalography-to-text decoding and zero-shot sentiment classification</title>
		<author>
			<persName coords=""><forename type="first">Zhenhailong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="12,474.06,139.70,50.35,8.59;12,317.05,150.66,204.72,8.59">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="5350" to="5358"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhenhailong Wang and Heng Ji. 2022. Open vocab- ulary electroencephalography-to-text decoding and zero-shot sentiment classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 5350-5358.</note>
</biblStruct>

<biblStruct coords="12,306.14,182.49,219.92,8.64;12,317.05,193.45,209.01,8.64;12,317.05,204.41,209.01,8.64;12,317.05,215.19,198.18,8.82" xml:id="b68">
	<monogr>
		<title level="m" type="main" coords="12,507.08,193.45,18.98,8.64;12,317.05,204.41,209.01,8.64;12,317.05,215.37,55.91,8.64">Selfsupervised learning for electroencephalogram: A systematic survey</title>
		<author>
			<persName coords=""><forename type="first">Weining</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhaohua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.05446</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Weining Weng, Yang Gu, Shuai Guo, Yuan Ma, Zhao- hua Yang, Yuchen Liu, and Yiqiang Chen. 2024. Self- supervised learning for electroencephalogram: A sys- tematic survey. arXiv preprint arXiv:2401.05446.</note>
</biblStruct>

<biblStruct coords="12,306.14,236.07,220.01,8.64;12,317.05,247.03,207.36,8.64;12,316.74,257.81,209.32,8.59;12,316.72,268.77,182.76,8.82" xml:id="b69">
	<analytic>
		<title level="a" type="main" coords="12,317.05,247.03,184.20,8.64">Brain signals for brain-computer interfaces</title>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><forename type="middle">R</forename><surname>Wolpaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chadwick</forename><forename type="middle">B</forename><surname>Boulay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="12,316.74,257.81,209.32,8.59;12,316.72,268.77,85.07,8.59">Brain-Computer Interfaces: Revolutionizing Human-Computer Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="29" to="46"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jonathan R Wolpaw and Chadwick B Boulay. 2010. Brain signals for brain-computer interfaces. In Brain-Computer Interfaces: Revolutionizing Human- Computer Interaction, pages 29-46. Springer.</note>
</biblStruct>

<biblStruct coords="12,306.14,289.65,219.51,8.64;12,317.05,300.61,207.36,8.64;12,317.05,311.57,208.60,8.64;12,317.05,322.53,207.36,8.64;12,317.05,333.31,177.27,8.82" xml:id="b70">
	<monogr>
		<title level="m" type="main" coords="12,371.86,322.53,152.55,8.64;12,317.05,333.48,34.68,8.64">Alljoined-a dataset for eeg-to-image decoding</title>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruno</forename><surname>Aristimunha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Max</forename><forename type="middle">Emanuel</forename><surname>Feucht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emma</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tazik</forename><surname>Shahjahan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martyna</forename><surname>Spyra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><forename type="middle">Zifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><surname>Short</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jioh</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.05553</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jonathan Xu, Bruno Aristimunha, Max Emanuel Feucht, Emma Qian, Charles Liu, Tazik Shahjahan, Martyna Spyra, Steven Zifan Zhang, Nicholas Short, Jioh Kim, et al. 2024. Alljoined-a dataset for eeg-to-image decoding. arXiv preprint arXiv:2404.05553.</note>
</biblStruct>

<biblStruct coords="12,306.14,354.19,219.52,8.64;12,317.05,365.14,207.36,8.64;12,317.05,376.10,209.01,8.64;12,317.05,387.06,207.36,8.64;12,317.05,398.02,207.36,8.64;12,317.05,408.80,209.01,8.82;12,317.05,419.76,64.04,8.82" xml:id="b71">
	<analytic>
		<title level="a" type="main" coords="12,395.77,376.10,130.30,8.64;12,317.05,387.06,207.36,8.64;12,317.05,398.02,207.36,8.64;12,317.05,408.98,42.65,8.64">Thoughts of brain eeg signal-totext conversion using weighted feature fusion-based multiscale dilated adaptive densenet with attention mechanism</title>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Muhammad</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Md</forename><surname>Amzad Hossain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lip</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ma</forename><surname>Haowei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ibrahim</forename><forename type="middle">M</forename><surname>Mehedi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Iskanderani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="12,368.04,408.80,158.01,8.59;12,317.05,419.76,13.15,8.59">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">105120</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jing Yang, Muhammad Awais, Md Amzad Hossain, Lip Yee, Ma Haowei, Ibrahim M Mehedi, and AIM Iskanderani. 2023. Thoughts of brain eeg signal-to- text conversion using weighted feature fusion-based multiscale dilated adaptive densenet with attention mechanism. Biomedical Signal Processing and Con- trol, 86:105120.</note>
</biblStruct>

<biblStruct coords="12,306.14,440.64,219.51,8.64;12,317.05,451.60,209.10,8.64;12,317.05,462.56,209.01,8.64;12,317.05,473.34,207.36,8.82;12,317.05,484.30,110.98,8.82" xml:id="b72">
	<monogr>
		<title level="m" type="main" coords="12,345.41,462.56,180.65,8.64;12,317.05,473.52,172.40,8.64">Decoding eeg speech perception with transformers and vae-based data augmentation</title>
		<author>
			<persName coords=""><forename type="first">Terrance</forename><surname>Yu-Hao Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pontus</forename><surname>Soederhaell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sadrishya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kateryna</forename><surname>Shapovalenko</surname></persName>
		</author>
		<idno>arXiv-2501</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
	<note type="raw_reference">Terrance Yu-Hao Chen, Yulin Chen, Pontus Soederhaell, Sadrishya Agrawal, and Kateryna Shapovalenko. 2025. Decoding eeg speech perception with trans- formers and vae-based data augmentation. arXiv e-prints, pages arXiv-2501.</note>
</biblStruct>

<biblStruct coords="12,306.14,505.18,218.26,8.64;12,317.05,516.13,207.52,8.64;12,317.05,527.09,209.01,8.64;12,317.05,537.87,209.01,8.82;12,317.05,548.83,208.60,8.59;12,317.05,559.97,62.53,8.64" xml:id="b73">
	<analytic>
		<title level="a" type="main" coords="12,409.02,516.13,115.56,8.64;12,317.05,527.09,209.01,8.64;12,317.05,538.05,76.79,8.64">Enhancing human-computer interaction with input from active and passive braincomputer interfaces</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><forename type="middle">O</forename><surname>Zander</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Kothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sabine</forename><surname>Jatzev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matti</forename><surname>Gaertner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="12,401.13,537.87,124.93,8.59;12,317.05,548.83,204.69,8.59">Brain-computer interfaces: Applying our minds to human-computer interaction</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="181" to="199"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Thorsten O Zander, Christian Kothe, Sabine Jatzev, and Matti Gaertner. 2010. Enhancing human-computer interaction with input from active and passive brain- computer interfaces. Brain-computer interfaces: Ap- plying our minds to human-computer interaction, pages 181-199.</note>
</biblStruct>

<biblStruct coords="12,306.14,580.67,218.27,8.64;12,317.05,591.63,209.01,8.64;12,317.05,602.59,207.36,8.64;12,317.05,613.37,207.36,8.82;12,316.74,624.33,144.40,8.82" xml:id="b74">
	<analytic>
		<title level="a" type="main" coords="12,478.05,591.63,48.02,8.64;12,317.05,602.59,207.36,8.64;12,317.05,613.55,127.51,8.64">a. Dmre2i: A framework based on diffusion model for the reconstruction from eeg to image</title>
		<author>
			<persName coords=""><forename type="first">Hong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nianzhang</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongguan</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Motonobu</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wanzeng</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="12,452.54,613.37,71.87,8.59;12,316.74,624.33,92.67,8.59">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">105125</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hong Zeng, Nianzhang Xia, Dongguan Qian, Motonobu Hattori, Chu Wang, and Wanzeng Kong. 2023a. Dm- re2i: A framework based on diffusion model for the reconstruction from eeg to image. Biomedical Signal Processing and Control, 86:105125.</note>
</biblStruct>

<biblStruct coords="12,306.14,645.21,219.92,8.64;12,317.05,656.17,208.60,8.64;12,317.05,667.13,207.36,8.64;12,317.05,678.08,207.35,8.64;12,317.05,688.86,207.36,8.82;12,316.72,699.82,80.65,8.82" xml:id="b75">
	<analytic>
		<title level="a" type="main" coords="12,420.65,667.13,103.76,8.64;12,317.05,678.08,207.35,8.64;12,317.05,689.04,58.04,8.64">Dcae: A dual conditional autoencoder framework for the reconstruction from eeg into image</title>
		<author>
			<persName coords=""><forename type="first">Hong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nianzhang</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haohao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Feifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wael</forename><surname>Zakaria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guojun</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coords="12,384.01,688.86,140.41,8.59;12,316.72,699.82,28.91,8.59">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">104440</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hong Zeng, Nianzhang Xia, Ming Tao, Deng Pan, Hao- hao Zheng, Chu Wang, Feifan Xu, Wael Zakaria, and Guojun Dai. 2023b. Dcae: A dual conditional autoencoder framework for the reconstruction from eeg into image. Biomedical Signal Processing and Control, 81:104440.</note>
</biblStruct>

<biblStruct coords="12,306.14,720.70,220.01,8.64;12,316.69,731.66,207.71,8.64;12,317.05,742.44,209.01,8.82;12,317.05,753.40,208.85,8.82;12,317.05,764.54,22.42,8.64" xml:id="b76">
	<analytic>
		<title level="a" type="main" coords="12,316.69,731.66,207.71,8.64;12,317.05,742.62,27.34,8.64">Adding conditional control to text-to-image diffusion models</title>
		<author>
			<persName coords=""><forename type="first">Lvmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="12,365.85,742.44,160.21,8.59;12,317.05,753.40,152.26,8.59">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3836" to="3847"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pages 3836- 3847.</note>
</biblStruct>

<biblStruct coords="13,70.87,75.34,218.27,8.64;13,81.31,86.30,209.48,8.64;13,81.78,97.08,207.36,8.82;13,81.78,108.04,75.27,8.59" xml:id="b77">
	<monogr>
		<title level="m" type="main" coords="13,224.67,86.30,66.12,8.64;13,81.78,97.26,133.90,8.64">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName coords=""><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09675</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Eval- uating text generation with bert. arXiv preprint arXiv:1904.09675.</note>
</biblStruct>

<biblStruct coords="13,70.87,128.14,218.27,8.64;13,81.78,139.10,207.36,8.64;13,81.78,149.88,207.35,8.82;13,81.78,160.84,208.60,8.59;13,81.78,171.98,89.09,8.64" xml:id="b78">
	<analytic>
		<title level="a" type="main" coords="13,242.84,128.14,46.30,8.64;13,81.78,139.10,207.36,8.64;13,81.78,150.06,25.88,8.64">Classifying phonological categories in imagined and articulated speech</title>
		<author>
			<persName coords=""><forename type="first">Shunan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Rudzicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coords="13,151.15,149.88,137.98,8.59;13,81.78,160.84,203.93,8.59">IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="992" to="996"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Shunan Zhao and Frank Rudzicz. 2015. Classifying phonological categories in imagined and articulated speech. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 992-996. IEEE.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>