{
    "paper1.pdf.tei.xml": {
        "abstract": "Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise. However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models (â‰¤3B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models. Instead, they perform better when fine-tuned on shorter, simpler reasoning chains that better align with their intrinsic learning capacity. To address this, we propose Mix Distillation, a simple yet effective strategy that balances reasoning complexity by combining long and short CoT examples or reasoning from both larger and smaller models. Our experiments demonstrate that Mix Distillation significantly improves small model reasoning performance compared to training on either data alone. These findings highlight the limitations of direct strong model distillation and underscore the importance of adapting reasoning complexity for effective reasoning capability transfer.",
        "num_figures": 19,
        "links": [
            "https://github.com/kermitt2/grobid",
            "#b0",
            "#b9",
            "#b0",
            "#b4",
            "#b21",
            "#b33",
            "#b21",
            "#b37",
            "#b37",
            "#b8",
            "#tab_3",
            "#b8",
            "#b2",
            "#b7",
            "#b5",
            "#b6",
            "#b29",
            "#b39",
            "#b40",
            "#b42",
            "#b45",
            "#tab_6",
            "#fig_0",
            "#tab_7",
            "#tab_8",
            "#fig_2",
            "#b18",
            "#fig_4",
            "#fig_5",
            "#b16",
            "#b19",
            "#b34",
            "#b38",
            "#b10",
            "#b15",
            "#b22",
            "#b23",
            "#b24",
            "#b43",
            "#b8",
            "#b12",
            "#b30",
            "#b35",
            "#b40",
            "#b44",
            "#b25",
            "#b41",
            "#tab_3",
            "#b45",
            "#b11",
            "#tab_4",
            "#tab_5",
            "#b42",
            "#b29",
            "#b40",
            "#b28",
            "#b6",
            "#b39",
            "#b5",
            "#b8",
            "#b2",
            "#b7",
            "#tab_6",
            "#tab_7",
            "#tab_7",
            "#tab_8",
            "#b18",
            "#fig_6",
            "#fig_7"
        ]
    },
    "paper10.pdf.tei.xml": {
        "abstract": "Large language models (LLMs) inherently display hallucinations since the precision of generated texts cannot be guaranteed purely by the parametric knowledge they include. Although retrieval-augmented generation (RAG) systems enhance the accuracy and reliability of generative models by incorporating external documents, these retrieved documents often fail to adequately support the model's responses in practical applications. To address this issue, we propose GGatrieval (Fine-Grained Grounded Alignment Retrieval for verifiable generation), which leverages an LLM to dynamically update queries and filter high-quality, reliable retrieval documents. Specifically, we parse the user query into its syntactic components and perform fine-grained grounded alignment with the retrieved documents. For query components that cannot be individually aligned, we propose a dynamic semantic compensation mechanism that iteratively refines and rewrites the query while continuously updating the retrieval results. This iterative process continues until the retrieved documents sufficiently support the query's response. Our approach introduces a novel criterion for filtering retrieved documents, closely emulating human strategies for acquiring targeted information. This ensures that the retrieved content effectively supports and verifies the generated outputs. On the ALCE benchmark, our method significantly surpasses a wide range of baselines, achieving state-ofthe-art performance.",
        "num_figures": 10,
        "links": [
            "https://github.com/kermitt2/grobid",
            "#fig_0",
            "#tab_3",
            "#fig_2",
            "#fig_3"
        ]
    },
    "paper2.pdf.tei.xml": {
        "abstract": "As artificial intelligence models have exploded in scale and capability, understanding of their internal mechanisms remains a critical challenge. Inspired by the success of dynamical systems approaches in neuroscience, here we propose a novel framework for studying computations in deep learning systems. We focus on the residual stream (RS) in transformer models, conceptualizing it as a dynamical system evolving across layers. We find that activations of individual RS units exhibit strong continuity across layers, despite the RS being a non-privileged basis. Activations in the RS accelerate and grow denser over layers, while individual units trace unstable periodic orbits. In reduced-dimensional spaces, the RS follows a curved trajectory with attractorlike dynamics in the lower layers. These insights bridge dynamical systems theory and mechanistic interpretability, establishing a foundation for a \"neuroscience of AI\" that combines theoretical rigor with large-scale data analysis to advance our understanding of modern neural networks.",
        "num_figures": 5,
        "links": [
            "https://github.com/kermitt2/grobid",
            "#b1",
            "#b0",
            "#b3",
            "#b11",
            "#b6",
            "#b17",
            "#b15",
            "#b14",
            "#b19",
            "#b16",
            "#b2",
            "#b18",
            "#b8",
            "#b13",
            "#b12",
            "#b7",
            "#fig_0",
            "#fig_0",
            "#fig_0",
            "#fig_0",
            "#fig_0",
            "#fig_0",
            "#fig_0",
            "#fig_0",
            "#fig_1",
            "#fig_1",
            "#fig_1",
            "#fig_2",
            "#fig_2",
            "#fig_2",
            "#fig_2",
            "#fig_3",
            "#fig_3",
            "#fig_3",
            "#fig_3",
            "#b17",
            "#b9",
            "#b4"
        ]
    },
    "paper3.pdf.tei.xml": {
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a range of text-generation tasks. However, LLMs still struggle with problems requiring multi-step decision-making and environmental feedback, such as online shopping, scientific reasoning, and mathematical problem-solving. Unlike pure text data, collecting large-scale decision-making data is challenging. Moreover, many powerful LLMs are only accessible through APIs, which hinders their fine-tuning for agent tasks due to cost and complexity. To address LLM agents' limitations, we propose a framework that can automatically learn a reward model from the environment without human annotations. This model can be used to evaluate the action trajectories of LLM agents and provide heuristics for task planning. Specifically, our approach involves employing one LLM-based agent to navigate an environment randomly, generating diverse action trajectories. Subsequently, a separate LLM is leveraged to assign a task intent and synthesize a negative response alongside the correct response for each trajectory. These triplets (task intent, positive response, and negative response) are then utilized as training data to optimize a reward model capable of scoring action trajectories. This reward model can be integrated with LLM-based agents and various planning algorithms to enhance task-solving performance. The effectiveness and generalizability of our framework are demonstrated through evaluations conducted on different agent benchmarks. In conclusion, our proposed framework represents a significant advancement in enhancing LLM agents' decision-making capabilities. By automating the learning of reward models, we overcome the challenges of data scarcity and API limitations, potentially revolutionizing the application of LLMs in complex and interactive environments. This research paves the way for more sophisticated AI agents capable of tackling a wide range of real-world problems requiring multi-step decision-making.",
        "num_figures": 21,
        "links": [
            "https://github.com/kermitt2/grobid",
            "#b4",
            "#b45",
            "#b6",
            "#b25",
            "#b44",
            "#b1",
            "#b38",
            "#b43",
            "#b7",
            "#b14",
            "#b67",
            "#b12",
            "#b33",
            "#b35",
            "#b76",
            "#b50",
            "#b57",
            "#b11",
            "#b5",
            "#b37",
            "#b10",
            "#b62",
            "#b21",
            "#b63",
            "#b75",
            "#b10",
            "#b72",
            "#b32",
            "#b54",
            "#b5",
            "#b64",
            "#b60",
            "#b17",
            "#b74",
            "#b41",
            "#b2",
            "#b31",
            "#b69",
            "#b48",
            "#b39",
            "#b53",
            "#b17",
            "#b16",
            "#b15",
            "#b15",
            "#b40",
            "#b34",
            "#b33",
            "#b8",
            "#b42",
            "#fig_6",
            "#b33",
            "#b13",
            "#b14",
            "#b22",
            "#b3",
            "#b49",
            "#b9",
            "#b52",
            "#b35",
            "#b12",
            "#b61",
            "#b12",
            "#b23",
            "#b0",
            "#tab_1",
            "#b0",
            "#b23",
            "#b12",
            "#fig_3",
            "#b32",
            "#b70",
            "#tab_2",
            "#fig_5",
            "#tab_5",
            "#tab_9",
            "#tab_10",
            "#tab_5",
            "https://huggingface.co/hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
            "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct",
            "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3",
            "https://huggingface.co/microsoft/Phi-3.5-mini-instruct",
            "https://huggingface.co/Efficient-Large-Model/VILA1.5-3b",
            "#tab_5",
            "#b35",
            "#b54",
            "#b51",
            "#b47",
            "#b35",
            "#b54",
            "#b47",
            "#fig_6",
            "#fig_3",
            "#fig_5",
            "#b51",
            "#b35",
            "#b47"
        ]
    },
    "paper4.pdf.tei.xml": {
        "abstract": "We investigate the training dynamics of deep classifiers by examining how hierarchical relationships between classes evolve during training. Through extensive experiments, we argue that the learning process in classification problems can be understood through the lens of label clustering. Specifically, we observe that networks tend to distinguish higher-level (hypernym) categories in the early stages of training, and learn more specific (hyponym) categories later. We introduce a novel framework to track the evolution of the feature manifold during training, revealing how the hierarchy of class relations emerges and refines across the network layers. Our analysis demonstrates that the learned representations closely align with the semantic structure of the dataset, providing a quantitative description of the clustering process. Notably, we show that in the hypernym label space, certain properties of neural collapse appear earlier than in the hyponym label space, helping to bridge the gap between the initial and terminal phases of learning. We believe our findings offer new insights into the mechanisms driving hierarchical learning in deep networks, paving the way for future advancements in understanding deep learning dynamics.",
        "num_figures": 26,
        "links": [
            "https://github.com/kermitt2/grobid",
            "#b2",
            "#b26",
            "#fig_1",
            "#foot_0",
            "#fig_1",
            "#b12",
            "#b34",
            "#b16",
            "#b23",
            "#b43",
            "#b33",
            "#b6",
            "#b6",
            "#b16",
            "#b39",
            "#b31",
            "#b46",
            "#b45",
            "#b45",
            "#b48",
            "#b38",
            "#b49",
            "#b35",
            "#b27",
            "#b13",
            "#b40",
            "#b27",
            "#b28",
            "#b44",
            "#b26",
            "#b10",
            "#b11",
            "#b37",
            "#b26",
            "#b19",
            "#b47",
            "#b12",
            "#b2",
            "#b34",
            "#b20",
            "#b20",
            "#fig_2",
            "#b15",
            "#b17",
            "#b26",
            "#tab_1",
            "#fig_1",
            "#fig_3",
            "#fig_4",
            "#fig_1",
            "#b29",
            "#fig_5",
            "#b46",
            "#b21",
            "#b22",
            "#b33",
            "#b6",
            "#b26",
            "#b26",
            "#b26",
            "#fig_4",
            "#fig_4",
            "#fig_4",
            "#tab_4",
            "#b9",
            "#b26",
            "#fig_7",
            "#fig_1",
            "#fig_3",
            "#fig_8",
            "#tab_5",
            "#tab_5",
            "#tab_6",
            "#fig_1",
            "#fig_9",
            "#tab_7",
            "#fig_12",
            "#fig_12",
            "#fig_4",
            "#fig_10",
            "#b26",
            "#fig_1",
            "#fig_13",
            "#foot_2",
            "#tab_8",
            "#b0",
            "#b1",
            "#b30",
            "#b30",
            "#tab_9",
            "#b30",
            "#b7",
            "#b30",
            "#fig_1",
            "#fig_1",
            "#b25"
        ]
    },
    "paper5.pdf.tei.xml": {
        "abstract": "Recent works have demonstrated that incorporating search during inference can significantly improve reasoning capabilities of language agents. Some approaches may make use of the ground truth or rely on model's own generated feedback. The search algorithm uses this feedback to then produce values that will update its criterion for exploring and exploiting various reasoning paths. In this study, we investigate how search and model's self-feedback can be leveraged for reasoning tasks. First, we explore differences in ground-truth feedback and self-feedback during search for math reasoning. Second, we observe limitations in applying search techniques to more complex tasks like tool-calling and design domain-specific approaches to address these gaps. Our experiments reveal challenges related to generalization when solely relying on self-feedback during search. For search to work effectively, either access to the ground-truth is needed or feedback mechanisms need to be carefully designed for the specific task.",
        "num_figures": 8,
        "links": [
            "https://github.com/kermitt2/grobid",
            "#b7",
            "#b4",
            "#b17",
            "#b14",
            "#b12",
            "#b0",
            "#b9",
            "#b15",
            "#b1",
            "#b2",
            "#b16",
            "#b0",
            "#b3",
            "#b13",
            "#b8",
            "#b12",
            "#b0",
            "#b11",
            "#b6",
            "#b4",
            "#b17",
            "#b14",
            "#b14",
            "#b2",
            "#tab_1",
            "#b5",
            "#b10",
            "#tab_3",
            "#tab_5"
        ]
    },
    "paper6.pdf.tei.xml": {
        "abstract": "Automating planning with LLMs presents transformative opportunities for traditional industries, yet remains underexplored. In commercial construction, the complexity of automated scheduling often requires manual intervention to ensure precision. We propose CONSTRUCTA, a novel framework leveraging LLMs to optimize construction schedules in complex projects like semiconductor fabrication. CONSTRUCTA addresses key challenges by: (1) integrating construction-specific knowledge through static RAG; (2) employing context-sampling techniques inspired by architectural expertise to provide relevant input; and (3) deploying Construction DPO to align schedules with expert preferences using RLHF. Experiments on proprietary data demonstrate performance improvements of +42.3% in missing value prediction, +79.1% in dependency analysis, and +28.9% in automated planning compared to baseline methods, showcasing its potential to revolutionize construction workflows and inspire domain-specific LLM advancements.",
        "num_figures": 9,
        "links": [
            "https://github.com/kermitt2/grobid",
            "#b27",
            "#b6",
            "#b41",
            "#b4",
            "#b2",
            "#b8",
            "#b17",
            "#fig_0",
            "#fig_0",
            "#fig_0",
            "#fig_2",
            "#foot_1",
            "#b20",
            "#b33",
            "#b47",
            "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
            "#tab_3",
            "#b28",
            "#b27",
            "#b45",
            "#b5",
            "#b32",
            "#b12",
            "#b7",
            "#b28",
            "#b26",
            "#b30",
            "#b23",
            "#b30",
            "#b45",
            "#b5",
            "#b1",
            "#b37",
            "#b3",
            "#b29",
            "#b18",
            "#b19",
            "#b16",
            "#b19",
            "#b21",
            "#b0",
            "#b46",
            "#b14",
            "#b10",
            "#b34",
            "#b40",
            "#b13",
            "#b31",
            "#b9",
            "#b22",
            "#b44",
            "#b43",
            "#b42",
            "#b35",
            "#b36",
            "#b15",
            "#fig_4",
            "#fig_4",
            "#fig_5",
            "#fig_6",
            "#fig_5",
            "#fig_6",
            "#fig_7"
        ]
    },
    "paper7.pdf.tei.xml": {
        "abstract": "Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) has opened new frontiers in brain signal decoding, enabling assistive communication, neural representation learning, and multimodal integration. BCIs, particularly those leveraging Electroencephalography (EEG), provide a non-invasive means of translating neural activity into meaningful outputs. Recent advances in deep learning, including Generative Adversarial Networks (GANs) and Transformer-based Large Language Models (LLMs), have significantly improved EEGbased generation of images, text, and speech. This paper provides a literature review of the state-of-the-art in EEG-based multimodal generation, focusing on (i) EEG-to-image generation through GANs, Variational Autoencoders (VAEs), and Diffusion Models, and (ii) EEG-totext generation leveraging Transformer based language models and contrastive learning methods. Additionally, we discuss the emerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We highlight key datasets, use cases, challenges, and EEG feature encoding methods that underpin generative approaches. By providing a structured overview of EEG-based generative AI, this survey aims to equip researchers and practitioners with insights to advance neural decoding, enhance assistive technologies, and expand the frontiers of brain-computer interaction.",
        "num_figures": 9,
        "links": [
            "https://github.com/kermitt2/grobid",
            "#b73",
            "#b69",
            "#b14",
            "#b64",
            "#b3",
            "#b59",
            "#b57",
            "#b67",
            "#b1",
            "#b64",
            "#fig_0",
            "#tab_2",
            "#fig_1",
            "#b10",
            "#b13",
            "#b68",
            "#b61",
            "#b3",
            "#b28",
            "#b3",
            "#b39",
            "#b55",
            "#b32",
            "#b56",
            "#b56",
            "#b33",
            "#b23",
            "#b56",
            "#b39",
            "#b42",
            "#b24",
            "#b17",
            "#b5",
            "#b27",
            "#b70",
            "#b58",
            "#b63",
            "#b22",
            "#b37",
            "#b21",
            "#b78",
            "#b40",
            "#b46",
            "#b53",
            "#b39",
            "#b28",
            "#b54",
            "#b60",
            "#b33",
            "#b23",
            "#b65",
            "#b23",
            "#b24",
            "#b39",
            "#b54",
            "#b33",
            "#b55",
            "#b0",
            "#b53",
            "#b3",
            "#b28",
            "#b55",
            "#b28",
            "#b56",
            "#b60",
            "#b56",
            "#b39",
            "#b56",
            "#b33",
            "#b23",
            "#b55",
            "#b54",
            "#b0",
            "#b6",
            "#b59",
            "#b59",
            "#b67",
            "#b66",
            "#b2",
            "#b62",
            "#b9",
            "#b38",
            "#b67",
            "#b2",
            "#b62",
            "#b66",
            "#b12",
            "#b11",
            "#b3",
            "#b28",
            "#b39",
            "#b56",
            "#b33",
            "#b55",
            "#b28",
            "#b56",
            "#b60",
            "#b44",
            "#b3",
            "#b28",
            "#b53",
            "#b28",
            "#b56",
            "#b51",
            "#b23",
            "#b32",
            "#b3",
            "#b55",
            "#b3",
            "#b54",
            "#b0",
            "#b24",
            "#b53",
            "#b3",
            "#b0",
            "#b60",
            "#b53",
            "#b54",
            "#b3",
            "#b39",
            "#fig_2",
            "#b6",
            "#b59",
            "#b71",
            "#b50",
            "#b67",
            "#b12",
            "#b11",
            "#b66",
            "#b2",
            "#b62",
            "#b38",
            "#b19",
            "#b9",
            "#b12",
            "#b2",
            "#b66",
            "#b62",
            "#b50",
            "#b9",
            "#b11",
            "#b38",
            "#b19",
            "#b67",
            "#b66",
            "#b2",
            "#b62",
            "#b9",
            "#b38",
            "#b12",
            "#b62",
            "#b66",
            "#b62",
            "#b2",
            "#b9",
            "#b50",
            "#b71",
            "#b67",
            "#b12",
            "#b62",
            "#b11",
            "#b66",
            "#b2",
            "#b6",
            "#b9",
            "#b45",
            "#b6",
            "#b67",
            "#b12",
            "#b34",
            "#b67",
            "#b12",
            "#b11",
            "#b66",
            "#b77",
            "#b2",
            "#b38",
            "#b12",
            "#b52",
            "#b9",
            "#b26",
            "#b49",
            "#b47",
            "#b20",
            "#b46",
            "#b40",
            "#b26",
            "#b49",
            "#b46",
            "#b40",
            "#b47",
            "#b20",
            "#b26",
            "#b46",
            "#b46",
            "#b40",
            "#b47",
            "#b20",
            "#b26",
            "#b49",
            "#b26",
            "#b25",
            "#b18",
            "#b46",
            "#b40",
            "#b47",
            "#b20",
            "#b20",
            "#b47",
            "#b76",
            "#b26",
            "#b20",
            "#b26",
            "#b20",
            "#b47",
            "#b26",
            "#b46",
            "#b49",
            "#b40",
            "#b47",
            "#b20",
            "#b23",
            "#b3"
        ]
    },
    "paper8.pdf.tei.xml": {
        "abstract": "Background: ChatGPT is an Artificial Intelligence that enables the revolution of many fields, which promises the path toward personalized education. However, the AI models fall short when tackling questions not in English. Thus, we believe investigating the reliability and robustness of such models in teaching and solving multilingual science questions is the first step toward fully adopting AI for personalized education. Approach: We use Korean mathematics questions as a validated dataset, which includes 586 questions. Other than testing the accuracy of ChatGPT, we evaluate the model's effectiveness in rating mathematics questions using eleven criteria. We also perform topic analysis, suggesting effective use of the models. Results: Out of 586 questions, ChatGPT achieves about 66.72% of accuracy (correctly answer n = 391/586 questions). Besides, ChatGPT's rating ability is substantially good and consistent with education theory and test taker's perspectives. The results highlight both the potential and limitations of ChatGPT in multilingual educational settings. While the model demonstrates a reasonable degree of accuracy (66.72%) in solving Korean mathematics questions, this also indicates room for improvement in handling non-English contexts. Its strong performance in question rating, which aligns with established educational theories and user perspectives, underscores its utility for assessment and content analysis. These findings suggest that ChatGPT can be valuable in personalized education, particularly when supported by domain-specific optimizations and multilingual training enhancements. Future work should address linguistic biases, improve accuracy across diverse languages, and integrate insights from topic analysis to enhance the model's reliability and effectiveness in diverse educational environments. This will pave the way for the broader adoption of AI in global and inclusive education systems.",
        "num_figures": 9,
        "links": [
            "https://github.com/kermitt2/grobid",
            "#b8",
            "#b3",
            "#b6",
            "#b9",
            "#b2",
            "#b0",
            "#b4",
            "#b5",
            "#b10",
            "#tab_0",
            "#b0",
            "#fig_0",
            "#tab_0",
            "#fig_0",
            "#fig_0",
            "#fig_1",
            "#fig_2"
        ]
    },
    "paper9.pdf.tei.xml": {
        "abstract": "Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously.",
        "num_figures": 22,
        "links": [
            "https://github.com/kermitt2/grobid",
            "#b38",
            "#b26",
            "#b32",
            "#b8",
            "#b33",
            "#b11",
            "#b39",
            "#b52",
            "#b16",
            "#b9",
            "#b7",
            "#b12",
            "#b50",
            "#b30",
            "#fig_0",
            "#b22",
            "#b17",
            "#b50",
            "#b9",
            "#b16",
            "#b15",
            "#b48",
            "#b40",
            "#b45",
            "#b34",
            "#b46",
            "#b2",
            "#b22",
            "#b39",
            "#b19",
            "#b5",
            "#b28",
            "#b3",
            "#b42",
            "#b30",
            "#b50",
            "#fig_2",
            "#b13",
            "#fig_1",
            "#b4",
            "#b34",
            "#b27",
            "#b3",
            "#b28",
            "#b41",
            "#fig_10",
            "#fig_10",
            "#b14",
            "#b50",
            "#b50",
            "#b5",
            "#b35",
            "#b19",
            "#b21",
            "#b47",
            "#b43",
            "#fig_2",
            "#b50",
            "#b5",
            "#b43",
            "#b45",
            "#b34",
            "#fig_4",
            "#fig_1",
            "#fig_4",
            "#tab_1",
            "#b50",
            "#b5",
            "#b43",
            "#fig_5",
            "#tab_4",
            "#b50",
            "#b45",
            "#b34",
            "#b50",
            "#b50",
            "#b21",
            "#b37",
            "#b36",
            "#b13",
            "#b10",
            "#fig_1",
            "#b21",
            "#b37",
            "#b13",
            "#b18",
            "#b18",
            "#tab_12",
            "#fig_12",
            "#b21",
            "#b37",
            "#b13",
            "#b18",
            "#b18"
        ]
    }
}